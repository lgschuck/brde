[
  {
    "objectID": "analise_dados.html",
    "href": "analise_dados.html",
    "title": "Análise de Dados e Informações",
    "section": "",
    "text": "c) Análise de dados e informações: Dado, informação, conhecimento e inteligência; Conceitos, fundamentos, características, técnicas e métodos de Business Intelligence (BI); Mapeamento de fontes de dados, Dados estruturados e dados não estruturados, Conceitos de OLAP e suas operações, Conceitos de Data Warehouse. Técnicas de modelagem e otimização de bases de dados multidimensionais, Construção de relatórios e dashboards interativos em ferramentas de BI, Manipulação de dados em planilhas, Geração de insights a partir de relatórios e dashboards, BI como suporte a processos de tomada decisão, Conceitos Básicos em Séries Temporais, Conceitos Básicos de estatística descritiva, probabilística e testes de hipótese, Manipulação, tratamento e visualização de dados, Tratamento de dados faltantes, Tratamento de dados categóricos, Normalização numérica, Detecção e tratamento de outliers"
  },
  {
    "objectID": "banco_dados.html",
    "href": "banco_dados.html",
    "title": "Banco de Dados",
    "section": "",
    "text": "a) Banco de Dados: Fundamentos de administração de dados: Segurança; Modelagem de dados: Modelo entidade-relacionamento (entidades, atributos, chaves e relacionamentos) e Normalização;"
  },
  {
    "objectID": "bd_transacao.html",
    "href": "bd_transacao.html",
    "title": "Conceitos de Transação, concorrência…",
    "section": "",
    "text": "b) Fundamentos de Banco de Dados: Conceito de transação, concorrência, recuperação, integridade\n\n\nColeções de operações que formam uma única unidade lógica de trabalho são chamadas transações. Um sistema de banco de dados precisa garantir a execução apropriada de transações apesar das falhas – a transação inteira é executada ou nenhuma parte dela é executada. Além do mais, ele precisa gerenciar a execução simultânea de transações de modo a evitar a introdução da inconsistência.\nConceito de transação: Uma transação é uma unidade de execução do programa que acessa e, possivelmente, atualiza vários itens de dados. Uma transação é delimitada pelas instruções (ou chamadas de função) na forma begin transaction e end transaction. A transação consiste em todas as operações executadas entre o begin transaction e o end transaction.\nEsse conjunto de operações precisa aparecer ao usuário como uma única unidade, indivisível. Como uma transação é indivisível, ela é executada em sua totalidade ou não é executada. Assim, se uma transação começa a ser executada, mas falha por um motivo qualquer, quaisquer mudanças no banco de dados que a transação possa ter feito são desfeitas. Essa propriedade “tudo ou nada” é conhecida como atomicidade.\nÉ necessário que o sistema de banco de dados mantenha as seguintes propriedades das transações:\n•Atomicidade. Todas as operações da transação são refletidas corretamente no banco de dados, ou nenhuma delas.\n•Consistência. A execução de uma transação isolada (ou seja, sem nenhuma outra transação executando simultaneamente) preserva a consistência do banco de dados.\n•Isolamento. Embora várias transações possam ser executadas simultaneamente, o sistema garante que, para cada par de transações Ti e Tj, parece para Ti ou Tj terminou a execução antes que Ti começasse, ou Tj iniciou a execução depois que Ti terminou. Assim, cada transação não está ciente das outras transações sendo executadas simultaneamente no sistema.\n•Durabilidade. Depois que uma transação for completada com sucesso, as mudanças que ela fez no banco de dados persistem, mesmo se houver falhas no sistema.\n\nFontes: Sistema de Banco de Dados - Abraham Silberschatz"
  },
  {
    "objectID": "conceitos_bi.html",
    "href": "conceitos_bi.html",
    "title": "Conceitos de BI",
    "section": "",
    "text": "c) Análise de dados e informações: Conceitos, fundamentos, características, técnicas e métodos de Business Intelligence (BI)"
  },
  {
    "objectID": "conceitos_bi.html#brasil-escola",
    "href": "conceitos_bi.html#brasil-escola",
    "title": "Conceitos de BI",
    "section": "Brasil Escola",
    "text": "Brasil Escola"
  },
  {
    "objectID": "conceitos_bi.html#business-intelligence",
    "href": "conceitos_bi.html#business-intelligence",
    "title": "Conceitos de BI",
    "section": "4. BUSINESS INTELLIGENCE",
    "text": "4. BUSINESS INTELLIGENCE\nO conceito de business Intelligence foi nomeado pelo Gartner Group, grupo com enfoque no uso de tecnologias para a tomada de decisão, nos anos 1990, porém, sua ideia base vem de muito antes. Quando se tem uma coleta de informações para a tomada de decisão, isso, a grosso modo, é business Intelligence. Mas a partir da nossa evolução no mundo tecnológico, o BI, vem sendo auxiliado e por inúmeras ferramentas, como o Sistemas de Informações Executivas/EIS, que começou a ser utilizado nos anos 1980, fornecendo informações a nível estratégicos das organizações, como relatórios dinâmicos e análise de tendências, sistema esse que mais tarde, juntamente com todas as suas ferramentas, seria chamado de soluções em BI.\nEm meados dos anos 1980, começou a surgir às primeiras soluções baseadas em gerenciamento de banco de dados com modelo relacional, isso implica que as informações geradas eram dispostas em tabelas com suas respectivas descrições, mesmo hoje é o modelo mais utilizado quando se diz respeito a banco de dados. Com essas novas ferramentas, se tornou mais viável o uso do sistema de informação Enterprise Resource Planning/ERP, que é a integração de todos os dados gerados pela organização.\nSegundo Davenport (1998), o ERP é definido como:\nUm software de negócio que permite à empresa automatizar e integrar a maioria de seus processos; compartilhar práticas de negócio e dados comuns pela empresa; e disponibilizar a informação em tempo real”.\nÉ visto como a solução para acabar com os vários programas que funcionam no mesmo ambiente empresarial, sem integração, produzindo informações de pouca qualidade para o negócio. Sistemas dessa natureza são adquiridos com o intuito de tornar os processos empresariais mais ágeis e extrair informações mais acuradas da empresa.\nMesmo sendo uma enorme vantagem o uso de softwares de ERP, o processo de análise de informações ainda era complicado, pois se utiliza um modelo relacional que, dependendo da sua arquitetura, não pode ser usado para a análise consistente de resultados. Os ERP são disponibilizados a partir de módulos, que se adaptam as necessidades das empresas, partindo disso e do problema em análise de dados, as empresas de ERP, passaram a incluir módulos específicos em BI.\nComo é citado pelo brasileiro Carlos Barbieri (2001, XX):\nNo fundo, tudo relativo à nova era da economia da informação, dedicada à captura de dados, informações e conhecimentos que permitam às empresas competirem com maior eficiência num ring de disputas leoninas”.\nConsiderando a utilização dos ERP, e as suas estruturas de dados, ficou nítido a necessidade de um armazenamento de dados especifico para se gerar informações confiáveis, então, começou a se pensar na utilização de um armazenamento de dados, que foi introduzido inicialmente na década de 1960 pela International Business Machines/IBM, empresa americana voltada para a área de informática, modelo chamado de Data Warehouse/DW (inicialmente o modelo era chamado de Information Warehouse).\nO Data Warehouse, segundo DALFOVO e TAMBORLIN (2006),\n[...] pode ser definido como um banco de dados especializado, que integra e gerencia o fluxo de informações a partir dos bancos de dados corporativos e fontes de dados externas à empresa, [...]“.\nNeste Sentido, o DW serve para criar uma visão centralizada e única dos dados que foram gerados em diversos outros bancos de dados. É a partir daí que o BI gera relatórios e informações, porém, ao contrário do ERP, podem ser utilizados por muitas outras pessoas na organização.\nMas afinal, qual o conceito de BI?\nO BI não pode ser enquadrado como um sistema, nem como um produto e nem as suas ferramentas, mas pode ser compreendido como o uso de arquiteturas, aplicativos e banco de dados (ZAMAN, 2005)\nDe acordo com Carlos Barbieri (2001, p.34), BI pode ser definido como “a utilização de variadas fontes de informação para se definir estratégias de competitividade nos negócios da empresa”, várias fontes de informações, pois como o acesso à tecnologia se tornou mais viável à população, mais informações de diferentes fontes, como mídias sociais, telefone e e-mail, são geradas. Informações essas que em sua maioria não podem ser analisadas, pois não foram “higienizadas”, ou estão em várias tabelas, onde se torna inviável a relação com outras tabelas, para gerar o que de fato é importante."
  },
  {
    "objectID": "conceitos_bi.html#etapas-técnicas-e-características-de-bi",
    "href": "conceitos_bi.html#etapas-técnicas-e-características-de-bi",
    "title": "Conceitos de BI",
    "section": "5. ETAPAS, TÉCNICAS E CARACTERÍSTICAS DE BI",
    "text": "5. ETAPAS, TÉCNICAS E CARACTERÍSTICAS DE BI\nO BI, como em qualquer projeto a ser implementando possuem suas etapas, que nem sempre precisam ser seguidas a riscas, mas facilitam no processo, pois assim, um modelo de projeto possa ser utilizado por outras organizações ou em implementações futuras.\nComumente, uma implementação de um projeto BI não se difere de outros projetos, no que diz respeito na forma como são levantadas as necessidades, porém, possuem suas particularidades. Abaixo, encontra-se um modelo para o desenvolvimento que pode ser a base para qualquer projeto.\n\nDefinição de requisitos\n\nPrimeira etapa, onde se deve entender a forma como o desenvolvimento vai acontecer e o resultado a ser alcançado.\n\nEstrutura do Data Warehouse\n\nDesenhar a estrutura necessária para o DW (ver seção 4.1), assim como a centralização das informações que são pertinentes, é onde também, se necessário, estruturar o Data Mart (ver seção 4.1.4).\n\nDefinição do ETL\n\nApós estruturar o DW, é necessário vincular a fonte de dados original, esse processo pode ser demorado então é de suma importância que sua implementação seja bastante consolidada (ver seção 4.1.6).\n\nEstrutura do cubo OLAP\n\nApós a estrutura do DW pronta, o ETL carregando essas informações, é necessário a criação de um cubo OLAP, que tem como finalidade agilizar a criação de relatórios, os detalhes estão na seção 4.2.\n\nDashboards\n\nApós o projeto finalizado, irão ser utilizados softwares que serão conectados ao Cubo OLAP e que possuem a característica de gerar Dashboards, ou gráficos para a análise e tomada de decisão na organização."
  },
  {
    "objectID": "dado_info.html",
    "href": "dado_info.html",
    "title": "Dado, informação, conhecimento e Inteligência",
    "section": "",
    "text": "c) Análise de dados e informações: Dado, informação, conhecimento e inteligência;\nDado\nDesde que entramos na ERA da INFORMAÇÃO, o dado é um elemento de suma importância. Nas atividades diárias necessitamos de aplicações que envolvem bancos de dados. Exemplos: aplicações de Internet Banking; reservas em hotéis ou companhias aéreas; etc.\nMas, o que é um dado? É um registro de alguma entidade. Um nome é um dado, uma foto é um dado, 134 é um dado, 5 é um dado, etc. Trata-se de uma sequência de símbolos, também conhecidos como signos, que podem ser representados com sons, imagens, textos, números e estruturas. Não há semântica envolvida no dado. Não há uma interpretação sobre essa sequência de símbolos. É algo “bruto”, como o número 10 ou a letra F.\nMoresi (2001) destaca que dados são fatos ou observações “crus”. Mais especificamente, os dados são medidas objetivas e quantitativas dos atributos (características) de entidades como pessoas, lugares, coisas e eventos (conjunto de fatos).\nOs dados são uma parte pequena da informação, que sozinhos não fazem sentido!\nInformação\nJá a informação é um dado depois de processado, é uma contextualização de um dado… Como assim? “5” é um dado, mas e se eu disser o seguinte: “No dia 5 não haverá aula!”. Nesse caso, o 5 passou a ter sentido (ou passou a ter “contexto”) e agora é uma informação!\nInformações são conjuntos de dados significativos e úteis a seres humanos em processos como o de tomada de decisões. “São dados interpretados, dotados de relevância e propósito” (DRUCKER, 1999).\nConforme destacado por Moresi (2001), informações são dados que foram organizados e ordenados de forma coerente e significativa para fins de compreensão e análise (sendo a base para ações coordenadas).\nA transformação de dados em informação é frequentemente realizada através da apresentação dos dados em uma forma compreensível ao usuário. As informações são produzidas pelo processamento de dados. Elas são utilizadas para revelar o significado dos dados.\n\nNa figura anterior, dados brutos registrados por um caixa de supermercados podem ser processados e organizados de modo a produzir informações úteis, tal como o total de unidades de detergentes vendidas ou a receita total de vendas do detergente para determinada loja ou território de vendas.\nConhecimento\nConhecimento (ou Capital Intelectual) é a habilidade de transformar a informação em ações reais. O conhecimento é uma mistura de elementos estruturados de forma intuitiva e, portanto, é difícil de ser colocado em palavras ou de ser plenamente entendido em termos lógicos.\nConhecimento, de acordo com Moresi (2001) é uma mistura fluída de experiências, informação contextual, valores e intuição, formando um painel na mente de uma pessoa que a habilita a avaliar e obter novas experiências e informações. O conhecimento é a consequência mental de angariar informações e, em sua forma mais desenvolvida, apresenta‑se como a capacidade de chegar a novas descobertas com base no aprendizado e na experiência. São informações que foram analisadas e avaliadas sobre a sua confiabilidade, sua relevância e sua importância.\nPara guardar uma informação, precisamos retê-la em nossa memória; para guardar um conhecimento, devemos incorporá-lo em nossa mente e, consequentemente, em nossa maneira de pensar.\nConhecimento demanda análise e avaliação sobre a confiabilidade, relevância e importância de dados e informações para a construção de um quadro de situação (Banca FCC/2015).\n\nInteligência\nA inteligência é o dom humano capaz de “digerir” as informações, por meio da análise, e transformá-la em conhecimento útil. Pode ser vista como o conhecimento que foi sintetizado e aplicadoa determinada situação para ganhar maior profundidade e consciência.\nBaseia-se na experiência e intuição e, portanto, é habilidade puramente humana. É a faculdade humana de conhecer, compreender, raciocinar, pensar e interpretar. Envolve exercício de ponderação para a tomada da melhor decisão, bem como noções de ética, bom e ruim, certo e errado.\nInteligência é a informação devidamente filtrada, destilada e analisada que pode apoiar a tomada de decisões. A transformação de conhecimento em inteligência ocorre por meio de síntese da experiência e, muito além do que qualquer sistema de análise de informação, necessita de habilidades humanas (MORESI, 2001).\nVamos esquematizar para facilitar a memorização desse assunto!\n\n\nFontes:\nPonto dos Concursos"
  },
  {
    "objectID": "data_warehouse.html",
    "href": "data_warehouse.html",
    "title": "Conceitos de Data Warehouse",
    "section": "",
    "text": "c) Análise de dados e informações: Conceitos de Data Warehouse"
  },
  {
    "objectID": "data_warehouse.html#data-warehouse---aws",
    "href": "data_warehouse.html#data-warehouse---aws",
    "title": "Conceitos de Data Warehouse",
    "section": "Data Warehouse - AWS",
    "text": "Data Warehouse - AWS\n\nO que é um data warehouse?\nUm data warehouse é um repositório central de informações que podem ser analisadas para tomar decisões mais adequadas. Os dados fluem de sistemas transacionais, bancos de dados relacionais e de outras fontes para o data warehouse, normalmente com uma cadência regular. Analistas de negócios, engenheiros de dados, cientistas de dados e tomadores de decisões acessam os dados por meio de ferramentas de inteligência de negócios (BI), clientes SQL e outros aplicativos de análise.\nDados e análises se tornaram indispensáveis para que as empresas se mantenham competitivas. Os usuários corporativos contam com relatórios, painéis e análises para extrair insights dos dados, monitorar a performance dos negócios e apoiar a tomada de decisões. Os data warehouses alimentam esses relatórios, painéis e ferramentas de análise armazenando dados de maneira eficiente para minimizar a entrada e saída (E/S) dos dados e fornecer resultados de consulta rapidamente para centenas e milhares de usuários simultaneamente.\n\n\nComo um data warehouse é arquitetado?\nUma arquitetura de data warehouses é composta de camadas. A camada superior é o cliente de front-end, que apresenta os resultados por meio de ferramentas de relatórios, análises e mineração de dados. A camada intermediária consiste no mecanismo de análises, usado para acessar e analisar os dados. A camada inferior da arquitetura é o servidor de banco de dados, onde os dados são carregados e armazenados. Os dados são armazenados de dois modos diferentes: 1) os dados acessados com frequência são armazenados em armazenamento muito rápido (como unidades SSD) e 2) os dados acessados com pouca frequência são armazenados em um armazenamento de objetos barato, como o Amazon S3. O data warehouse garantirá automaticamente que os dados acessados com frequência sejam movidos para o armazenamento “rápido”, para otimizar a velocidade da consulta.\n\n\n\nComo funciona um data warehouse?\nUm data warehouse pode conter vários bancos de dados. Dentro de cada banco de dados, os dados são organizados em tabelas e colunas. Dentro de cada coluna, você pode definir uma descrição dos dados, como número inteiro, campo de dados ou sequência. As tabelas podem ser organizadas dentro de esquemas, que você pode considerar como pastas. Quando os dados são consumidos, eles são armazenados em várias tabelas descritas pelo esquema. As ferramentas de consulta usam o esquema para determinar as tabelas de dados que serão acessadas e analisadas.\n\n\n\nQuais são os benefícios de usar um data warehouse?\nOs benefícios de um data warehouse incluem o seguinte:\n\nTomada de decisão adequada\nDados consolidados de várias fontes\nAnálise de dados históricos\nQualidade, consistência e precisão de dados\nSeparação do processamento analítico dos bancos de dados transacionais, o que melhora o desempenho dos dois sistemas\n\n\n\nComo os data warehouses, os bancos de dados e data lakes funcionam juntos?\nNormalmente, as empresas usam uma combinação de banco de dados, data lake e data warehouse para armazenar e analisar dados. A arquitetura de lake house do Amazon Redshift facilita essa integração.\nÀ medida que o volume e a variedade de dados aumentam, é vantajoso seguir um ou mais padrões comuns para trabalhar com dados em seu banco de dados, data lake e data warehouse:\n\n\nImagem (acima): armazenar os dados em um banco de dados ou datalake, preparar os dados, mover os dados selecionados para um data warehouse e executar relatórios.\n\nImage (acima): armazenar os dados em uma data warehouse, analisar os dados e compartilhá-los para usar com outros serviços de análise e machine learning.\nUm data warehouse é projetado especificamente para análises de dados, que envolvem a leitura de grandes quantidades de dados para compreender relações e tendências entre os dados. Um banco de dados é usado para capturar e armazenar dados, como o registro de detalhes de uma transação.\nAo contrário de um data warehouse, um data lake é um repositório centralizado para todos os dados, incluindo estruturados, semiestruturados e não estruturados. Um data warehouse exige que os dados sejam organizados em um formato tabular, onde o esquema torna-se necessário. O formato tabular é necessário para que o SQL possa ser usado para consultar os dados, mas nem todos os aplicativos exigem que os dados estejam em formato de tabela. Alguns aplicativos, como análise de big data, pesquisa de texto completo e machine learning, podem acessar dados mesmo que sejam “semiestruturados” ou completamente não estruturados.\n\n\n\nComparação entre data warehouses e data lakes\n\n\n\n\n\n\n\n\nCaracterísticas\nData warehouse\nData lake\n\n\n\n\nDados\nDados relacionais de sistemas transacionais, bancos de dados operacionais e aplicativos de linha de negócios\nTodos os dados, incluindo estruturados, semiestruturados e não estruturados\n\n\nEsquema\nGeralmente projetado antes da implementação do data warehouse, mas também pode ser gravado no momento da análise\n(esquema na gravação ou esquema na leitura)\nGravado no momento da análise (esquema na leitura)\n\n\nPreço/performance\nResultados de consulta mais rápidos, usando armazenamento local\nResultados da consulta cada vez mais rápidos usando armazenamento de baixo custo e desacoplamento de computação e armazenamento\n\n\nQualidade dos dados\nDados altamente organizados, que representam a versão central da verdade\nQuaisquer dados, organizados ou não (ou seja, dados brutos)\n\n\nUsuários\nAnalistas de negócios, cientistas de dados e desenvolvedores de dados\nAnalistas de negócios (usando dados organizados), cientistas de dados, desenvolvedores de dados, engenheiros de dados e arquitetos de dados\n\n\nAnálises\nGeração de relatórios em lote, BI e visualizações\nMachine learning, análise exploratória, descoberta de dados, streaming, análise operacional, big data e criação de perfil\n\n\n\n\n\nComparação entre data warehouses e bancos de dados\n\n\n\n\n\n\n\n\nCaracterísticas\nData warehouse\nBanco de dados transacional\n\n\n\n\nCargas de trabalho adequadas\nAnálises, relatórios e big data\nProcessamento de transações\n\n\nFonte de dados\nDados coletados e normalizados de diversas fontes\nDados capturados no estado em que se encontram, de uma única fonte, como um sistema transacional\n\n\nCaptura de dados\nOperações de gravação em massa, executadas normalmente em uma programação de lotes pré-determinada\nOtimizado para operações contínuas de gravação à medida que novos dados são disponibilizados para maximizar o throughput das transações\n\n\nNormalização de dados\nEsquemas desnormalizados, como Star ou Snowflake\nEsquemas estáticos altamente normalizados\n\n\nArmazenamento de dados\nOtimizado para simplicidade de acesso e alto desempenho de consultas usando armazenamento colunar\nOtimizado para operações de gravação de alto throughput em um único bloco físico orientado a linhas\n\n\nAcesso aos dados\nOtimizado para minimizar a E/S e maximizar o throughput de dados\nGrandes volumes de pequenas operações de leitura\n\n\n\n\n\nComo um data mart se compara a um data warehouse?\nUm data mart é um data warehouse que atende às necessidades de uma equipe ou unidade de negócios específica, como finanças, marketing ou vendas. O data mart é menor, mais focado e pode conter resumos de dados para atender melhor à comunidade de usuários. Um data mart também pode ser uma parte de um data warehouse.\n\n\n\nComparação entre data warehouses e data marts\n\n\n\nCaracterísticas\nData warehouse\nData mart\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaracterísticas\nData warehouse\nData mart\n\n\n\n\nEscopo\nVárias áreas centralizadas e integradas\nUma área específica e descentralizada\n\n\nUsuários\nDe toda a organização\nUma única comunidade ou departamento\n\n\nFonte de dados\nMuitas fontes\nUma ou poucas fontes, ou uma parte dos dados já coletados em um data warehouse\n\n\nTamanho\nGrande, pode variar de centenas de gigabytes a petabytes\nPequeno, normalmente até algumas dezenas de gigabytes\n\n\nProjeto\nDe cima para baixo\nDe baixo para cima\n\n\nDetalhes dos dados\nDados completos e detalhados\nPode manter dados resumidos"
  },
  {
    "objectID": "data_warehouse.html#data-warehouse---brasil-escola",
    "href": "data_warehouse.html#data-warehouse---brasil-escola",
    "title": "Conceitos de Data Warehouse",
    "section": "Data Warehouse - Brasil Escola",
    "text": "Data Warehouse - Brasil Escola\n\n5.1. DATA WAREHOUSE\nO conceito de Data Warehouse surgiu da necessidade das organizações em integrar os dados de diferentes servidores e máquinas em apenas um local e que essas informações servissem para que fossem gerados relatórios para as análises da empresa. O DW usa um modelo relacional dimensional, isto é, as informações estão dispostas de formas intuitivas, facilitando o acesso e a geração de resultados.\nOutro fator determinante para o desenvolvimento de um armazém de dados é o fato de que os modelos tradicionais, gerados pelos sistemas ERP, estão estruturados de forma transacional, o que dificulta gerar informações para as análises de resultados. De acordo com Singh, 2001: “O DW é a área de armazenamento de dados históricos e integrados destinados a sistemas de suporte à decisão”.\nAssim, Barbieri, 2001, conclui que:\nPode ser definido como um banco de dados, destinado a sistemas de apoio à tomada de decisão e cujos dados foram armazenados em estruturas lógicas dimensionais, possibilitando o seu processamento analítico por ferramentas especiais”.\nAs decisões para a utilização de um DW, parte do princípio que as informações precisam ser confiáveis para que as decisões não sejam tomadas de forma errônea. Como afirma Kimball, 2002: “Queremos que as pessoas usem informações para apoiar decisões mais baseadas em fatos”. As consultas e relatórios são acessados diretamente no DW, evitando dados sem confiabilidade dos provedores de informações originais.\n\n5.1.1. Características\n\n\n5.1.2. Orientado por Assunto\nOs bancos de dados transacionais, comumente possuem todos os dados das organizações dispostas em tabelas, isso faz com que os dados nem sempre serão de fácil análise. Os DW, por sua vez são orientados aos assuntos mais pertinentes para as empresas, como a análise de mercado de determinado produto ou veículo. Na figura 1, nota-se a diferença entre sistemas operacionais comum e após a criação do DW.\nFigura 1: Orientada por assunto\n\nFonte: INMON, 2005.\nBarbieri, 2001, nos alerta que a falta e objetivo, são primordiais para o fracasso de qualquer projeto, mas isso ainda é mais agravante no que diz respeito ao DW. É preciso também pensar no futuro, se necessário uma expansão no mesmo isso deve acontecer de forma relativamente fácil, pois se o projeto for mal estrutura, pode ser que o projeto futuramente tenha que partir do zero.\n\n\n5.1.3. Integrado\nÉ comum as organizações possuírem mais de uma representante, ou filial, sendo para desenvolver as mesmas ações ou de diferente tipo, fazendo com que em certos momentos as informações serão escritas de formas diferentes a qual está na matriz, ou representante principal. Ou então o sistema ERP é diferente, isso gera inconsistência nos dados, já que as informações são provenientes de mais de uma fonte.\nUma das características mais importantes do DW é a capacidade de ler essas informações e armazená-las de forma confiável. Independe de onde vem as informações, ou da data em que foi criado, a integração no DW sempre será consistente, por exemplo. Na inserção do CPF de clientes de um grupo de empresas, na organização X o CPF é inserido da seguinte forma: 999.999.999-99 já na empresa Y insere assim: 99999999999, a princípio pode ser uma questão simples, mas no momento em que for cruzar as informações, não será possível “linkar” um CPF com o outro. O DW irá transformar essas informações num modelo único.\n\n\n5.1.4. Histórico\nO que influência o tamanho do DW é principalmente as informações históricas das organizações, a forma mais eficaz de análise de tendência é tendo informações de anos anteriores para a comparação. Os dados históricos dos DW podem ter facilmente mais de 5 anos. Nos sistemas tradicionais, as informações remetem a posição atual dos dados no momento da pesquisa, ou num curto período de tempo, não sendo possível uma análise mais a fundo. É comum nos sistemas tradicionais a remoção de dados antigos para a liberação de espaço no banco de dados, isso não ocorre no DW pois as informações sempre serão importantes para as análises e projeção do futuro.\n\n\n5.1.5. Não Volátil\nHá apenas duas operações executadas no DW, a primeira delas é a transação de carga dos dados provenientes dos sistemas provedores de informações e a segunda é o processo de leitura dos dados para a geração de relatórios. Não é possível a escrita de dados nas dimensões do DW diretamente como acontece nos bancos de dados tradicionais, sendo apenas para a leitura, fazendo com que as informações permaneçam estáveis mesmo após longos períodos de tempo.\n\n\n5.1.6. Modelos\nTodas as características do DW, bem como suas diferenças aos sistemas tradicionais passam inteiramente pelos modelos a serem usados para a construção do novo repositório de dados. Os modelos, a grosso modo é a forma como os dados serão organizados e estruturados, como as entidades serão conectadas e como irão interagir entre si.\nAs opções de estrutura no DW variam das necessidades de cada caso, as mais comuns atualmente são o Modelo Estrela (Star Schema) e o Modelo Floco de Neve (Snow Flake), cada um com sua característica e limitação referente ao outro.\n\n\n5.1.7. Modelo Estrela\nO modelo estrela foi proposto por Ralph Kimball, para ser um modelo altamente redundante, onde todas as descrições seriam repetidas em cada dimensão. Sua estrutura é composta por uma tabela central de Fatos e um conjunto de tabelas ligadas a ela, que são chamadas de dimensões. As dimensões são compostas por eventos ou características do mesmo, enquanto a tabela fato, como o próprio nome diz, armazena os fatos ocorridos, por exemplo, o Fato é a venda de um veículo, as dimensões são as informações dessa venda, como a data que ocorreu, veículo vendido, valor da venda e assim por diante. Na figura 2 pode se verificar a estrutura do modelo estrela.\nFigura 2: Modelo Estrela.\n\n Fonte: MACHADO, 2004.\nSegundo Singh (2001) uma característica importante desse modelo é fato de suas dimensões serem desnormalizadas, isto gera várias duplicidades no banco, mas também garante confiabilidade nas consultas. Barbieri, 2001, cita como desvantagem do modelo estrela o fato de ele não ter uma perfeita coesão entre os Data Marts e um esforço redobrado na extração de dados, já que várias informações são duplicadas.\n\n\n5.1.8. Modelo Floco de Neve\nO modelo floco de neve também possui uma tabela Fatos ligada as entidades de dimensões, porém, ao contrário do que ocorre com o modelo estrela, há entidades relacionando entre si, isto diminui drasticamente o tamanho do DW, porém as consultas podem se tornar um pouco complicadas. Na figura 3 observa-se a estrutura do modelo Floco de neve.\nFigura 3: Modelo Floco de neve\n\nFonte: MACHADO, 2004.\nSegundo Machado (2004), O modelo floco de neve é o resultado da decomposição de uma ou mais dimensões que possuem hierarquias entre seus membros. O que difere também do modelo estrela é o fato das dimensões serem normalizadas, não ocorrendo as duplicidades que ocorre no modelo estrela, SINGH (2001) completa dizendo que normalizando os dados das tabelas dimensionais de um modelo estrela transforma o mesmo em um modelo floco de neve.\n\n\n5.1.9. Estrutura Do Data Warehouse\nSegundo o Kimball group, grupo especializado na concepção de Data Warehouse para Business Intelligence, a estrutura base para um Data warehouse seria, composta por três componentes, Data Sources, Data Staging Area e Data Presentation Area. Na figura 4 observa-se a organização básica de um DW.\nFigura 4: DW\n\nFonte: kimballgroup, 2002\n\n\n5.1.10. Data Source\nSão De Onde As Informações Serão Extraídas, A Fonte Dos Dados, Independentemente do servidor que está em uso, por exemplo, uma organização possui duas fontes de dados, uma baseada em SGBD Oracle e a outra em SQL server, o DW tem por função integrar esses dados em apenas um local de forma que possam ser cruzadas informações.\n\n\n5.1.11. Data Staging\nÉ onde estão armazenados os dados, é um meio termo entre o sistema operacional e a camada de apresentação, fazendo uso de conjuntos de processos chamados de ETL (Extração, Transformação e carregamento), ver seção 4.1.6 É onde ocorre a “higienização” dos dados, onde são definidas as estruturas, os fatos e as dimensões do DW. O Data Staging, não é de acesso livre aos usuários, pois não gera relatórios e nem consultas, ficando a cargo do Data Presentation Area.\nKIMBALL (2002), indica o requisito do Data Staging:\nO requisito de arquitetura chave para a Data Staging é que ela esteja fora do alcance dos usuários de negócios, não fornecendo serviços de consulta e apresentação.”\nFigura 5: Staging area\n\nFonte: MACHADO, 2004.\n\n\n5.1.12. Data Presentation Area\nÉ onde os dados estão organizados e prontos para serem acessados pelos usuários. Com base num modelo dimensional, os dados são acessados de forma intuitiva e as informações são consistentes. As ferramentas que possuem essa finalidade são conectadas ao cubo OLAP (ver seção 4.2) e com auxílio das operações apresentam os relatórios para o usuário final.\n\n\n5.1.13. Data Mart\nTendo como base o exemplo citado referente a venda de um determinado veículo, é possível extrair as informações dessa venda, porém se necessário obter várias vendas, com vários filtros essas informações tendem a se tornarem complexas para se analisar e consequentemente para tomar decisões, em razão disso, a ideia de um diretório exclusivo a determinado assunto dentro do próprio DW, ou extraindo informações diretamente das fontes de dados, acabou se tornando bastante necessário.\nSegundo Silva (2003), há um consenso sobre Data marts:\n“Há um consenso entre os fornecedores de soluções de Data Warehouse. A idéia é começar pequeno, mas pensando grande. E é o que está acontecendo. Na maioria dos casos, as empresas que optam pelo Data Warehouse iniciam o processo a partir de uma área específica da empresa para depois ir crescendo aos poucos.”\nBarbieri (2001), nos elucida que, Data Mart é um depósito de dados que atende as áreas especificas da empresa, ou seja, separa por assuntos os dados coletados, Primak (2008) complementa dizendo que pode-se dividir um DW em vários Data Marts, com seus determinados assuntos e diminuindo o tempo de resposta e facilitando o acesso a essas informações, no cenário da venda de veículos, temos a tabela principal, que no caso é a “fatovendas” que tem todas as informações de vendas de determinado veículo, mas é possível também a criação de Data Marts específicos, como “fatorh”, que terá as informações do recursos humanos da organização, “fatofluxo”, responsável por catalogar o fluxo de clientes e assim por diante, mas ainda assim, possibilitando cruzar essas informações utilizando operações (ver seção 4.2.1), por exemplo, é possível emitir um dashboard das vendas pelo fluxo de clientes, podendo ter ideia de quantos clientes são necessários para finalizar uma venda, ou então traçar uma linha do tempo das vendas, pelo valor investido em marketing naquele período. Na figura 6, nota-se como se estrutura o Data Mart em um DW.\nFigura 6: Organização do Data Mart\n\nFonte: NERY, 2007.\n\n\n5.1.14. Data Mining\nA diferença básica do BI para o Data mining se refere a quem é direcionado a atuação. O BI tem como base fornecer as informações para o nível estratégico da organização, onde se tomam as decisões a níveis gerenciais, já o Data mining fornece as informações em níveis menores, são usadas principalmente na área de atuação, ou no plano tático da empresa.\nO DM é constituído por um conjunto de 3 conceitos básicos para o seu sucesso, são eles:\n\nEstatística\nInteligencia artificial\nMachine learning\n\nO Machine Learning nada mais é que a combinação dos dois primeiros conceitos, é a grosso modo o fato do DM “aprender com os dados”, com cálculos estatísticos e com uma boa inteligência artificial é possível que o software trace caminhos sozinhos para a obtenção de resultados.\n\n\n5.1.15. ETL – Extract, Transform And Load\nÉ o processo de extração dos dados de fontes externas para o Data Warehouse, transformando suas tabelas e informações em dimensões e fatos no DW e carregando todas as informações de forma “limpa” e consistente no banco em questão.\nFigura 7: ETL\n\nFONTE: CANALTECH, 2014.\nSegundo KIMBALL (1998), ETL é o Conjunto de processos pelos quais os dados de origem operacional são preparados para o Data Warehouse. KIMBALL (1998) completa, é o processo mais crítico e demorado na construção de um DW, podendo levar até 60% do total de horas da implementação do projeto. Pois os modelos relacionais, nem sempre dispõe de uma arquitetura que facilite isso, possuindo também o fator de quantidade de dados que o sistema possui.\nBarbieri, 2001, diz que o conceito do processo ETL, pode ser dividido em:\n\nFiltro de dados\n\nOs bancos de dados comuns não são normalizados, isto faz com que os dados possam ter informações indesejáveis, o papel do ETL nessa etapa é filtrar e não carregar no DW essas informações.\n\nIntegração de dados\n\nFazer com que todas as informações a determinado assunto sejam correlacionadas, independente se ela está num sistema no banco de dados, ou em planilhas locais.\n\nCondensação de dados\n\nÉ condensar as informações de forma sumariada, ou seja, as vendas de determinado dia, precisam sempre estar presentes juntamente com as outras vendas do mesmo dia.\n\nConversão de dados\n\nCada banco de dados apresentam as informações de formas distintas, podendo ser uma virgula ao invés de um ponto, ou até o símbolo de moeda ser diferente, o ETL converte esse modelo em um outro modelo padrão do DW.\n\nDerivação de dados\n\nContinuação da conversão de dados, mas atuando apenas nas informações, não em como o modelo é empregado.\nUma boa ferramenta de ETL deve ser capaz de se adaptar as mais formas de banco de dados, suas linguagens e seus formatos. Atualmente a oferta de ferramentas de ETL é bastante elevada, empresas que possuem ferramentas de BI, ou ferramentas para a construção de um DW normalmente disponibilizam um software especifico para a função, como é o caso SQL Server Integration Services e do Pentaho Data Integration, Ferramentas ETL dos softwares da Microsoft e da Pentaho, respectivamente."
  },
  {
    "objectID": "fundamentos_bd.html",
    "href": "fundamentos_bd.html",
    "title": "Fundamentos de Banco de Dados",
    "section": "",
    "text": "b) Fundamentos de Banco de Dados: Conceitos: Sistemas de gerência de banco de dados (SGBD), Arquitetura, modelos lógicos e representação física; Organização física e métodos de acesso; Conceito de transação, concorrência, recuperação, integridade; Linguagens de definição (DDL) e manipulação de dados (DML) em SGBDs relacionais; Procedimentos (stored procedures), funções (functions), visões (views), visões materializadas (materialized views) e gatilhos (triggers), Linguagem de consulta estruturada (SQL, Avaliação de modelos de dados, Técnicas de engenharia reversa para criação e atualização de modelos de dados, Integração dos dados (ETL, Transferência de Arquivos e Integração via Base de Dados, Data Lakes e Soluções para Big Data, Diferenciação entre bancos relacionais, multidimensionais, documentos e grafos."
  },
  {
    "objectID": "gerenciamento_projetos.html",
    "href": "gerenciamento_projetos.html",
    "title": "Gerenciamento de Projetos",
    "section": "",
    "text": "f) Gerenciamento de Projetos: Conceitos básicos do PMBOK 7ª Edição, Metodologia SCRUM;"
  },
  {
    "objectID": "governanca_ti.html",
    "href": "governanca_ti.html",
    "title": "Governança de TI",
    "section": "",
    "text": "g) Governança de TI: ITIL versão 4 (ITIL 4): Operação de Serviços (Gerenciamento de Eventos, Gerenciamento de Incidentes, Gerenciamento de Problemas, Cumprimento de Requisições, Gerenciamento de Acessos), Desenho de Serviços (Gerenciamento de Níveis de Serviço, Gerenciamento de Capacidade, Gerenciamento de Disponibilidade, Gerenciamento de Continuidade de Serviços de TI, Gerenciamento de Continuidade de Negócio), Transição de Serviços (Gerenciamento de Configuração e Ativos de Serviços de TI, Gerenciamento de Liberação e Implantação, Gerenciamento de Mudanças), Melhoria Contínua de Serviços, Métricas (Fatores Críticos de Sucesso - CSFs, Índices Chave de Performance - KPIs)."
  },
  {
    "objectID": "governanca_ti.html#o-que-é-o-sistema-itil",
    "href": "governanca_ti.html#o-que-é-o-sistema-itil",
    "title": "Governança de TI",
    "section": "O que é o Sistema ITIL",
    "text": "O que é o Sistema ITIL\nO Sistema ITIL é um conjunto de boas práticas de gerenciamento de serviços de tecnologia de informação. Ele está de acordo com a norma ISO/IEC 20000, primeira padronização da International Organization for Standardization (ISO) voltada exclusivamente para a gestão de TI.\nITIL é a sigla para Information Technology Infrastructure Library – que pode ser traduzido para “biblioteca de infraestrutura de tecnologia da informação”.\nO sistema foi desenvolvido pela Agência Central de Computação e Telecomunicações (CCTA, na sigla em inglês) do Reino Unido na década de 1980, com o objetivo de estabelecer um padrão de segurança e confiabilidade na gestão de processos de TI, garantindo assim uma boa experiência para os usuários.\nPara isso, o modelo descreve boas práticas de infraestrutura, manutenção e operações. As orientações estão alinhadas aos métodos ágeis, ao DevOps e ao Lean, bastante utilizados por times de tecnologia.\nDesde que o ITIL foi proposto pela CCTA, o sistema passou por quatro grandes atualizações:\n\nITIL v1\nA primeira versão do ITIL era voltada para as agências governamentais, que começavam a se informatizar na década de 1980. Era, literalmente, uma biblioteca: uma coleção de livros físicos que chegou a mais de 30 volumes em 1996.\n\n\nITIL v2\nNos anos 2000 foi lançada a segunda versão do ITIL. Os 30 volumes foram condensados em 9, mas ainda eram voltados para entidades do governo britânico.\n\n\nITIL v3\nCom a popularização do modelo em empresas e demais entidades privadas, foi lançada uma nova atualização em maio de 2007.\nO ITIL v3 era descrito em 5 livros, que reuniam 26 processos e 4 funções. A maior inovação foi o conceito do Ciclo de Vida de Serviço (CVS), que era composto por dois componentes básicos:\n\nNúcleo do ITIL: conjunto de melhores práticas que podem ser adotadas por todas as organizações que prestam serviços ao negócio;\nGuias complementares do ITIL: boas práticas complementares reunidas em publicações específicas para diferentes setores da indústria, modelos operacionais e arquiteturas de TI.\n\nEm 2011, oITIL v3 ganhou uma grande atualização para dar clareza a conceitos e adicionar novas práticas ao CVS.\nÉ nesse período também que a CCTA foi incorporada ao Escritório de Comércio Governamental (OGC, na sigla em inglês), entidade do Reino Unido responsável por promover a eficiência nos processos de negócios do Estado.\nO sistema ITIL passou a ser atualizado pelo OGC até 2013, quando uma joint venture entre o governo britânico e a empresa Capita, a Axelos, assumiu o framework. Em 2021, a Axelos passou a fazer parte do grupo PeopleCert.\n\n\nITIL 4\nA quarta e última atualização do ITIL veio em fevereiro de 2019, com a publicação do livro “ITIL Foundations”.\nO modelo de gestão de TI foi alterado para atender as necessidades da Era Digital, com foco na criação de valor para os usuários, na condução de estratégias de negócios e na adaptação à transformação digital.\n\n\nAs principais mudanças do ITIL 4\nO ITIL 4 foi desenvolvido em conjunto entre a Axelos e a comunidade de profissionais de TI para adaptar o ITIL v3 às mudanças cada vez mais aceleradas da Era Digital.\nA principal mudança é a maior flexibilidade na execução dos processos. Na versão anterior, havia um certo engessamento no CVS, que dependia de uma série de estágios para ser executado.\nO ITIL 4 propõe o Sistema de Valor de Serviço (SVS) para alterar esse cenário. O SVS é um conjunto de componentes e atividades de uma empresa que possibilita a criação de valor.\nA flexibilidade vem da criação de um ecossistema entre organização, fornecedores, stakeholders e clientes. Todos devem atuar juntos para manter o sistema funcionando.\nOs componentes do SVS são:\n\nCadeia de valor de serviço: modelo operacional flexível para a entrega e aprimoramento contínuo de serviços. Tem como atividades principais planejar, melhorar, engajar, desenhar, construir e entregar;\n34 práticas que atualizam os processos do ITIL v3;\nGovernança: conjunto de normas e práticas que são a base para a definição de processos internos, de acordo com as exigências do setor e os valores da organização. Facilita a integração com outros frameworks como o COBIT.\nMelhoria contínua.\n\n\n\n\nRepresentação gráfica do Sistema de Valor de Serviços do ITIL 4.\n\n\nOutra mudança importante é a inclusão de tecnologias emergentes, como Cloud Computing, Infraestrutura como Serviço (IaaS), Machine Learning e blockchain.\nO ITIL 4 também introduziu novos conceitos, que você vai conhecer em detalhes a seguir.\n\n\nOs princípios do ITIL 4\nSão 7 princípios que devem orientar os profissionais de TI na adoção do SVS e, assim, adaptar o ITIL à realidade de suas empresas:\n\nConcentrar-se no valor;\nComeçar por onde você está;\nAvançar iterativamente com feedback;\nColaborar e promover a visibilidade;\nPensar e trabalhar pensando no todo;\nManter os processos simples e práticos;\nOtimizar e automatizar constantemente.\n\n\n\nAs 4 dimensões do ITIL 4\n\n\n\nRepresentação gráfica das dimensões do ITIL 4.\n\n\nAs dimensões do ITIL são necessárias para a entrega de valor ao cliente, além de facilitar a visão holística da gestão de serviços. Todas são afetadas por fatores internos e externos à organização.\nAs 4 dimensões são:\n\nOrganizações e pessoas;\nInformação e tecnologia;\nParceiros e fornecedores;\nFluxos de valor e processos.\n\n\n\nAs 34 práticas do ITIL 4\nAs práticas do ITIL 4 são “um conjunto de recursos necessários para realizar o trabalho ou cumprir um objetivo”. Elas têm como objetivo dar uma visão holística do sistema de serviços, ao considerar elementos como cultura, tecnologia, informações e gerenciamento de dados.\nA palavra “prática” também evita as ambiguidades do termo “processos”, que é usado no dia a dia das empresas em todos os departamentos. Hoje, o ITIL considera um processo como “um conjunto de atividades que transformam entradas em saídas”.\nAs práticas são divididas em 3 grandes grupos:\n\n\n1. Práticas gerais de gestão\n\nGerenciamento da estratégia\nGerenciamento da segurança da informação\nGerenciamento de fornecedor\nGerenciamento de mudança organizacional\nGerenciamento de projetos\nGerenciamento de relacionamento\nGerenciamento de riscos\nGerenciamento de talento e força de trabalho\nGerenciamento do conhecimento\nGerenciamento do portfólio\nGerenciamento financeiro dos serviços\nGestão da arquitetura\nMedição e reporte\nMelhoria contínua\n\n\n\n2. Práticas de gestão de serviço\n\nAnálise de negócio\nCentral de serviço\nDesenho de serviço\nGerenciamento de ativos de TI\nGerenciamento de capacidade e desempenho\nGerenciamento do catálogo de serviços\nGerenciamento de configuração de serviço\nGerenciamento de continuidade de serviço\nGerenciamento de disponibilidade\nGerenciamento de incidente\nGerenciamento de liberação\nGerenciamento de nível de serviço\nGerenciamento de problema\nGerenciamento de requisição de serviço\nHabilitação de mudança\nMonitoramento e gerenciamento de evento\nValidação e teste de serviço\n\n\n\n3. Práticas de gestão técnica\n\nDesenvolvimento e gerenciamento de software\nGerenciamento de implantação\nGerenciamento de infraestrutura e plataforma\n\nNenhuma prática está vinculada a um elemento do SVS nem é pré-requisito para a adoção de outras práticas. Não é obrigatório usar todas as 34 simultaneamente."
  },
  {
    "objectID": "governanca_ti.html#itil-4---prof.-gabriel-pacheco-youtube",
    "href": "governanca_ti.html#itil-4---prof.-gabriel-pacheco-youtube",
    "title": "Governança de TI",
    "section": "ITIL 4 - Prof. Gabriel Pacheco (Youtube)",
    "text": "ITIL 4 - Prof. Gabriel Pacheco (Youtube)"
  },
  {
    "objectID": "governanca_ti.html#itil-4---prof.-adriano-martins-antonio-youtube",
    "href": "governanca_ti.html#itil-4---prof.-adriano-martins-antonio-youtube",
    "title": "Governança de TI",
    "section": "ITIL 4 - Prof. Adriano Martins Antonio (Youtube)",
    "text": "ITIL 4 - Prof. Adriano Martins Antonio (Youtube)\nPrincípios Orientadores da ITIL 4 (Preparatório para o exame ITIL 4 Foundation)\nSistema de Valor de Serviços da ITIL 4 - ITSM\nSimulado 1 Comentado da ITIL 4 Foundation (40 Perguntas - Parte 1)\nSimulado 1 Comentado da ITIL 4 Foundation (40 Perguntas - Parte 2)\nSimulado 2 Comentado da ITIL 4 Foundation (40 Perguntas - Parte 1)\nSimulado 2 Comentado da ITIL 4 Foundation (40 Perguntas - Parte 2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resumo",
    "section": "",
    "text": "Material de estudo para concurso do BRDE, cargo Analista de Sistemas – Subárea Ciência de Dados.\nEdital\nData da prova: 12/03/2023\nREQUISITO: Diploma de graduação, devidamente registrado, em pelo menos um dos seguintes cursos de nível superior: Análise de Sistemas; Tecnologia da Informação; Sistemas de Informação; Processamento de Dados; Ciência da Computação; Engenharia da Computação; Engenharia de Sistemas; Bacharelado em Informática; Administração com Ênfase em Análise de Sistemas."
  },
  {
    "objectID": "index.html#conhecimentos-específicos",
    "href": "index.html#conhecimentos-específicos",
    "title": "Resumo",
    "section": "Conhecimentos Específicos",
    "text": "Conhecimentos Específicos\n\nCargo C07: Analista de Sistemas – Ciência de Dados\na) Banco de Dados: Fundamentos de administração de dados: Segurança; Modelagem de dados: Modelo entidade-relacionamento (entidades, atributos, chaves e relacionamentos) e Normalização;\nb) Fundamentos de Banco de Dados: Conceitos: Sistemas de gerência de banco de dados (SGBD), Arquitetura, modelos lógicos e representação física; Organização física e métodos de acesso; Conceito de transação, concorrência, recuperação, integridade; Linguagens de definição (DDL) e manipulação de dados (DML) em SGBDs relacionais; Procedimentos (stored procedures), funções (functions), visões (views), visões materializadas (materialized views) e gatilhos (triggers), Linguagem de consulta estruturada (SQL, Avaliação de modelos de dados, Técnicas de engenharia reversa para criação e atualização de modelos de dados, Integração dos dados (ETL, Transferência de Arquivos e Integração via Base de Dados, Data Lakes e Soluções para Big Data, Diferenciação entre bancos relacionais, multidimensionais, documentos e grafos.\nc) Análise de dados e informações: Dado, informação, conhecimento e inteligência; Conceitos, fundamentos, características, técnicas e métodos de Business Intelligence (BI); Mapeamento de fontes de dados, Dados estruturados e dados não estruturados, Conceitos de OLAP e suas operações, Conceitos de Data Warehouse. Técnicas de modelagem e otimização de bases de dados multidimensionais, Construção de relatórios e dashboards interativos em ferramentas de BI, Manipulação de dados em planilhas, Geração de insights a partir de relatórios e dashboards, BI como suporte a processos de tomada decisão, Conceitos Básicos em Séries Temporais, Conceitos Básicos de estatística descritiva, probabilística e testes de hipótese, Manipulação, tratamento e visualização de dados, Tratamento de dados faltantes, Tratamento de dados categóricos, Normalização numérica, Detecção e tratamento de outliers;\nd) Aprendizado de máquina (machine learning): Regressão Linear e Regressão Logística, Classificação, Métricas de avaliação, Overfitting e underfitting de modelos, Regularização; Seleção de modelos: Erro de Generalização, Validação Cruzada, Conjuntos de Treino, Validação e Teste; Conceitos de aprendizado não supervisionado, Clustering, Árvores de decisão e random forests, Máquina de suporte de vetores (SVM), Naive Bayes, K-NN;\ne) Plataforma de BI Microsoft: SQL Server, SQL Server Management Studio, SQL Server Integration Services, SQL Server Analysis Services, Power BI Report Server, Power BI Desktop, Serviço Power BI em Nuvem;\nf) Gerenciamento de Projetos: Conceitos básicos do PMBOK 7ª Edição, Metodologia SCRUM;\ng) Governança de TI: ITIL versão 4 (ITIL 4): Operação de Serviços (Gerenciamento de Eventos, Gerenciamento de Incidentes, Gerenciamento de Problemas, Cumprimento de Requisições, Gerenciamento de Acessos), Desenho de Serviços (Gerenciamento de Níveis de Serviço, Gerenciamento de Capacidade, Gerenciamento de Disponibilidade, Gerenciamento de Continuidade de Serviços de TI, Gerenciamento de Continuidade de Negócio), Transição de Serviços (Gerenciamento de Configuração e Ativos de Serviços de TI, Gerenciamento de Liberação e Implantação, Gerenciamento de Mudanças), Melhoria Contínua de Serviços, Métricas (Fatores Críticos de Sucesso - CSFs, Índices Chave de Performance - KPIs)."
  },
  {
    "objectID": "index.html#conhecimentos-básicos",
    "href": "index.html#conhecimentos-básicos",
    "title": "Resumo",
    "section": "Conhecimentos Básicos",
    "text": "Conhecimentos Básicos\n\nLÍNGUA PORTUGUESA\n\n1. Leitura e compreensão de textos:\n1.1 Assunto. 1.2 Estruturação do texto. 1.3 Ideias principais e secundárias. 1.4 Relação entre as ideias. 1.5 Efeitos de sentido. 1.6 Figuras de linguagem. 1.7 Recursos de argumentação. 1.8 Informações implícitas: pressupostos e subentendidos. 1.9 Coesão e coerência textuais.\n\n\n2. Léxico:\n2.1 Significação de palavras e expressões no texto. 2.2 Substituição de palavras e de expressões no texto. 2.3 Estrutura e formação de palavras.\n\n\n3. Aspectos linguísticos:\n3.1 Relações morfossintáticas. 3.2 Ortografia: emprego de letras e acentuação gráfica sistema oficial vigente (inclusive o Acordo Ortográfico vigente, conforme Decreto 7.875/12). 3.3 Relações entre fonemas e grafias. 3.4 Flexões e emprego de classes gramaticais. 3.5 Vozes verbais e sua conversão. 3.6 Concordância nominal e verbal. 3.7 Regência nominal e verbal (inclusive emprego do acento indicativo de crase). 3.8 Coordenação e subordinação: emprego das conjunções, das locuções conjuntivas e dos pronomes relativos. 3.9 Pontuação.\n\n\n\nLÍNGUA INGLESA\n\nReading Comprehension.\nSimple and compound sentences: a. Noun clauses; b. Relative clauses;\n\n\n\nClause combinations – coordinators and subordinators; d. Conditional sentences;\n\n\n\nNouns: a. Compound nouns; b.Countable/ uncountable nouns;\nArticles.\nPronouns.\nAdjectives.\nAdverbs.\nPrepositions and phrasal verbs.\nVerbs.\nWord order.\nVocabulary and false friends.\nCollocations.\nPronunciation.\n\n\n\nRACIOCÍNIO LÓGICO/ANALÍTICO/QUANTITATIVO\n\nEstrutura lógica de relações arbitrárias entre pessoas, lugares, objetos ou eventos fictícios; deduzir novas informações das relações fornecidas e avaliar as condições usadas para estabelecer a estrutura daquelas relações. Diagramas lógicos.\nProposições e conectivos: Conceito de proposição, valores lógicos das proposições, proposições simples, proposições compostas. Operações lógicas sobre proposições: Negação, conjunção, disjunção, disjunção exclusiva, condicional, bicondicional.\nConstrução de tabelas-verdade. Tautologias, contradições e contingências. Implicação lógica, equivalência lógica, Leis De Morgan. Argumentação e dedução lógica.\nSentenças abertas, operações lógicas sobre sentenças abertas. Quantificador universal, quantificador existencial, negação de proposições quantificadas.\nArgumentos Lógicos Dedutivos; Argumentos Categóricos."
  },
  {
    "objectID": "integracao_dados.html",
    "href": "integracao_dados.html",
    "title": "Integração de Dados",
    "section": "",
    "text": "b) Fundamentos de Banco de Dados: Integração dos dados (ETL, Transferência de Arquivos e Integração via Base de Dados, Data Lakes e Soluções para Big Data, Diferenciação entre bancos relacionais, multidimensionais, documentos e grafos"
  },
  {
    "objectID": "integracao_dados.html#data-lake",
    "href": "integracao_dados.html#data-lake",
    "title": "Integração de Dados",
    "section": "Data Lake",
    "text": "Data Lake\n\nO que é um data lake?\nUm data lake é um repositório centralizado que permite armazenar todos os seus dados estruturados e não estruturados em qualquer escala. Você pode armazenar seus dados como estão, sem precisar primeiro estruturá-los e executar diferentes tipos de análise, desde painéis e visualizações até processamento de big data, análise em tempo real e machine learning para orientar melhores decisões.\n\n\nPor que você precisa de um data lake?\nAs organizações que geram valor empresarial com êxito a partir de seus dados superarão seus pares. Uma pesquisa da Aberdeen revelou que as organizações que implementaram um data lake superaram em 9% a performance de empresas semelhantes no crescimento orgânico da receita. Esses líderes foram capazes de fazer novos tipos de análise, como machine learning em novas fontes, como arquivos de log, dados de fluxos de cliques, mídia social e dispositivos conectados à Internet armazenados no data lake. Isso os ajudou a identificar e agir de acordo com as oportunidades de crescimento dos negócios mais rapidamente, atraindo e retendo clientes, aumentando a produtividade, mantendo dispositivos proativamente e tomando decisões informadas.\n\n\n\nData lakes comparados a data warehouses: duas abordagens diferentes\nDependendo dos requisitos, uma organização típica exigirá um data warehouse e um data lake, pois atendem a diferentes necessidades e casos de uso.\nUm data warehouse é um banco de dados otimizado para analisar dados relacionais provenientes de sistemas transacionais e aplicações de linha de negócios. A estrutura de dados e o esquema são definidos antecipadamente para otimizar consultas SQL rápidas, em que os resultados são normalmente usados para relatórios e análises operacionais. Os dados são limpos, enriquecidos e transformados para que possam atuar como a “fonte única da verdade” em que os usuários podem confiar.\nUm data lake é diferente porque armazena dados relacionais de aplicações de linha de negócios e dados não relacionais de aplicativos móveis, dispositivos IoT e mídias sociais. A estrutura dos dados ou esquema não é definida quando os dados são capturados. Isso significa que você pode armazenar todos os seus dados sem um design cuidadoso ou a necessidade de saber para quais perguntas você pode precisar de respostas no futuro. Diferentes tipos de análise em seus dados, como consultas SQL, análise de big data, pesquisa de texto completo, análise em tempo real e machine learning, podem ser usados para descobrir insights.\nÀ medida que as organizações com data warehouses veem os benefícios dos data lakes, elas evoluem seu warehouse para incluir data lakes e habilitar diversos recursos de consulta, casos de uso de ciência de dados e recursos avançados para descobrir novos modelos de informações. A Gartner chama essa evolução de “Solução de gerenciamento de dados para análise” ou “DMSA”.\n\n\n\n\n\n\n\n\nCaracterísticas\nData warehouse\nData lake\n\n\n\n\nDados\nRelacionais de sistemas transacionais, bancos de dados operacionais e aplicações de linha de negócios\nNão relacionais e relacionais de dispositivos de IoT, sites, aplicações móveis, mídia social e aplicações corporativas\n\n\nEsquema\nDefinido antes da implementação do DW (esquema na gravação)\nGravado no momento da análise (esquema na leitura)\n\n\nPreço/performance\nResultados de consulta mais rápidos, usando armazenamento de maior custo\nResultados de consulta ficando mais rápidos, usando armazenamento de menor custo\n\n\nQualidade dos dados\n\nDados altamente selecionados, que representam a versão central da verdade\nQuaisquer dados, selecionados ou não (ou seja, dados brutos)\n\n\n\nUsuários\nAnalistas de negócios\nCientistas de dados, desenvolvedores de dados e analistas de negócios (usando dados selecionados)\n\n\nAnálises\nGeração de relatórios em lote, BI e visualizações\nMachine learning, análises preditivas, descoberta de dados e criação de perfis\n\n\n\nOs elementos essenciais de uma solução de data lake e análise\nÀ medida que as organizações estão criando data lakes e uma plataforma de análise, elas precisam considerar vários recursos importantes, incluindo:\nMovimentação de dados\nOs data lakes permitem que você importe qualquer quantidade de dados que possa vir em tempo real. Os dados são coletados de várias fontes e movidos para o data lake em seu formato original. Esse processo permite escalar para dados de qualquer tamanho, economizando tempo na definição de estruturas de dados, esquemas e transformações.\n\nArmazene e catalogue dados com segurança\nOs data lakes permitem que você armazene dados relacionais, como bancos de dados operacionais e dados de aplicações de linha de negócios, e dados não relacionais, como aplicativos móveis, dispositivos IoT e mídias sociais. Eles também oferecem a capacidade de entender quais dados estão no lago por meio de crawling, catalogação e indexação de dados. Por fim, os dados devem ser protegidos para garantir que seus ativos de dados estejam protegidos.\n\n\nAnálises\nOs data lakes permitem que várias funções da organização, como cientistas de dados, desenvolvedores de dados e analistas de negócios, acessem dados com sua escolha de ferramentas e frameworks analíticos. Isso inclui frameworks de código aberto, como Apache Hadoop, Presto e Apache Spark, e ofertas comerciais de fornecedores de data warehouse e inteligência empresarial. Os data lakes permitem que você execute análises sem a necessidade de mover seus dados para um sistema de análise separado.\n\n\nMachine learning\nO data lakes permitirão que as organizações gerem diferentes tipos de insights, incluindo relatórios sobre dados históricos e machine learning, onde os modelos são criados para prever resultados prováveis e sugerir uma série de ações prescritas para alcançar o resultado ideal.\nO valor de um data lake\nA capacidade de aproveitar mais dados, de mais fontes, em menos tempo, e de capacitar os usuários a colaborar e analisar dados de diferentes maneiras leva a uma tomada de decisão melhor e mais rápida. Exemplos em que os data lakes agregaram valor incluem:\n\n\nMelhores interações com o cliente\nUm data lake pode combinar dados de clientes de uma plataforma de CRM com análise de mídia social, uma plataforma de marketing que inclui histórico de compras e tíquetes de incidentes para capacitar a empresa a entender o grupo de clientes mais lucrativo, a causa da perda de clientes e as promoções ou recompensas que aumentará a fidelidade.\n\n\nMelhorar as opções de inovação em P&D\nUm data lake pode ajudar suas equipes de P&D a testar hipóteses, refinar suposições e avaliar resultados, como escolher os materiais certos no design do produto, resultando em uma performance mais rápida, em pesquisas genômicas que levam a medicamentos mais eficazes ou no entendimento da disposição dos clientes de pagar por atributos diferentes.\n\n\nAumente as eficiências operacionais\nA Internet das Coisas (IoT) apresenta mais maneiras de coletar dados sobre processos como fabricação, com dados em tempo real provenientes de dispositivos conectados à Internet. Um data lake facilita o armazenamento e a execução de análises em dados de IoT gerados por máquina para descobrir maneiras de reduzir custos operacionais e aumentar a qualidade.  \nOs desafios dos data lakes\nO principal desafio com uma arquitetura de data lake é que os dados brutos são armazenados sem supervisão do conteúdo. Para que um data lake torne os dados utilizáveis, ele precisa ter mecanismos definidos para catalogar e proteger os dados. Sem esses elementos, os dados não podem ser encontrados ou confiáveis, resultando em um “pântano de dados”. Atender às necessidades de públicos mais amplos exige que os data lakes tenham governança, consistência semântica e controles de acesso.\n\n\nImplantar data lakes na nuvem\nData lakes são uma workload ideal para ser implantada na nuvem, porque a nuvem oferece performance, escalabilidade, confiabilidade, disponibilidade, um conjunto diversificado de mecanismos analíticos e enormes economias de escala. Uma pesquisa da ESG revelou que 39% dos entrevistados consideram a nuvem sua principal implantação para análise, 41% para data warehouses e 43% para Spark. Os principais motivos pelos quais os clientes perceberam a nuvem como uma vantagem para data lakes são: melhor segurança, tempo de implantação mais rápido, melhor disponibilidade, atualizações de recursos/funcionalidades mais frequentes, mais elasticidade, mais cobertura geográfica e custos vinculados à utilização real."
  },
  {
    "objectID": "integracao_dados.html#etl-sas",
    "href": "integracao_dados.html#etl-sas",
    "title": "Integração de Dados",
    "section": "ETL (SAS)",
    "text": "ETL (SAS)\nETL é um tipo de data integration em três etapas (extração, transformação, carregamento) usado para combinar dados de diversas fontes. Ele é comumente utilizado para construir um data warehouse. Nesse processo, os dados são retirados (extraídos) de um sistema-fonte, convertidos (transformados) em um formato que possa ser analisado, e armazenados (carregados) em um armazém ou outro sistema. Extração, carregamento, transformação (ELT) é uma abordagem alternativa, embora relacionada, projetada para jogar o processamento para o banco de dados, de modo a aprimorar a performance.\n\nHistória do ETL\nETL ganhou popularidade nos anos 1970, quando as organizações começaram a usar múltiplos repositórios ou bancos de dados para armazenar diferentes tipos de informações de negócios. A necessidade de integrar os dados que se espalhavam pelos databases cresceu rapidamente. O ETL tornou-se o método padrão para coletar dados de fontes diferentes e transformá-los antes de carregá-los no sistema-alvo ou destino.\nNo final dos anos 1980 e início dos 1990, os data warehouses entraram em cena. Sendo um tipo diferente de banco de dados, eles forneceram um acesso integrado a dados de múltiplos sistemas – computadores mainframes, minicomputadores, computadores pessoais e planilhas. Mas diferentes departamentos costumam usar diferentes ferramentas ETL com diferentes armazéns. Adicione isso a junções e aquisições, e muitas empresas acabam com distintas soluções ETL, que não foram integradas.\nCom o tempo, o número de formatos, fontes e sistemas de dados aumentou muito. Extrair, transformar e carregar é, hoje, apenas um dos vários métodos que as organizações utilizam para coletar, importar e processar dados. ETL e ELT são, ambos, partes importantes de uma estratégia ampla de data integration das empresas.\n\n\nQual a importância do ETL?\nHá anos, inúmeras empresas têm confiado no processo de ETL para obter uma visão consolidada dos dados que geram as melhores decisões de negócios. Hoje, esse método de integrar dados de múltiplos sistemas e fontes ainda é um componente central do kit de ferramentas de data integration de uma organização.\n\n\n\n\n\nExtract Transfrom Load - infographic\n\n\nO ETL é usado para mover e transformar dados de múltiplas fontes, e carregá-los em vários destinos, como o Hadoop.\n\nQuando utilizado com um data warehouse corporativo (dados em repouso), o ETL fornece o contexto histórico completo para a empresa;\nAo fornecer uma visão consolidada, o ETL facilita para os usuários corporativos a análise e a criação de relatórios sobre dados relevantes às suas iniciativas;\nO ETL pode melhorar a produtividade de profissionais analíticos, porque ele codifica e reutiliza processos que movem os dados sem que esses profissionais possuam a capacidade técnica de escrever códigos ou scripts;\nO ETL evoluiu ao longo do tempo para suportar os requisitos emergentes de integração para coisas como streaming data;\nAs organizações precisam tanto de ETL quanto ELT para unir dados, manter a precisão e fornecer a auditoria necessária para armazenar dados, criar relatórios e realizar análises. \n\n\n\nComo o ETL é usado?\nFerramentas centrais de ETL e ELT trabalham em conjunto com outras ferramentas de data integration e com outros vários aspectos do gerenciamento de dados – como data quality, data governance, virtualização e metadados. As utilizações populares de hoje incluem:\n\nETL e usos tradicionais\nETL é um método comprovado com o qual muitas empresas contam todos os dias – como varejistas, que precisam olhar os dados de vendas regularmente, ou operadoras de saúde procurando por um quadro preciso de seu uso. O ETL pode combinar e exibir dados de transações de um data warehouse ou outro banco de dados, de modo que eles estejam sempre prontos para analistas de negócios os visualizarem em um formato compreensível. O ETL também é utilizado para migrar dados de sistemas arcaicos para sistemas modernos, com diferentes formatos possíveis. É frequentemente usado para consolidar dados de fusões de empresas e para coletar e unir dados de fornecedores ou parceiros externos.\n\n\n\n\nETL com big data – transformações e adaptadores\nVence quem conseguir o maior número de dados. Embora isso não seja, necessariamente, uma verdade, ter acesso fácil a um amplo escopo de dados pode dar às empresas uma vantagem competitiva. Hoje, elas precisam de acesso a todo tipo de big data – vídeos, mídias sociais, a Internet das Coisas (IoT), logs do servidor, dados espaciais, dados abertos ou de crowdsource e muito mais. Fornecedores de ETL frequentemente adicionam novas transformações às suas ferramentas para suportar essas requisições emergentes e novas fontes de dados. Adaptadores oferecem acesso a uma ampla variedade de fontes de dados, e as ferramentas de data integration interagem com esses adaptadores para extrair e carregar dados de modo eficaz.\n\n\n\nETL para Hadoop – e mais\nO ETL evoluiu para oferecer suporte à integração entre muito mais que data warehouses tradicionais. Ferramentas avançadas de ETL podem carregar e converter dados estruturados e não-estruturados no Hadoop. Essas ferramentas leem e escrevem múltiplos arquivos em paralelo de, e para, Hadoop, simplificando como informações são fundidas em um processo de transformação comum. Algumas soluções incorporam bibliotecas de transformações ETL pré-construídas para os dados de transação e interação que são executados em Hadoop. ETL também oferece suporte à integração entre sistemas transacionais, bancos de dados operacionais, plataformas de BI, centralizadores master data management (MDM) e a nuvem.\n\n\n\nETL e acesso aos dados self-service\nData preparation self-service é uma tendência de rápido crescimento que coloca o poder de acesso, mistura e transformação de dados nas mãos dos usuários organizacionais e outros profissionais não-técnicos. Sendo específico em sua natureza, essa abordagem aumenta a agilidade organizacional e libera a TI de abastecer usuários com diferentes formatos de dados. Menos tempo é desperdiçado na preparação de dados e mais tempo é gasto na geração de insights. Consequentemente, tanto profissionais de TI ou de outros ramos da organização podem melhorar sua produtividade e as empresas podem escalonar seu uso de dados para tomarem decisões melhores.\n\n\n\nETL e data quality\nO ETL e outras ferramentas de data integration – utilizadas pra limpar, perfilar e auditar dados – garantem que os dados sejam confiáveis. As ferramentas ETL integram-se às de data quality, e fornecedores de ETL incorporam ferramentas relacionadas em suas soluções, como aquelas utilizadas para mapeamento e linhagem de dados.\n\n\n\nETL e metadados\nMetadados nos auxiliam a entender a linhagem dos dados (de onde eles vieram) e seu impacto em outros ativos de dados na organização. Conforme arquiteturas de dados se tornam mais complexas, é importante rastrear como os diferentes elementos de dados na sua organização são utilizados e relacionados. Por exemplo, se você adiciona o nome de uma conta do Twitter à sua base de dados de clientes, você vai precisar saber o que será afetado, como, por exemplo, tarefas, aplicações ou relatórios ETL.\n\n\nComo funciona?\nO ETL está intimamente relacionado a várias outras funções, processos e técnicas de data integration. Compreendê-las fornece uma visão mais clara de como o ETL funciona. \n\n\n\n\n\n\n\n\nSQL\nLinguagem de consulta estruturada (SQL, na sigla em inglês) é o método mais comum de acessar e transformar os dados de um database.\n\n\n\n\nTranformações, regras de negócios e adaptadores \n\nApós extrair os dados, o ETL utiliza regras de negócios para transformá-los em novos formatos. Os dados transformados são, então, carregados no destino.\n\n\nData mapping\nO mapeamento de dados (data mapping) é parte do processo de transformação. O mapeamento fornece instruções detalhadas para uma aplicação sobre como obter os dados necessários para processar. Ele também descreve qual campo de origem é mapeado para qual campo de destino. Por exemplo, o terceiro atributo de um feed de dados de atividades de um website pode ser o nome de usuário; o quarto pode ser o horário de quando a atividade aconteceu e o quinto pode ser o produto no qual o usuário clicou. Uma aplicação ou processo ETL, usando esses dados, teria que mapear esses mesmos campos ou atributos do sistema-fonte (por exemplo, a página de informações de atividade do site) no formato necessário pelo sistema de destino. Se o sistema de destino for um sistema de gestão de relacionamento com o cliente, ele pode primeiro armazenar o nome de usuário e o horário em quinto; ele pode nem sequer armazenar o produto. Nesse caso, uma transformação para formatar a data no formato esperado (e na ordem correta) pode acontecer enquanto os dados são lidos pela fonte e redigidos no destino.\n\n\nScripts\nETL é um método que automatiza os scripts (conjunto de instruções) que são executados no plano de fundo para mover e transformar os dados. Antes do ETL, scripts eram escritos individualmente em C ou COBOL para transferir dados entre sistemas específicos. Isso resultou em múltiplos bancos de dados executando diversos scripts. As primeiras ferramentas de ETL eram executadas em mainframes como um processo em lote. Ferramentas posteriores migraram para plataformas UNIX e PC. As organizações de hoje ainda utilizam tanto scripts quanto métodos de movimento programático de dados.\n\n\nETL versus ELT\nNo princípio, havia o ETL. Então, as empresas adicionaram o ELT, um método complementar. ELT extrai dados de um sistema-fonte, os carrega em um sistema de destino e, então, usa o poder de processamento do sistema-fonte para conduzir as transformações. Isso acelera o processamento de dados porque acontece onde os dados estão.\n\n\nData quality\nAntes que os dados sejam integrados, um ambiente de teste é normalmente criado onde eles possam ser limpos e padronizados (SP e São Paulo, Senhor e Sr. ou Bia e Beatriz), endereços possam ser verificados e duplicatas, removidas. Muitas soluções ainda são independentes, mas procedimentos de data quality podem agora ser executados como uma das transformações no processo de data integration.\n\n\nAgendando e processando\nFerramentas e tecnologias ETL podem fornecer tanto agendamento em lote quanto capacidades em tempo real. Elas podem também processar dados em altos volumes no servidor ou podem reduzir o processamento para o nível do banco de dados. Essa abordagem de processamento em um banco de dados, em vez de em um mecanismo especializado, evita duplicação dos dados e a necessidade de usar capacidades extras na plataforma do banco de dados.\n\n\nProcessamento em lote\nETL normalmente se refere a um processamento em lote que envolve a movimentação de grandes volumes de dados entre dois sistemas durante o que é chamado de “janela”. Nesse período de tempo determinado – por exemplo, entre o meio-dia e a uma da tarde – nenhuma ação pode ocorrer com o sistema-fonte, ou alvo, enquanto os dados são sincronizados. A maioria dos bancos realiza os processamentos em lote durante a noite para resolver transações que ocorrem durante o dia.\n\n\nWeb services\nWeb services são um método baseado na internet para fornecer dados ou funcionalidades a várias aplicações em tempo quase real. Esse método simplifica os processos de data integration e pode entregar, rapidamente, mais valor a partir dos dados. Por exemplo, imagine que um cliente entre em contato com a sua central de atendimento. Você poderia criar um web service que devolve o perfil completo do cliente em uma fração de segundo ao informar, simplesmente, um número de telefone para o web service que extrai os dados de múltiplas fontes ou de um hub MDM. Com um conhecimento mais rico do cliente, o atendente pode tomar decisões melhores sobre como interagir com esse cliente.\n\n\nMaster data management \n\nMDM é o processo de unir os dados para criar uma visão única deles, através de múltiplas fontes. Ele inclui tanto ETL quanto capacidades de data integration para misturar as informações e criar um “registro de ouro” ou um “melhor registro”.\n\n\n\nData virtualization\nVirtualização é um método ágil de misturar os dados para criar um panorama virtual sem movê-los. Data virtualization difere de ETL porque, embora mapeamento e combinação de dados ainda ocorram, não há a necessidade de uma tabela de teste física para armazenar os resultados. Isso porque o panorama é geralmente armazenado na memória e em cachê para melhorar a performance. Algumas soluções de data virtualization, como a SAS Federation Server, fornecem funções dinâmicas de mascaramento de dados, aleatorização e hashing para proteger dados confidenciais de grupos específicos. O SAS também fornece data quality sob demanda enquanto a exibição é gerada.\n\n\n\nEvent stream processing and ETL\nQuando a velocidade dos dados aumenta para milhões de eventos por segundo, event stream processing pode ser usado para monitorar e processar fluxos de dados, e ajudar a tomar decisões mais rapidamente. Um exemplo no setor de produção de energia é utilizar análises preditivas em fluxos de dados para detectar quando uma bomba submersa precisa de reparo, reduzindo o tempo de inatividade, além do escopo e do tamanho do dano à bomba."
  },
  {
    "objectID": "integracao_dados.html#etl-oracle",
    "href": "integracao_dados.html#etl-oracle",
    "title": "Integração de Dados",
    "section": "ETL (Oracle)",
    "text": "ETL (Oracle)\n\nO que é ETL?\nExtrair, transformar e carregar (ETL) é o processo que as organizações orientadas a dados usam para coletar dados de várias fontes e reuni-los para dar suporte à descoberta, à geração de relatórios, à análise e à tomada de decisões.\nAs origens de dados podem ser muito diversas em tipo, formato, volume e confiabilidade, de modo que os dados precisam ser processados para serem úteis quando reunidos. Os armazenamentos de dados de destino podem ser bancos de dados, data warehouses ou data lakes, dependendo das metas e da implementação técnica.\n\n\nAs três etapas distintas do ETL\nExtrair\nDurante a extração, o ETL identifica os dados e os copia de suas origens, de forma que possa transportar os dados para o armazenamento de dados de destino. Os dados podem vir de fontes estruturadas e não estruturadas, incluindo documentos, emails, aplicações de negócios, bancos de dados, equipamentos, sensores, terceiros e muito mais.\nTransformar\nComo os dados extraídos são brutos em sua forma original, eles precisam ser mapeados e transformados para prepará-los para o armazenamento de dados eventual. No processo de transformação, o ETL valida, autentica, desduplica e/ou agrega os dados de formas que tornam os dados resultantes confiáveis e consultáveis.\nCarregar\nO ETL move os dados transformados para o armazenamento de dados de destino. Esta etapa pode implicar o carregamento inicial de todos os dados de origem ou pode ser o carregamento de alterações incrementais nos dados de origem. Você pode carregar os dados em tempo real ou em lotes programados.\n\n\nELT ou ETL: Qual é a diferença?\nA etapa de transformação é de longe a mais complexa do processo ETL. Por conseguinte, a ETL e a ELT diferem em dois pontos principais:\n\nQuando a transformação ocorre\nO local da transformação\n\nEm um data warehouse tradicional, os dados são extraídos primeiro de “sistemas de origem” (sistemas de ERP, sistemas de CRM etc.). As ferramentas OLAP e as consultas SQL dependem da padronização das dimensões dos conjuntos de dados para obter resultados agregados. Isso significa que os dados devem passar por uma série de transformações.\nTradicionalmente, essas transformações foram feitas antes que os dados fossem carregados no sistema de destino, normalmente um data warehouse relacional.\nNo entanto, à medida que as tecnologias subjacentes de armazenamento e processamento de dados que sustentam o armazenamento de dados evoluem, tornou-se possível efetuar transformações no sistema alvo. Os processos ETL e ELT envolvem áreas de preparação. No ETL, essas áreas são encontradas na ferramenta, sejam elas proprietárias ou personalizadas. Elas ficam entre o sistema de origem (por exemplo, um sistema CRM) e o sistema de destino (o data warehouse).\nPor outro lado, com ELTs, a área de preparação está no data warehouse, e o mecanismo de banco de dados que alimenta o DBMS faz as transformações, em vez de uma ferramenta ETL. Portanto, uma das consequências imediatas dos ELTs é que você perde as funções de preparação e limpeza de dados que as ferramentas ETL fornecem para ajudar no processo de transformação de dados.\n\n\nData warehouses corporativos e de ETL\nTradicionalmente, as ferramentas para ETL eram usadas principalmente para entregar dados a data warehouses corporativos que suportam aplicações de BI (Business Intelligence). Esses data warehouses são projetados para representar uma fonte confiável de verdade sobre tudo o que está acontecendo em uma empresa em todas as atividades. Os dados nesses warehouses são cuidadosamente estruturados com esquemas, metadados e regras rígidos que controlam a validação de dados.\nAs ferramentas de ETL para data warehouses corporativos devem atender aos requisitos de integração de dados, como carregamentos em lote de alto volume e alto desempenho; processos de integração de fluxo lento e orientados a eventos; transformações programáveis; e orquestrações para que possam lidar com as transformações e fluxos de trabalho mais exigentes e tenham conectores para as mais diversas fontes de dados.\nApós carregar os dados, você tem várias estratégias para mantê-los sincronizados entre os armazenamentos de dados de origem e de destino. É possível recarregar o conjunto de dados completo periodicamente, programar atualizações periódicas dos dados mais recentes ou confirmar para manter a sincronização total entre o data warehouse de origem e o de destino. Essa integração em tempo real é denominada captura de dados de alteração (CDC). Para esse processo avançado, as ferramentas ETL precisam entender a semântica de transação dos bancos de dados de origem e transmitir corretamente essas transações para o data warehouse de destino.\n\n\nETL e data marts\nOs data marts são armazenamentos de dados de destino menores e mais focados do que os data warehouses corporativos. Eles podem, por exemplo, se concentrar em informações sobre um único departamento ou uma única linha de produtos. Por isso, os usuários de ferramentas ETL para data marts costumam ser especialistas em linha de negócios (LOB), analistas de dados e/ou cientistas de dados.\nAs ferramentas ETL para data marts devem ser usadas pelo pessoal de negócios e pelos gerentes de dados, em vez de por programadores e pela equipe de TI. Portanto, essas ferramentas devem ter um workflow visual para facilitar a configuração de pipelines ETL.\n\n\nETL ou ELT e data lakes\nOs Data lakes seguem um padrão diferente dos data warehouses e data marts. Os data lakes geralmente armazenam seus dados no armazenamento de objetos ou nos HDFS (Hadoop Distributed File Systems), e portanto podem armazenar dados menos estruturados sem esquema; e oferecem suporte a várias ferramentas para consultar esses dados não estruturados.\nUm padrão adicional que isso permite é extrair, carregar e transformar (ELT), no qual os dados são armazenados primeiro “como estão” e serão transformados, analisados e processados depois que os dados forem capturados no data lake. Esse padrão oferece vários benefícios.\n\nTodos os dados são registrados; nenhum sinal é perdido devido à agregação ou filtragem.\nOs dados podem ser ingeridos muito rapidamente, o que é útil para o streaming da Internet das Coisas (IoT), análise de log, métricas de site etc.\nEle permite a descoberta de tendências que não eram esperadas no momento da captura.\nEle permite a implementação de novas técnicas de inteligência artificial (IA) que se destacam na detecção de padrões em conjuntos de dados grandes e não estruturados.\n\nAs ferramentas ETL para data lakes incluem ferramentas visuais de integração de dados, porque são eficazes para cientistas de dados e engenheiros de dados. As ferramentas adicionais que geralmente são usadas na arquitetura do data lake incluem o seguinte:\n\nServiços de Cloud Streaming que podem ingerir grandes fluxos de dados em tempo real em data lakes para mensagens, logs de aplicações, telemetria operacional, rastreamento de dados de fluxo de cliques na Web, processamento de eventos e análise de segurança. A compatibilidade com o Kafka garante que esses serviços possam recuperar dados de origens de dados quase infinitas.\nServiços de nuvem baseados em Spark que podem executar rapidamente tarefas de processamento e transformação de dados em conjuntos de dados muito grandes. Os serviços Spark podem carregar os conjuntos de dados a partir do armazenamento de objetos ou do HDFS, processá-los e transformá-los na memória em clusters escaláveis de instâncias de computação e gravar a saída de volta no data lake ou em data marts e/ou data warehouses.\n\n\n\nCasos de uso ETL\nO processo ETL é fundamental para muitos setores por causa de sua capacidade de ingerir dados de forma rápida e confiável em data lakes para ciência de dados e análise, criando modelos de alta qualidade. As soluções ETL também podem carregar e transformar dados transacionais em escala para criar uma visão organizada de grandes volumes de dados. Isso permite que as empresas visualizem e prevejam tendências do setor. Vários setores contam com o ETL para permitir insights acionáveis, tomada de decisões rápida e maior eficiência.\nServiços financeiros\nAs instituições de serviços financeiros coletam grandes quantidades de dados estruturados e não estruturados para obter insights sobre o comportamento do consumidor. Esses insights podem analisar o risco, otimizar os serviços financeiros dos bancos, melhorar as plataformas online e até mesmo fornecer caixas eletrônicos com dinheiro.\nSetores de petróleo e gás\nOs setores de petróleo e gás usam soluções ETL para gerar previsões sobre uso, armazenamento e tendências em áreas geográficas específicas. O ETL funciona para reunir o máximo de informações possível de todos os sensores de um site de extração e processar essas informações para facilitar a leitura.\nAutomotivo\nAs soluções de ETL permitem que as concessionárias e os fabricantes entendam os padrões de vendas, calibrem suas campanhas de marketing, reabram o estoque e acompanhem os leads dos clientes.\nTelecomunicações\nCom o volume e a variedade sem precedentes de dados produzidos hoje, os provedores de telecomunicações contam com soluções ETL para melhor gerenciar e entender esses dados. Uma vez processados e analisados esses dados, as empresas podem usá-los para melhorar a publicidade, a mídia social, o SEO, a satisfação do cliente, a lucratividade e muito mais.\nAssistência Médica\nCom a necessidade de reduzir custos e também melhorar a assistência médica, o setor de assistência médica emprega soluções ETL para gerenciar registros de pacientes, coletar informações de seguros e atender a requisitos regulatórios em evolução.\nCiências biológicas\nOs laboratórios clínicos contam com soluções ETL e inteligência artificial (IA) para processar vários tipos de dados que estão sendo produzidos por instituições de pesquisa. Por exemplo, colaborar no desenvolvimento de vacinas requer que grandes quantidades de dados sejam coletados, processados e analisados.\nSetor público\nCom o surgimento dos recursos de Internet das Coisas (IoT), as cidades inteligentes estão usando o ETL e o poder da IA para otimizar o tráfego, monitorar a qualidade da água, melhorar o estacionamento e muito mais."
  },
  {
    "objectID": "olap.html",
    "href": "olap.html",
    "title": "Conceitos de OLAP",
    "section": "",
    "text": "c) Análise de dados e informações: Conceitos de OLAP"
  },
  {
    "objectID": "olap.html#canaltech",
    "href": "olap.html#canaltech",
    "title": "Conceitos de OLAP",
    "section": "Canaltech",
    "text": "Canaltech\nO OLAP, do inglês “On-line Analytical Processing”, trata da capacidade de analisar grandes volumes de informações nas mais diversas perspectivas dentro de um Data Warehouse (DW). O OLAP também faz referência às ferramentas analíticas utilizadas no BI para a visualização das informações gerenciais e dá suporte para as funções de análises do negócio organizacional.\nOs sistemas OLTP e OLAP se diferenciam em diversos outros aspectos. Vejamos:\n\nEm resumo podemos dizer que a grande diferença está no fato de que um está direcionado ao funcionamento dentro do ambiente operacional (OLTP) e o outro com foco essencialmente gerencial (OLAP)."
  },
  {
    "objectID": "olap.html#brasil-escola",
    "href": "olap.html#brasil-escola",
    "title": "Conceitos de OLAP",
    "section": "Brasil Escola",
    "text": "Brasil Escola\n5.2. OLAP\nSegundo Michel (2003), OLAP (On-Line Analytical Processing ou Processamento Analítico On-Line) é um sistema de informação multidimensional cuja tecnologia de construção permite aos analistas de negócios, gerentes e executivos analisar e visualizar dados corporativos de forma rápida, consistente e principalmente interativa, ou seja, é onde são extraídos e gerados os relatórios para os usuários. Na figura 8, mostra que o OLAP fornece informações aos usuários.\nFigura 8: OLAP\n\nDa mesma forma como o BI, O OLAP não pode ser definido, como uma ferramenta ou um processo, mas sim um conjunto dos mesmos, pois os elementos essenciais para a criação de um OLAP é sua aplicabilidade em diversas camadas da tecnologia, como armazenamento e linguagem de programação, THOMSEN, 2002, complementa dizendo que “[...]De modo geral, pode-se falar de conceitos OLAP, linguagens OLAP, camadas de produtos OLAP e produtos de OLAP completos[...]”.\nO OLAP se difere do ETL, basicamente, pelo fato de ETL fazer a extração de dados diretamente de vários bancos, visando a sua organização e as soluções OLAP, extraem informações que foram geradas pelo ETL, se referindo a um conjunto de ferramentas voltadas para o acesso e análise ad-hoc de dados.\nBILL INMON, 2002 conceitua ad-hoc como:\nConsultas com acesso casual único e tratamento dos dados segundo parâmetros nunca antes utilizados, geralmente executados de forma iterativa e heurística. Isso tudo nada mais é do que o próprio usuário gerar consultas de acordo com suas necessidades de cruzar as informações de uma forma não vista e com métodos que o levem a descoberta daquilo que procura.”\nEm BI, o OLAP pode se apresentar principalmente de duas formas, como MOLAP, que é mais indicado para Data Marts e ROLAP que é mais indicado para Data Warehouse. Nos Data Marts o método de armazenamento de dados OLAP é chamado de MOLAP, que usa a tecnologia MDDB (MultiDimensional Database), isto se deve pelo fato de que os DM são mais específicos e a análise será mais limitada e com pouco detalhamento. Nos DW, o método é o ROLAP, que utiliza a tecnologia (Relational DataBase Management System), que possibilita um uso maior de funções e uma análise com mais confiabilidade na grande gama de informações que o DW possui.\nPara “navegar” nas dimensões do “cubo” OLAP, emprega-se o uso de operadores dimensionais, que tem papeis distintos, podendo ser para aumentar e diminuir a granulidade, que é o nível de detalhamento a ser visualizado, ou então para ordenar e classificar as informações, na seção 4.2.1 algumas operações são demonstradas.\n\n5.3. Operações\nUma das características mais importantes das ferramentas OLAP é a possibilidade de realizar algumas operações no decorrer da implementação, que nos fornece total controle das informações a serem exibidas e ordenadas. Existem variados tipos de comandos, mas comumente no BI utiliza-se algumas principais e são elas: Drill Across, Drill Up, Drill Down e Drill-Through.\nFigura 9: Operações\n\n5.3.1. Drill Across\nÉ um comando para pular de um nível intermediário dentro de uma dimensão para outra dimensão. É necessário a utilização de duas tabelas fatos e essas tabelas tem que compartilhar a mesma dimensão intermediária. Segundo Kimball (2002), “trata-se de uma operação sobre dois cubos. Os dados nos dois cubos são combinados nas dimensões comuns aos mesmos”. Além de “pular” entre as dimensões, também é possível compara-las, por exemplo, é possível traçar um comparativo entre duas dimensões, como o valor total de vendas, pelo numero de um determinado produto vendido, sendo necessário elas apenas compartilharem alguma dimensão. Barbieri, 2001 completa dizendo que “[...] embora correlacionadas, estão em estruturas separadas, porém unidas por algumas dimensões coerentes”.\n\n5.3.2. Drill Up e Drill Down\nO Drill-up é o aumento na hierarquia de uma dimensão, por exemplo, imagina uma dimensão “Tempo” onde estão organizadas as informações em dia, mês, semestre e ano, vamos supor que queiramos ir, do dia 10 para o mês de março, essas operações não vão de um cubo a outro, mas sim na mesma dimensão, aumentando assim a granularidade do DW e diminui o nível de detalhamento.\nJá o Drill Down é o contrário de Drill UP, é a descida na hierarquia de uma dimensão, indo de um mês para um dia diminuindo a granularidade e aumentando o nível de detalhe.\n\n\n5.3.3. Drill Through\nPossui o funcionamento parecido com o Drill Down, porém, tem como característica a possibilidade buscar os dados, ou informações, fora da estrutura principal. Por exemplo, após alcançar o nível máximo de detalhe em uma tabela fato, tem necessidade de se obter mais detalhe sobre determinada célula, como a nota fiscal, por exemplo, com o Drill Through é possível acessar o arquivo de origem dessa informação, diminuindo a granularidade, aumentando o nível de detalhe e saindo da estrutura principal do DW ou de um Data Mart."
  },
  {
    "objectID": "scrum.html",
    "href": "scrum.html",
    "title": "Metodologia SCRUM",
    "section": "",
    "text": "f) Gerenciamento de Projetos: Metodologia SCRUM;"
  },
  {
    "objectID": "scrum.html#glossário-scrum",
    "href": "scrum.html#glossário-scrum",
    "title": "Metodologia SCRUM",
    "section": "Glossário Scrum",
    "text": "Glossário Scrum"
  },
  {
    "objectID": "scrum.html#resumo-scrum",
    "href": "scrum.html#resumo-scrum",
    "title": "Metodologia SCRUM",
    "section": "Resumo Scrum",
    "text": "Resumo Scrum\n\n\n\n\n\n\n3 Funções:\nProduct Owner\nScrum Master\nIntegrante de Equipe\n\n\n5 Eventos:\nSprint\nPlanejamento do Sprint\nDaily Scrum: reunião diária de 15 minutos, no mesmo horário e local.\nRevisão do Sprint\nRetrospectiva do Sprint\n\n\n3 Artefatos:\nBacklog do produto\nBacklog do sprint\nIncrementos\n\n\n5 Valores:\nComprometimento\nCoragem\nFoco\nRespeito\nTransparência\n\nFontes:\nhttps://www.scrum.org/resources/what-is-scrum\nhttps://pt.wikipedia.org/wiki/Scrum\nScrum Guia Prático - J.J. Sutherland, editora Sextante, 2020."
  }
]