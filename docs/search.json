[
  {
    "objectID": "analise_dados.html",
    "href": "analise_dados.html",
    "title": "Análise de Dados e Informações",
    "section": "",
    "text": "c) Análise de dados e informações: Dado, informação, conhecimento e inteligência; Conceitos, fundamentos, características, técnicas e métodos de Business Intelligence (BI); Mapeamento de fontes de dados, Dados estruturados e dados não estruturados, Conceitos de OLAP e suas operações, Conceitos de Data Warehouse. Técnicas de modelagem e otimização de bases de dados multidimensionais, Construção de relatórios e dashboards interativos em ferramentas de BI, Manipulação de dados em planilhas, Geração de insights a partir de relatórios e dashboards, BI como suporte a processos de tomada decisão, Conceitos Básicos em Séries Temporais, Conceitos Básicos de estatística descritiva, probabilística e testes de hipótese, Manipulação, tratamento e visualização de dados, Tratamento de dados faltantes, Tratamento de dados categóricos, Normalização numérica, Detecção e tratamento de outliers"
  },
  {
    "objectID": "banco_dados.html",
    "href": "banco_dados.html",
    "title": "Banco de Dados",
    "section": "",
    "text": "a) Banco de Dados: Fundamentos de administração de dados: Segurança; Modelagem de dados: Modelo entidade-relacionamento (entidades, atributos, chaves e relacionamentos) e Normalização;"
  },
  {
    "objectID": "bd_transacao.html",
    "href": "bd_transacao.html",
    "title": "Conceitos de Transação, concorrência…",
    "section": "",
    "text": "b) Fundamentos de Banco de Dados: Conceito de transação, concorrência, recuperação, integridade\n\n\nColeções de operações que formam uma única unidade lógica de trabalho são chamadas transações. Um sistema de banco de dados precisa garantir a execução apropriada de transações apesar das falhas – a transação inteira é executada ou nenhuma parte dela é executada. Além do mais, ele precisa gerenciar a execução simultânea de transações de modo a evitar a introdução da inconsistência.\nConceito de transação: Uma transação é uma unidade de execução do programa que acessa e, possivelmente, atualiza vários itens de dados. Uma transação é delimitada pelas instruções (ou chamadas de função) na forma begin transaction e end transaction. A transação consiste em todas as operações executadas entre o begin transaction e o end transaction.\nEsse conjunto de operações precisa aparecer ao usuário como uma única unidade, indivisível. Como uma transação é indivisível, ela é executada em sua totalidade ou não é executada. Assim, se uma transação começa a ser executada, mas falha por um motivo qualquer, quaisquer mudanças no banco de dados que a transação possa ter feito são desfeitas. Essa propriedade “tudo ou nada” é conhecida como atomicidade.\nÉ necessário que o sistema de banco de dados mantenha as seguintes propriedades das transações:\n•Atomicidade. Todas as operações da transação são refletidas corretamente no banco de dados, ou nenhuma delas.\n•Consistência. A execução de uma transação isolada (ou seja, sem nenhuma outra transação executando simultaneamente) preserva a consistência do banco de dados.\n•Isolamento. Embora várias transações possam ser executadas simultaneamente, o sistema garante que, para cada par de transações Ti e Tj, parece para Ti ou Tj terminou a execução antes que Ti começasse, ou Tj iniciou a execução depois que Ti terminou. Assim, cada transação não está ciente das outras transações sendo executadas simultaneamente no sistema.\n•Durabilidade. Depois que uma transação for completada com sucesso, as mudanças que ela fez no banco de dados persistem, mesmo se houver falhas no sistema.\n\nFontes: Sistema de Banco de Dados - Abraham Silberschatz"
  },
  {
    "objectID": "conceitos_bi.html",
    "href": "conceitos_bi.html",
    "title": "Conceitos de BI",
    "section": "",
    "text": "c) Análise de dados e informações: Conceitos, fundamentos, características, técnicas e métodos de Business Intelligence (BI)"
  },
  {
    "objectID": "conceitos_bi.html#brasil-escola",
    "href": "conceitos_bi.html#brasil-escola",
    "title": "Conceitos de BI",
    "section": "Brasil Escola",
    "text": "Brasil Escola"
  },
  {
    "objectID": "conceitos_bi.html#business-intelligence",
    "href": "conceitos_bi.html#business-intelligence",
    "title": "Conceitos de BI",
    "section": "4. BUSINESS INTELLIGENCE",
    "text": "4. BUSINESS INTELLIGENCE\nO conceito de business Intelligence foi nomeado pelo Gartner Group, grupo com enfoque no uso de tecnologias para a tomada de decisão, nos anos 1990, porém, sua ideia base vem de muito antes. Quando se tem uma coleta de informações para a tomada de decisão, isso, a grosso modo, é business Intelligence. Mas a partir da nossa evolução no mundo tecnológico, o BI, vem sendo auxiliado e por inúmeras ferramentas, como o Sistemas de Informações Executivas/EIS, que começou a ser utilizado nos anos 1980, fornecendo informações a nível estratégicos das organizações, como relatórios dinâmicos e análise de tendências, sistema esse que mais tarde, juntamente com todas as suas ferramentas, seria chamado de soluções em BI.\nEm meados dos anos 1980, começou a surgir às primeiras soluções baseadas em gerenciamento de banco de dados com modelo relacional, isso implica que as informações geradas eram dispostas em tabelas com suas respectivas descrições, mesmo hoje é o modelo mais utilizado quando se diz respeito a banco de dados. Com essas novas ferramentas, se tornou mais viável o uso do sistema de informação Enterprise Resource Planning/ERP, que é a integração de todos os dados gerados pela organização.\nSegundo Davenport (1998), o ERP é definido como:\nUm software de negócio que permite à empresa automatizar e integrar a maioria de seus processos; compartilhar práticas de negócio e dados comuns pela empresa; e disponibilizar a informação em tempo real”.\nÉ visto como a solução para acabar com os vários programas que funcionam no mesmo ambiente empresarial, sem integração, produzindo informações de pouca qualidade para o negócio. Sistemas dessa natureza são adquiridos com o intuito de tornar os processos empresariais mais ágeis e extrair informações mais acuradas da empresa.\nMesmo sendo uma enorme vantagem o uso de softwares de ERP, o processo de análise de informações ainda era complicado, pois se utiliza um modelo relacional que, dependendo da sua arquitetura, não pode ser usado para a análise consistente de resultados. Os ERP são disponibilizados a partir de módulos, que se adaptam as necessidades das empresas, partindo disso e do problema em análise de dados, as empresas de ERP, passaram a incluir módulos específicos em BI.\nComo é citado pelo brasileiro Carlos Barbieri (2001, XX):\nNo fundo, tudo relativo à nova era da economia da informação, dedicada à captura de dados, informações e conhecimentos que permitam às empresas competirem com maior eficiência num ring de disputas leoninas”.\nConsiderando a utilização dos ERP, e as suas estruturas de dados, ficou nítido a necessidade de um armazenamento de dados especifico para se gerar informações confiáveis, então, começou a se pensar na utilização de um armazenamento de dados, que foi introduzido inicialmente na década de 1960 pela International Business Machines/IBM, empresa americana voltada para a área de informática, modelo chamado de Data Warehouse/DW (inicialmente o modelo era chamado de Information Warehouse).\nO Data Warehouse, segundo DALFOVO e TAMBORLIN (2006),\n[...] pode ser definido como um banco de dados especializado, que integra e gerencia o fluxo de informações a partir dos bancos de dados corporativos e fontes de dados externas à empresa, [...]“.\nNeste Sentido, o DW serve para criar uma visão centralizada e única dos dados que foram gerados em diversos outros bancos de dados. É a partir daí que o BI gera relatórios e informações, porém, ao contrário do ERP, podem ser utilizados por muitas outras pessoas na organização.\nMas afinal, qual o conceito de BI?\nO BI não pode ser enquadrado como um sistema, nem como um produto e nem as suas ferramentas, mas pode ser compreendido como o uso de arquiteturas, aplicativos e banco de dados (ZAMAN, 2005)\nDe acordo com Carlos Barbieri (2001, p.34), BI pode ser definido como “a utilização de variadas fontes de informação para se definir estratégias de competitividade nos negócios da empresa”, várias fontes de informações, pois como o acesso à tecnologia se tornou mais viável à população, mais informações de diferentes fontes, como mídias sociais, telefone e e-mail, são geradas. Informações essas que em sua maioria não podem ser analisadas, pois não foram “higienizadas”, ou estão em várias tabelas, onde se torna inviável a relação com outras tabelas, para gerar o que de fato é importante."
  },
  {
    "objectID": "conceitos_bi.html#etapas-técnicas-e-características-de-bi",
    "href": "conceitos_bi.html#etapas-técnicas-e-características-de-bi",
    "title": "Conceitos de BI",
    "section": "5. ETAPAS, TÉCNICAS E CARACTERÍSTICAS DE BI",
    "text": "5. ETAPAS, TÉCNICAS E CARACTERÍSTICAS DE BI\nO BI, como em qualquer projeto a ser implementando possuem suas etapas, que nem sempre precisam ser seguidas a riscas, mas facilitam no processo, pois assim, um modelo de projeto possa ser utilizado por outras organizações ou em implementações futuras.\nComumente, uma implementação de um projeto BI não se difere de outros projetos, no que diz respeito na forma como são levantadas as necessidades, porém, possuem suas particularidades. Abaixo, encontra-se um modelo para o desenvolvimento que pode ser a base para qualquer projeto.\n\nDefinição de requisitos\n\nPrimeira etapa, onde se deve entender a forma como o desenvolvimento vai acontecer e o resultado a ser alcançado.\n\nEstrutura do Data Warehouse\n\nDesenhar a estrutura necessária para o DW (ver seção 4.1), assim como a centralização das informações que são pertinentes, é onde também, se necessário, estruturar o Data Mart (ver seção 4.1.4).\n\nDefinição do ETL\n\nApós estruturar o DW, é necessário vincular a fonte de dados original, esse processo pode ser demorado então é de suma importância que sua implementação seja bastante consolidada (ver seção 4.1.6).\n\nEstrutura do cubo OLAP\n\nApós a estrutura do DW pronta, o ETL carregando essas informações, é necessário a criação de um cubo OLAP, que tem como finalidade agilizar a criação de relatórios, os detalhes estão na seção 4.2.\n\nDashboards\n\nApós o projeto finalizado, irão ser utilizados softwares que serão conectados ao Cubo OLAP e que possuem a característica de gerar Dashboards, ou gráficos para a análise e tomada de decisão na organização."
  },
  {
    "objectID": "dados_estruturados.html",
    "href": "dados_estruturados.html",
    "title": "Dados estruturados e dados não estruturados",
    "section": "",
    "text": "c) Análise de dados e informações: Dados estruturados e dados não estruturados"
  },
  {
    "objectID": "dados_estruturados.html#dados-estruturados-e-não-estruturados---universidade-tecnologia",
    "href": "dados_estruturados.html#dados-estruturados-e-não-estruturados---universidade-tecnologia",
    "title": "Dados estruturados e dados não estruturados",
    "section": "Dados Estruturados e Não Estruturados - Universidade tecnologia",
    "text": "Dados Estruturados e Não Estruturados - Universidade tecnologia"
  },
  {
    "objectID": "dados_estruturados.html#dados-estruturados-e-não-estruturados-o-que-são-e-quais-suas-diferenças---xp-educação",
    "href": "dados_estruturados.html#dados-estruturados-e-não-estruturados-o-que-são-e-quais-suas-diferenças---xp-educação",
    "title": "Dados estruturados e dados não estruturados",
    "section": "Dados estruturados e não estruturados: o que são e quais suas diferenças? - XP Educação",
    "text": "Dados estruturados e não estruturados: o que são e quais suas diferenças? - XP Educação"
  },
  {
    "objectID": "dados_faltantes.html",
    "href": "dados_faltantes.html",
    "title": "Dados Faltantes",
    "section": "",
    "text": "c) Análise de dados e informações: Tratamento de dados faltantes"
  },
  {
    "objectID": "dados_faltantes.html#a-prevenção-e-o-tratamento-dos-dados-ausentes",
    "href": "dados_faltantes.html#a-prevenção-e-o-tratamento-dos-dados-ausentes",
    "title": "Dados Faltantes",
    "section": "A prevenção e o tratamento dos dados ausentes",
    "text": "A prevenção e o tratamento dos dados ausentes\nDados ausentes (ou valores ausentes) são definidos como o valor de dados que não é armazenado para uma variável na observação de interesse.\nA falta de dados apresenta vários problemas. Primeiro, a ausência de dados reduz o poder estatístico, que se refere à probabilidade de o teste rejeitar a hipótese nula quando ela é falsa. Em segundo lugar, os dados perdidos podem causar viés na estimativa dos parâmetros. Em terceiro lugar, pode reduzir a representatividade das amostras. Em quarto lugar, pode complicar a análise do estudo. Cada uma dessas distorções pode ameaçar a validade dos ensaios e pode levar a conclusões inválidas.\n\nTipos de dados ausentes\n\nFaltando completamente ao acaso\nFaltando completamente ao acaso (MCAR) é definido como quando a probabilidade de que os dados estejam faltando não está relacionada ao valor específico que se supõe ser obtido ou ao conjunto de respostas observadas. MCAR é uma suposição ideal, mas irracional, para muitos estudos realizados no campo da anestesiologia. No entanto, se os dados estiverem faltando por projeto, devido a uma falha do equipamento ou porque as amostras foram perdidas em trânsito ou tecnicamente insatisfatórias, tais dados são considerados MCAR.\nA vantagem estatística dos dados que são MCAR é que a análise permanece imparcial. A energia pode ser perdida no projeto, mas os parâmetros estimados não são influenciados pela ausência dos dados.\n\n\nFaltando ao acaso\nMissing at random (MAR) é uma suposição mais realista para os estudos realizados no campo anestésico. Os dados são considerados MAR quando a probabilidade de que as respostas estejam ausentes depende do conjunto de respostas observadas, mas não está relacionada aos valores ausentes específicos que se espera obter.\nComo tendemos a considerar a aleatoriedade como não produzindo viés, podemos pensar que o MAR não apresenta um problema. No entanto, MAR não significa que os dados ausentes possam ser ignorados. Se uma variável de abandono for MAR, podemos esperar que a probabilidade de abandono da variável em cada caso seja condicionalmente independente da variável, que é obtida atualmente e que se espera obter no futuro, dado o histórico da variável obtida antes a esse caso.\n\n\nFaltando não ao acaso\nSe os caracteres dos dados não atenderem aos de MCAR ou MAR, eles se enquadram na categoria de falta não aleatória (MNAR).\nOs casos de dados MNAR são problemáticos. A única maneira de obter uma estimativa imparcial dos parâmetros em tal caso é modelar os dados ausentes. O modelo pode então ser incorporado a um modelo mais complexo para estimar os valores ausentes.\n\n\n\nTécnicas para lidar com os dados ausentes\n\nListwise ou exclusão de caso\nDe longe, a abordagem mais comum para os dados ausentes é simplesmente omitir os casos com os dados ausentes e analisar os dados restantes. Essa abordagem é conhecida como análise de caso completo (ou caso disponível) ou exclusão de lista.\nA exclusão Listwise é o método usado com mais frequência para lidar com dados ausentes e, portanto, tornou-se a opção padrão para análise na maioria dos pacotes de software estatístico. Alguns pesquisadores insistem que isso pode introduzir viés na estimativa dos parâmetros. No entanto, se a suposição de MCAR for satisfeita, sabe-se que uma exclusão de lista produz estimativas imparciais e resultados conservadores. Quando os dados não cumprem a suposição de MCAR, a exclusão listwise pode causar viés nas estimativas dos parâmetros.\nSe houver uma amostra grande o suficiente, onde a potência não é um problema e a suposição de MCAR for satisfeita, a exclusão de lista pode ser uma estratégia razoável. No entanto, quando não há uma amostra grande ou a suposição de MCAR não é satisfeita, a exclusão de lista não é a estratégia ideal.\n\n\nExclusão de pares\nA exclusão de pares elimina informações apenas quando o ponto de dados específico necessário para testar uma suposição específica está ausente. Se houver dados ausentes em outro lugar no conjunto de dados, os valores existentes serão usados no teste estatístico. Como uma exclusão pairwise usa todas as informações observadas, ela preserva mais informações do que a exclusão listwise, que pode excluir o caso com quaisquer dados ausentes. Esta abordagem apresenta os seguintes problemas: 1) os parâmetros do modelo estarão em diferentes conjuntos de dados com diferentes estatísticas, como tamanho da amostra e erros padrão; e 2) pode produzir uma matriz de intercorrelação que não é positiva definida, o que provavelmente impedirá uma análise mais aprofundada.\nA exclusão de pares é conhecida por ser menos tendenciosa para os dados MCAR ou MAR, e os mecanismos apropriados são incluídos como covariáveis. No entanto, se houver muitas observações ausentes, a análise será deficiente.\n\n\nSubstituição média\nEm uma substituição média, o valor médio de uma variável é usado no lugar do valor de dados ausentes para essa mesma variável. Isso permite que os pesquisadores utilizem os dados coletados em um conjunto de dados incompleto. O fundamento teórico da substituição da média é que a média é uma estimativa razoável para uma observação selecionada aleatoriamente de uma distribuição normal. No entanto, com valores ausentes que não são estritamente aleatórios, especialmente na presença de uma grande desigualdade no número de valores ausentes para as diferentes variáveis, o método de substituição de média pode levar a um viés inconsistente. Além disso, esta abordagem não adiciona novas informações, mas apenas aumenta o tamanho da amostra e leva a uma subestimação dos erros. Assim, a substituição média não é geralmente aceita.\n\n\nImputação de regressão\nA imputação é o processo de substituir os dados perdidos por valores estimados. Em vez de excluir qualquer caso que tenha algum valor ausente, essa abordagem preserva todos os casos substituindo os dados ausentes por um valor provável estimado por outras informações disponíveis. Após todos os valores ausentes terem sido substituídos por esta abordagem, o conjunto de dados é analisado usando as técnicas padrão para dados completos.\nNa imputação de regressão, as variáveis existentes são usadas para fazer uma previsão e, em seguida, o valor previsto é substituído como se fosse um valor real obtido. Essa abordagem tem várias vantagens, porque a imputação retém uma grande quantidade de dados sobre a exclusão de lista ou de pares e evita alterar significativamente o desvio padrão ou a forma da distribuição. No entanto, como em uma substituição de média, enquanto uma imputação de regressão substitui um valor que é previsto de outras variáveis, nenhuma informação nova é adicionada, enquanto o tamanho da amostra foi aumentado e o erro padrão foi reduzido.\n\n\n\nÚltima observação realizada\nNo campo da pesquisa em anestesiologia, muitos estudos são realizados com a abordagem longitudinal ou de séries temporais, nas quais os sujeitos são medidos repetidamente em uma série de pontos no tempo. Um dos métodos de imputação mais amplamente utilizados nesse caso é a última observação realizada (LOCF). Este método substitui cada valor ausente pelo último valor observado do mesmo assunto. Sempre que faltar um valor, este é substituído pelo último valor observado.\nEste método é vantajoso, pois é fácil de entender e comunicar entre os estatísticos e os clínicos ou entre um patrocinador e o pesquisador.\nEmbora simples, esse método assume fortemente que o valor do resultado permanece inalterado pelos dados ausentes, o que parece improvável em muitos cenários (especialmente nas tentativas anestésicas). Produz uma estimativa enviesada do efeito do tratamento e subestima a variabilidade do resultado estimado. Consequentemente, a Academia Nacional de Ciências recomendou contra o uso acrítico da imputação simples, incluindo LOCF e a observação de linha de base realizada, afirmando que:\nMétodos de imputação única, como a última observação realizada e a observação da linha de base, não devem ser usados como a abordagem principal para o tratamento de dados ausentes, a menos que as suposições subjacentes sejam cientificamente justificadas.\n\nProbabilidade máxima\nExistem várias estratégias que usam o método de máxima verossimilhança para lidar com os dados ausentes. Neles, a suposição de que os dados observados são uma amostra extraída de uma distribuição normal multivariada é relativamente fácil de entender. Depois que os parâmetros são estimados usando os dados disponíveis, os dados ausentes são estimados com base nos parâmetros que acabaram de ser estimados.\nQuando há dados ausentes, mas relativamente completos, as estatísticas que explicam as relações entre as variáveis podem ser calculadas usando o método de máxima verossimilhança. Ou seja, os dados perdidos podem ser estimados usando a distribuição condicional das outras variáveis.\n\n\nExpectativa-Maximização\nExpectation-Maximization (EM) é um tipo de método de máxima verossimilhança que pode ser usado para criar um novo conjunto de dados, no qual todos os valores ausentes são imputados com valores estimados pelos métodos de máxima verossimilhança. Essa abordagem começa com a etapa de expectativa, durante a qual os parâmetros (por exemplo, variâncias, covariâncias e médias) são estimados, talvez usando a exclusão listwise. Essas estimativas são usadas para criar uma equação de regressão para prever os dados ausentes. A etapa de maximização usa essas equações para preencher os dados ausentes. A etapa de expectativa é então repetida com os novos parâmetros, onde as novas equações de regressão são determinadas para “preencher” os dados ausentes. As etapas de expectativa e maximização são repetidas até que o sistema se estabilize, quando a matriz de covariância para a iteração subsequente é virtualmente a mesma da iteração anterior.\nUma característica importante da imputação de maximização de expectativa é que, quando o novo conjunto de dados sem valores ausentes é gerado, um termo de perturbação aleatório para cada valor imputado é incorporado para refletir a incerteza associada à imputação. No entanto, a imputação de maximização de expectativa tem algumas desvantagens. Essa abordagem pode levar muito tempo para convergir, especialmente quando há uma grande fração de dados ausentes, e é muito complexa para ser aceita por alguns estatísticos excepcionais. Essa abordagem pode levar a estimativas de parâmetros enviesadas e subestimar o erro padrão.\nPara o método de imputação de maximização de expectativa, um valor predito com base nas variáveis disponíveis para cada caso é substituído pelos dados ausentes. Como uma única imputação omite as possíveis diferenças entre as múltiplas imputações, uma única imputação tenderá a subestimar os erros padrão e, assim, superestimar o nível de precisão. Assim, uma única imputação dá ao pesquisador mais poder aparente do que os dados na realidade.\n\n\nImputação múltipla\nA imputação múltipla é outra estratégia útil para lidar com os dados ausentes. Em uma imputação múltipla, em vez de substituir um único valor para cada dado ausente, os valores ausentes são substituídos por um conjunto de valores plausíveis que contêm a variabilidade natural e a incerteza dos valores corretos.\nEsta abordagem começa com uma previsão dos dados perdidos usando os dados existentes de outras variáveis . Os valores ausentes são substituídos pelos valores previstos e um conjunto de dados completo chamado conjunto de dados imputados é criado. Este processo itera a repetibilidade e cria vários conjuntos de dados imputados (daí o termo “imputação múltipla”). Cada conjunto de dados imputados múltiplos produzido é então analisado usando os procedimentos de análise estatística padrão para dados completos e fornece resultados de análises múltiplas. Posteriormente, combinando esses resultados de análise, um único resultado de análise geral é produzido.\nO benefício da imputação múltipla é que, além de restaurar a variabilidade natural dos valores ausentes, ela incorpora a incerteza devido aos dados ausentes, o que resulta em uma inferência estatística válida. Restaurar a variabilidade natural dos dados ausentes pode ser obtido substituindo os dados ausentes pelos valores imputados que são previstos usando as variáveis correlacionadas com os dados ausentes. A incorporação da incerteza é feita produzindo diferentes versões dos dados ausentes e observando a variabilidade entre os conjuntos de dados imputados.\nFoi demonstrado que a imputação múltipla produz inferência estatística válida que reflete a incerteza associada à estimativa dos dados ausentes. Além disso, a imputação múltipla revela-se robusta à violação dos pressupostos de normalidade e produz resultados adequados mesmo na presença de um pequeno tamanho amostral ou de um elevado número de dados perdidos.\nCom o desenvolvimento de novos softwares estatísticos, embora os princípios estatísticos da imputação múltipla possam ser difíceis de entender, a abordagem pode ser utilizada facilmente.\n\n\nAnálise sensitiva\nA análise de sensibilidade é definida como o estudo que define como a incerteza na saída de um modelo pode ser alocada para as diferentes fontes de incerteza em suas entradas.\nAo analisar os dados ausentes, suposições adicionais sobre os motivos dos dados ausentes são feitas e essas suposições geralmente são aplicáveis à análise primária. No entanto, as suposições não podem ser definitivamente validadas quanto à correção. Portanto, o National Research Council propôs que a análise de sensibilidade fosse realizada para avaliar a robustez dos resultados aos desvios da suposição MAR.\n\nFontes: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/"
  },
  {
    "objectID": "dado_info.html",
    "href": "dado_info.html",
    "title": "Dado, informação, conhecimento e Inteligência",
    "section": "",
    "text": "c) Análise de dados e informações: Dado, informação, conhecimento e inteligência;\nDado\nDesde que entramos na ERA da INFORMAÇÃO, o dado é um elemento de suma importância. Nas atividades diárias necessitamos de aplicações que envolvem bancos de dados. Exemplos: aplicações de Internet Banking; reservas em hotéis ou companhias aéreas; etc.\nMas, o que é um dado? É um registro de alguma entidade. Um nome é um dado, uma foto é um dado, 134 é um dado, 5 é um dado, etc. Trata-se de uma sequência de símbolos, também conhecidos como signos, que podem ser representados com sons, imagens, textos, números e estruturas. Não há semântica envolvida no dado. Não há uma interpretação sobre essa sequência de símbolos. É algo “bruto”, como o número 10 ou a letra F.\nMoresi (2001) destaca que dados são fatos ou observações “crus”. Mais especificamente, os dados são medidas objetivas e quantitativas dos atributos (características) de entidades como pessoas, lugares, coisas e eventos (conjunto de fatos).\nOs dados são uma parte pequena da informação, que sozinhos não fazem sentido!\nInformação\nJá a informação é um dado depois de processado, é uma contextualização de um dado… Como assim? “5” é um dado, mas e se eu disser o seguinte: “No dia 5 não haverá aula!”. Nesse caso, o 5 passou a ter sentido (ou passou a ter “contexto”) e agora é uma informação!\nInformações são conjuntos de dados significativos e úteis a seres humanos em processos como o de tomada de decisões. “São dados interpretados, dotados de relevância e propósito” (DRUCKER, 1999).\nConforme destacado por Moresi (2001), informações são dados que foram organizados e ordenados de forma coerente e significativa para fins de compreensão e análise (sendo a base para ações coordenadas).\nA transformação de dados em informação é frequentemente realizada através da apresentação dos dados em uma forma compreensível ao usuário. As informações são produzidas pelo processamento de dados. Elas são utilizadas para revelar o significado dos dados.\n\nNa figura anterior, dados brutos registrados por um caixa de supermercados podem ser processados e organizados de modo a produzir informações úteis, tal como o total de unidades de detergentes vendidas ou a receita total de vendas do detergente para determinada loja ou território de vendas.\nConhecimento\nConhecimento (ou Capital Intelectual) é a habilidade de transformar a informação em ações reais. O conhecimento é uma mistura de elementos estruturados de forma intuitiva e, portanto, é difícil de ser colocado em palavras ou de ser plenamente entendido em termos lógicos.\nConhecimento, de acordo com Moresi (2001) é uma mistura fluída de experiências, informação contextual, valores e intuição, formando um painel na mente de uma pessoa que a habilita a avaliar e obter novas experiências e informações. O conhecimento é a consequência mental de angariar informações e, em sua forma mais desenvolvida, apresenta‑se como a capacidade de chegar a novas descobertas com base no aprendizado e na experiência. São informações que foram analisadas e avaliadas sobre a sua confiabilidade, sua relevância e sua importância.\nPara guardar uma informação, precisamos retê-la em nossa memória; para guardar um conhecimento, devemos incorporá-lo em nossa mente e, consequentemente, em nossa maneira de pensar.\nConhecimento demanda análise e avaliação sobre a confiabilidade, relevância e importância de dados e informações para a construção de um quadro de situação (Banca FCC/2015).\n\nInteligência\nA inteligência é o dom humano capaz de “digerir” as informações, por meio da análise, e transformá-la em conhecimento útil. Pode ser vista como o conhecimento que foi sintetizado e aplicadoa determinada situação para ganhar maior profundidade e consciência.\nBaseia-se na experiência e intuição e, portanto, é habilidade puramente humana. É a faculdade humana de conhecer, compreender, raciocinar, pensar e interpretar. Envolve exercício de ponderação para a tomada da melhor decisão, bem como noções de ética, bom e ruim, certo e errado.\nInteligência é a informação devidamente filtrada, destilada e analisada que pode apoiar a tomada de decisões. A transformação de conhecimento em inteligência ocorre por meio de síntese da experiência e, muito além do que qualquer sistema de análise de informação, necessita de habilidades humanas (MORESI, 2001).\nVamos esquematizar para facilitar a memorização desse assunto!\n\n\nFontes:\nPonto dos Concursos"
  },
  {
    "objectID": "data_warehouse.html",
    "href": "data_warehouse.html",
    "title": "Conceitos de Data Warehouse",
    "section": "",
    "text": "c) Análise de dados e informações: Conceitos de Data Warehouse"
  },
  {
    "objectID": "data_warehouse.html#data-warehouse---aws",
    "href": "data_warehouse.html#data-warehouse---aws",
    "title": "Conceitos de Data Warehouse",
    "section": "Data Warehouse - AWS",
    "text": "Data Warehouse - AWS\n\nO que é um data warehouse?\nUm data warehouse é um repositório central de informações que podem ser analisadas para tomar decisões mais adequadas. Os dados fluem de sistemas transacionais, bancos de dados relacionais e de outras fontes para o data warehouse, normalmente com uma cadência regular. Analistas de negócios, engenheiros de dados, cientistas de dados e tomadores de decisões acessam os dados por meio de ferramentas de inteligência de negócios (BI), clientes SQL e outros aplicativos de análise.\nDados e análises se tornaram indispensáveis para que as empresas se mantenham competitivas. Os usuários corporativos contam com relatórios, painéis e análises para extrair insights dos dados, monitorar a performance dos negócios e apoiar a tomada de decisões. Os data warehouses alimentam esses relatórios, painéis e ferramentas de análise armazenando dados de maneira eficiente para minimizar a entrada e saída (E/S) dos dados e fornecer resultados de consulta rapidamente para centenas e milhares de usuários simultaneamente.\n\n\nComo um data warehouse é arquitetado?\nUma arquitetura de data warehouses é composta de camadas. A camada superior é o cliente de front-end, que apresenta os resultados por meio de ferramentas de relatórios, análises e mineração de dados. A camada intermediária consiste no mecanismo de análises, usado para acessar e analisar os dados. A camada inferior da arquitetura é o servidor de banco de dados, onde os dados são carregados e armazenados. Os dados são armazenados de dois modos diferentes: 1) os dados acessados com frequência são armazenados em armazenamento muito rápido (como unidades SSD) e 2) os dados acessados com pouca frequência são armazenados em um armazenamento de objetos barato, como o Amazon S3. O data warehouse garantirá automaticamente que os dados acessados com frequência sejam movidos para o armazenamento “rápido”, para otimizar a velocidade da consulta.\n\n\n\nComo funciona um data warehouse?\nUm data warehouse pode conter vários bancos de dados. Dentro de cada banco de dados, os dados são organizados em tabelas e colunas. Dentro de cada coluna, você pode definir uma descrição dos dados, como número inteiro, campo de dados ou sequência. As tabelas podem ser organizadas dentro de esquemas, que você pode considerar como pastas. Quando os dados são consumidos, eles são armazenados em várias tabelas descritas pelo esquema. As ferramentas de consulta usam o esquema para determinar as tabelas de dados que serão acessadas e analisadas.\n\n\n\nQuais são os benefícios de usar um data warehouse?\nOs benefícios de um data warehouse incluem o seguinte:\n\nTomada de decisão adequada\nDados consolidados de várias fontes\nAnálise de dados históricos\nQualidade, consistência e precisão de dados\nSeparação do processamento analítico dos bancos de dados transacionais, o que melhora o desempenho dos dois sistemas\n\n\n\nComo os data warehouses, os bancos de dados e data lakes funcionam juntos?\nNormalmente, as empresas usam uma combinação de banco de dados, data lake e data warehouse para armazenar e analisar dados. A arquitetura de lake house do Amazon Redshift facilita essa integração.\nÀ medida que o volume e a variedade de dados aumentam, é vantajoso seguir um ou mais padrões comuns para trabalhar com dados em seu banco de dados, data lake e data warehouse:\n\n\nImagem (acima): armazenar os dados em um banco de dados ou datalake, preparar os dados, mover os dados selecionados para um data warehouse e executar relatórios.\n\nImage (acima): armazenar os dados em uma data warehouse, analisar os dados e compartilhá-los para usar com outros serviços de análise e machine learning.\nUm data warehouse é projetado especificamente para análises de dados, que envolvem a leitura de grandes quantidades de dados para compreender relações e tendências entre os dados. Um banco de dados é usado para capturar e armazenar dados, como o registro de detalhes de uma transação.\nAo contrário de um data warehouse, um data lake é um repositório centralizado para todos os dados, incluindo estruturados, semiestruturados e não estruturados. Um data warehouse exige que os dados sejam organizados em um formato tabular, onde o esquema torna-se necessário. O formato tabular é necessário para que o SQL possa ser usado para consultar os dados, mas nem todos os aplicativos exigem que os dados estejam em formato de tabela. Alguns aplicativos, como análise de big data, pesquisa de texto completo e machine learning, podem acessar dados mesmo que sejam “semiestruturados” ou completamente não estruturados.\n\n\n\nComparação entre data warehouses e data lakes\n\n\n\n\n\n\n\n\nCaracterísticas\nData warehouse\nData lake\n\n\n\n\nDados\nDados relacionais de sistemas transacionais, bancos de dados operacionais e aplicativos de linha de negócios\nTodos os dados, incluindo estruturados, semiestruturados e não estruturados\n\n\nEsquema\nGeralmente projetado antes da implementação do data warehouse, mas também pode ser gravado no momento da análise\n(esquema na gravação ou esquema na leitura)\nGravado no momento da análise (esquema na leitura)\n\n\nPreço/performance\nResultados de consulta mais rápidos, usando armazenamento local\nResultados da consulta cada vez mais rápidos usando armazenamento de baixo custo e desacoplamento de computação e armazenamento\n\n\nQualidade dos dados\nDados altamente organizados, que representam a versão central da verdade\nQuaisquer dados, organizados ou não (ou seja, dados brutos)\n\n\nUsuários\nAnalistas de negócios, cientistas de dados e desenvolvedores de dados\nAnalistas de negócios (usando dados organizados), cientistas de dados, desenvolvedores de dados, engenheiros de dados e arquitetos de dados\n\n\nAnálises\nGeração de relatórios em lote, BI e visualizações\nMachine learning, análise exploratória, descoberta de dados, streaming, análise operacional, big data e criação de perfil\n\n\n\n\n\nComparação entre data warehouses e bancos de dados\n\n\n\n\n\n\n\n\nCaracterísticas\nData warehouse\nBanco de dados transacional\n\n\n\n\nCargas de trabalho adequadas\nAnálises, relatórios e big data\nProcessamento de transações\n\n\nFonte de dados\nDados coletados e normalizados de diversas fontes\nDados capturados no estado em que se encontram, de uma única fonte, como um sistema transacional\n\n\nCaptura de dados\nOperações de gravação em massa, executadas normalmente em uma programação de lotes pré-determinada\nOtimizado para operações contínuas de gravação à medida que novos dados são disponibilizados para maximizar o throughput das transações\n\n\nNormalização de dados\nEsquemas desnormalizados, como Star ou Snowflake\nEsquemas estáticos altamente normalizados\n\n\nArmazenamento de dados\nOtimizado para simplicidade de acesso e alto desempenho de consultas usando armazenamento colunar\nOtimizado para operações de gravação de alto throughput em um único bloco físico orientado a linhas\n\n\nAcesso aos dados\nOtimizado para minimizar a E/S e maximizar o throughput de dados\nGrandes volumes de pequenas operações de leitura\n\n\n\n\n\nComo um data mart se compara a um data warehouse?\nUm data mart é um data warehouse que atende às necessidades de uma equipe ou unidade de negócios específica, como finanças, marketing ou vendas. O data mart é menor, mais focado e pode conter resumos de dados para atender melhor à comunidade de usuários. Um data mart também pode ser uma parte de um data warehouse.\n\n\n\nComparação entre data warehouses e data marts\n\n\n\nCaracterísticas\nData warehouse\nData mart\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaracterísticas\nData warehouse\nData mart\n\n\n\n\nEscopo\nVárias áreas centralizadas e integradas\nUma área específica e descentralizada\n\n\nUsuários\nDe toda a organização\nUma única comunidade ou departamento\n\n\nFonte de dados\nMuitas fontes\nUma ou poucas fontes, ou uma parte dos dados já coletados em um data warehouse\n\n\nTamanho\nGrande, pode variar de centenas de gigabytes a petabytes\nPequeno, normalmente até algumas dezenas de gigabytes\n\n\nProjeto\nDe cima para baixo\nDe baixo para cima\n\n\nDetalhes dos dados\nDados completos e detalhados\nPode manter dados resumidos"
  },
  {
    "objectID": "data_warehouse.html#data-warehouse---brasil-escola",
    "href": "data_warehouse.html#data-warehouse---brasil-escola",
    "title": "Conceitos de Data Warehouse",
    "section": "Data Warehouse - Brasil Escola",
    "text": "Data Warehouse - Brasil Escola\n\n5.1. DATA WAREHOUSE\nO conceito de Data Warehouse surgiu da necessidade das organizações em integrar os dados de diferentes servidores e máquinas em apenas um local e que essas informações servissem para que fossem gerados relatórios para as análises da empresa. O DW usa um modelo relacional dimensional, isto é, as informações estão dispostas de formas intuitivas, facilitando o acesso e a geração de resultados.\nOutro fator determinante para o desenvolvimento de um armazém de dados é o fato de que os modelos tradicionais, gerados pelos sistemas ERP, estão estruturados de forma transacional, o que dificulta gerar informações para as análises de resultados. De acordo com Singh, 2001: “O DW é a área de armazenamento de dados históricos e integrados destinados a sistemas de suporte à decisão”.\nAssim, Barbieri, 2001, conclui que:\nPode ser definido como um banco de dados, destinado a sistemas de apoio à tomada de decisão e cujos dados foram armazenados em estruturas lógicas dimensionais, possibilitando o seu processamento analítico por ferramentas especiais”.\nAs decisões para a utilização de um DW, parte do princípio que as informações precisam ser confiáveis para que as decisões não sejam tomadas de forma errônea. Como afirma Kimball, 2002: “Queremos que as pessoas usem informações para apoiar decisões mais baseadas em fatos”. As consultas e relatórios são acessados diretamente no DW, evitando dados sem confiabilidade dos provedores de informações originais.\n\n5.1.1. Características\n\n\n5.1.2. Orientado por Assunto\nOs bancos de dados transacionais, comumente possuem todos os dados das organizações dispostas em tabelas, isso faz com que os dados nem sempre serão de fácil análise. Os DW, por sua vez são orientados aos assuntos mais pertinentes para as empresas, como a análise de mercado de determinado produto ou veículo. Na figura 1, nota-se a diferença entre sistemas operacionais comum e após a criação do DW.\nFigura 1: Orientada por assunto\n\nFonte: INMON, 2005.\nBarbieri, 2001, nos alerta que a falta e objetivo, são primordiais para o fracasso de qualquer projeto, mas isso ainda é mais agravante no que diz respeito ao DW. É preciso também pensar no futuro, se necessário uma expansão no mesmo isso deve acontecer de forma relativamente fácil, pois se o projeto for mal estrutura, pode ser que o projeto futuramente tenha que partir do zero.\n\n\n5.1.3. Integrado\nÉ comum as organizações possuírem mais de uma representante, ou filial, sendo para desenvolver as mesmas ações ou de diferente tipo, fazendo com que em certos momentos as informações serão escritas de formas diferentes a qual está na matriz, ou representante principal. Ou então o sistema ERP é diferente, isso gera inconsistência nos dados, já que as informações são provenientes de mais de uma fonte.\nUma das características mais importantes do DW é a capacidade de ler essas informações e armazená-las de forma confiável. Independe de onde vem as informações, ou da data em que foi criado, a integração no DW sempre será consistente, por exemplo. Na inserção do CPF de clientes de um grupo de empresas, na organização X o CPF é inserido da seguinte forma: 999.999.999-99 já na empresa Y insere assim: 99999999999, a princípio pode ser uma questão simples, mas no momento em que for cruzar as informações, não será possível “linkar” um CPF com o outro. O DW irá transformar essas informações num modelo único.\n\n\n5.1.4. Histórico\nO que influência o tamanho do DW é principalmente as informações históricas das organizações, a forma mais eficaz de análise de tendência é tendo informações de anos anteriores para a comparação. Os dados históricos dos DW podem ter facilmente mais de 5 anos. Nos sistemas tradicionais, as informações remetem a posição atual dos dados no momento da pesquisa, ou num curto período de tempo, não sendo possível uma análise mais a fundo. É comum nos sistemas tradicionais a remoção de dados antigos para a liberação de espaço no banco de dados, isso não ocorre no DW pois as informações sempre serão importantes para as análises e projeção do futuro.\n\n\n5.1.5. Não Volátil\nHá apenas duas operações executadas no DW, a primeira delas é a transação de carga dos dados provenientes dos sistemas provedores de informações e a segunda é o processo de leitura dos dados para a geração de relatórios. Não é possível a escrita de dados nas dimensões do DW diretamente como acontece nos bancos de dados tradicionais, sendo apenas para a leitura, fazendo com que as informações permaneçam estáveis mesmo após longos períodos de tempo.\n\n\n5.1.6. Modelos\nTodas as características do DW, bem como suas diferenças aos sistemas tradicionais passam inteiramente pelos modelos a serem usados para a construção do novo repositório de dados. Os modelos, a grosso modo é a forma como os dados serão organizados e estruturados, como as entidades serão conectadas e como irão interagir entre si.\nAs opções de estrutura no DW variam das necessidades de cada caso, as mais comuns atualmente são o Modelo Estrela (Star Schema) e o Modelo Floco de Neve (Snow Flake), cada um com sua característica e limitação referente ao outro.\n\n\n5.1.7. Modelo Estrela\nO modelo estrela foi proposto por Ralph Kimball, para ser um modelo altamente redundante, onde todas as descrições seriam repetidas em cada dimensão. Sua estrutura é composta por uma tabela central de Fatos e um conjunto de tabelas ligadas a ela, que são chamadas de dimensões. As dimensões são compostas por eventos ou características do mesmo, enquanto a tabela fato, como o próprio nome diz, armazena os fatos ocorridos, por exemplo, o Fato é a venda de um veículo, as dimensões são as informações dessa venda, como a data que ocorreu, veículo vendido, valor da venda e assim por diante. Na figura 2 pode se verificar a estrutura do modelo estrela.\nFigura 2: Modelo Estrela.\n\n Fonte: MACHADO, 2004.\nSegundo Singh (2001) uma característica importante desse modelo é fato de suas dimensões serem desnormalizadas, isto gera várias duplicidades no banco, mas também garante confiabilidade nas consultas. Barbieri, 2001, cita como desvantagem do modelo estrela o fato de ele não ter uma perfeita coesão entre os Data Marts e um esforço redobrado na extração de dados, já que várias informações são duplicadas.\n\n\n5.1.8. Modelo Floco de Neve\nO modelo floco de neve também possui uma tabela Fatos ligada as entidades de dimensões, porém, ao contrário do que ocorre com o modelo estrela, há entidades relacionando entre si, isto diminui drasticamente o tamanho do DW, porém as consultas podem se tornar um pouco complicadas. Na figura 3 observa-se a estrutura do modelo Floco de neve.\nFigura 3: Modelo Floco de neve\n\nFonte: MACHADO, 2004.\nSegundo Machado (2004), O modelo floco de neve é o resultado da decomposição de uma ou mais dimensões que possuem hierarquias entre seus membros. O que difere também do modelo estrela é o fato das dimensões serem normalizadas, não ocorrendo as duplicidades que ocorre no modelo estrela, SINGH (2001) completa dizendo que normalizando os dados das tabelas dimensionais de um modelo estrela transforma o mesmo em um modelo floco de neve.\n\n\n5.1.9. Estrutura Do Data Warehouse\nSegundo o Kimball group, grupo especializado na concepção de Data Warehouse para Business Intelligence, a estrutura base para um Data warehouse seria, composta por três componentes, Data Sources, Data Staging Area e Data Presentation Area. Na figura 4 observa-se a organização básica de um DW.\nFigura 4: DW\n\nFonte: kimballgroup, 2002\n\n\n5.1.10. Data Source\nSão De Onde As Informações Serão Extraídas, A Fonte Dos Dados, Independentemente do servidor que está em uso, por exemplo, uma organização possui duas fontes de dados, uma baseada em SGBD Oracle e a outra em SQL server, o DW tem por função integrar esses dados em apenas um local de forma que possam ser cruzadas informações.\n\n\n5.1.11. Data Staging\nÉ onde estão armazenados os dados, é um meio termo entre o sistema operacional e a camada de apresentação, fazendo uso de conjuntos de processos chamados de ETL (Extração, Transformação e carregamento), ver seção 4.1.6 É onde ocorre a “higienização” dos dados, onde são definidas as estruturas, os fatos e as dimensões do DW. O Data Staging, não é de acesso livre aos usuários, pois não gera relatórios e nem consultas, ficando a cargo do Data Presentation Area.\nKIMBALL (2002), indica o requisito do Data Staging:\nO requisito de arquitetura chave para a Data Staging é que ela esteja fora do alcance dos usuários de negócios, não fornecendo serviços de consulta e apresentação.”\nFigura 5: Staging area\n\nFonte: MACHADO, 2004.\n\n\n5.1.12. Data Presentation Area\nÉ onde os dados estão organizados e prontos para serem acessados pelos usuários. Com base num modelo dimensional, os dados são acessados de forma intuitiva e as informações são consistentes. As ferramentas que possuem essa finalidade são conectadas ao cubo OLAP (ver seção 4.2) e com auxílio das operações apresentam os relatórios para o usuário final.\n\n\n5.1.13. Data Mart\nTendo como base o exemplo citado referente a venda de um determinado veículo, é possível extrair as informações dessa venda, porém se necessário obter várias vendas, com vários filtros essas informações tendem a se tornarem complexas para se analisar e consequentemente para tomar decisões, em razão disso, a ideia de um diretório exclusivo a determinado assunto dentro do próprio DW, ou extraindo informações diretamente das fontes de dados, acabou se tornando bastante necessário.\nSegundo Silva (2003), há um consenso sobre Data marts:\n“Há um consenso entre os fornecedores de soluções de Data Warehouse. A idéia é começar pequeno, mas pensando grande. E é o que está acontecendo. Na maioria dos casos, as empresas que optam pelo Data Warehouse iniciam o processo a partir de uma área específica da empresa para depois ir crescendo aos poucos.”\nBarbieri (2001), nos elucida que, Data Mart é um depósito de dados que atende as áreas especificas da empresa, ou seja, separa por assuntos os dados coletados, Primak (2008) complementa dizendo que pode-se dividir um DW em vários Data Marts, com seus determinados assuntos e diminuindo o tempo de resposta e facilitando o acesso a essas informações, no cenário da venda de veículos, temos a tabela principal, que no caso é a “fatovendas” que tem todas as informações de vendas de determinado veículo, mas é possível também a criação de Data Marts específicos, como “fatorh”, que terá as informações do recursos humanos da organização, “fatofluxo”, responsável por catalogar o fluxo de clientes e assim por diante, mas ainda assim, possibilitando cruzar essas informações utilizando operações (ver seção 4.2.1), por exemplo, é possível emitir um dashboard das vendas pelo fluxo de clientes, podendo ter ideia de quantos clientes são necessários para finalizar uma venda, ou então traçar uma linha do tempo das vendas, pelo valor investido em marketing naquele período. Na figura 6, nota-se como se estrutura o Data Mart em um DW.\nFigura 6: Organização do Data Mart\n\nFonte: NERY, 2007.\n\n\n5.1.14. Data Mining\nA diferença básica do BI para o Data mining se refere a quem é direcionado a atuação. O BI tem como base fornecer as informações para o nível estratégico da organização, onde se tomam as decisões a níveis gerenciais, já o Data mining fornece as informações em níveis menores, são usadas principalmente na área de atuação, ou no plano tático da empresa.\nO DM é constituído por um conjunto de 3 conceitos básicos para o seu sucesso, são eles:\n\nEstatística\nInteligencia artificial\nMachine learning\n\nO Machine Learning nada mais é que a combinação dos dois primeiros conceitos, é a grosso modo o fato do DM “aprender com os dados”, com cálculos estatísticos e com uma boa inteligência artificial é possível que o software trace caminhos sozinhos para a obtenção de resultados.\n\n\n5.1.15. ETL – Extract, Transform And Load\nÉ o processo de extração dos dados de fontes externas para o Data Warehouse, transformando suas tabelas e informações em dimensões e fatos no DW e carregando todas as informações de forma “limpa” e consistente no banco em questão.\nFigura 7: ETL\n\nFONTE: CANALTECH, 2014.\nSegundo KIMBALL (1998), ETL é o Conjunto de processos pelos quais os dados de origem operacional são preparados para o Data Warehouse. KIMBALL (1998) completa, é o processo mais crítico e demorado na construção de um DW, podendo levar até 60% do total de horas da implementação do projeto. Pois os modelos relacionais, nem sempre dispõe de uma arquitetura que facilite isso, possuindo também o fator de quantidade de dados que o sistema possui.\nBarbieri, 2001, diz que o conceito do processo ETL, pode ser dividido em:\n\nFiltro de dados\n\nOs bancos de dados comuns não são normalizados, isto faz com que os dados possam ter informações indesejáveis, o papel do ETL nessa etapa é filtrar e não carregar no DW essas informações.\n\nIntegração de dados\n\nFazer com que todas as informações a determinado assunto sejam correlacionadas, independente se ela está num sistema no banco de dados, ou em planilhas locais.\n\nCondensação de dados\n\nÉ condensar as informações de forma sumariada, ou seja, as vendas de determinado dia, precisam sempre estar presentes juntamente com as outras vendas do mesmo dia.\n\nConversão de dados\n\nCada banco de dados apresentam as informações de formas distintas, podendo ser uma virgula ao invés de um ponto, ou até o símbolo de moeda ser diferente, o ETL converte esse modelo em um outro modelo padrão do DW.\n\nDerivação de dados\n\nContinuação da conversão de dados, mas atuando apenas nas informações, não em como o modelo é empregado.\nUma boa ferramenta de ETL deve ser capaz de se adaptar as mais formas de banco de dados, suas linguagens e seus formatos. Atualmente a oferta de ferramentas de ETL é bastante elevada, empresas que possuem ferramentas de BI, ou ferramentas para a construção de um DW normalmente disponibilizam um software especifico para a função, como é o caso SQL Server Integration Services e do Pentaho Data Integration, Ferramentas ETL dos softwares da Microsoft e da Pentaho, respectivamente."
  },
  {
    "objectID": "documentos.html",
    "href": "documentos.html",
    "title": "Documentos",
    "section": "",
    "text": "b) Fundamentos de Banco de Dados: Diferenciação entre bancos relacionais, multidimensionais, documentos e grafos."
  },
  {
    "objectID": "documentos.html#banco-de-dados---introdução-a-bancos-de-dados-não-relacionais---nosql",
    "href": "documentos.html#banco-de-dados---introdução-a-bancos-de-dados-não-relacionais---nosql",
    "title": "Documentos",
    "section": "Banco de Dados - Introdução a Bancos de Dados Não Relacionais - NoSQL",
    "text": "Banco de Dados - Introdução a Bancos de Dados Não Relacionais - NoSQL"
  },
  {
    "objectID": "documentos.html#o-que-é-um-banco-de-dados-de-documentos",
    "href": "documentos.html#o-que-é-um-banco-de-dados-de-documentos",
    "title": "Documentos",
    "section": "O que é um banco de dados de documentos?",
    "text": "O que é um banco de dados de documentos?\nUm banco de dados de documentos é um tipo de banco de dados não relacional projetado para armazenar e consultar dados como documentos do tipo JSON. Os bancos de dados de documentos facilitam para que os desenvolvedores armazenem e consultem dados usando o mesmo formato de modelo de documento que usam no código do aplicativo. A natureza flexível, semiestruturada e hierárquica dos documentos e dos bancos de dados de documentos permite que eles evoluam conforme as necessidades dos aplicativos. O modelo de documentos funciona bem com casos de uso como catálogos, perfis de usuários e sistemas de gerenciamento de conteúdo, onde cada documento é único e evolui com o passar do tempo. Os bancos de dados de documentos possibilitam uma indexação flexível, consultas ad hoc eficientes e análises de dados em grupos de documentos.\nNo exemplo a seguir, um documento semelhante ao JSON descreve um livro.\n[\n    {\n        \"year\" : 2013,\n        \"title\" : \"Turn It Down, Or Else!\",\n        \"info\" : {\n            \"directors\" : [ \"Alice Smith\", \"Bob Jones\"],\n            \"release_date\" : \"2013-01-18T00:00:00Z\",\n            \"rating\" : 6.2,\n            \"genres\" : [\"Comedy\", \"Drama\"],\n            \"image_url\" : \"http://ia.media-imdb.com/images/N/O9ERWAU7FS797AJ7LU8HN09AMUP908RLlo5JF90EWR7LJKQ7@@._V1_SX400_.jpg\",\n            \"plot\" : \"A rock band plays their music at high volumes, annoying the neighbors.\",\n            \"actors\" : [\"David Matthewman\", \"Jonathan G. Neff\"]\n        }\n    },\n    {\n        \"year\": 2015,\n        \"title\": \"The Big New Movie\",\n        \"info\": {\n            \"plot\": \"Nothing happens at all.\",\n            \"rating\": 0\n        }\n    }\n]\n\nCasos de uso\nGerenciamento de conteúdo\nUm banco de dados de documentos é uma ótima opção para aplicativos de gerenciamento de conteúdo, como blogs e plataformas de vídeo. Com um banco de dados de documentos, cada entidade que o aplicativo rastrear pode ser armazenado como um documento único. Um desenvolvedor consegue atualizar um aplicativo de maneira mais intuitiva no banco de dados de documentos, à medida que as exigências evoluem. Além disso, se o modelo de dados precisar mudar, somente os documentos afetados precisarão ser atualizados. Nenhuma atualização de esquema é exigida e nenhum tempo de inatividade de banco de dados é necessário para fazer as alterações. \nCatálogos\nBancos de dados de documentos são eficientes e eficazes para o armazenamento de informações de catálogo. Por exemplo, em um aplicativo de comércio eletrônico, diferentes produtos costumam ter números de atributos diferentes. Gerenciar milhares de atributos em bancos de dados relacionais é ineficiente e afeta a performance de leitura. Ao usar um banco de dados de documentos, os atributos de cada produto podem ser descritos em um único documento para gerenciamento fácil e maior velocidade de leitura. Alterar os atributos de um produto não afetará os outros.\n\nFontes:\nhttps://aws.amazon.com/pt/nosql/document/"
  },
  {
    "objectID": "fundamentos_bd.html",
    "href": "fundamentos_bd.html",
    "title": "Fundamentos de Banco de Dados",
    "section": "",
    "text": "b) Fundamentos de Banco de Dados: Conceitos: Sistemas de gerência de banco de dados (SGBD), Arquitetura, modelos lógicos e representação física; Organização física e métodos de acesso; Conceito de transação, concorrência, recuperação, integridade; Linguagens de definição (DDL) e manipulação de dados (DML) em SGBDs relacionais; Procedimentos (stored procedures), funções (functions), visões (views), visões materializadas (materialized views) e gatilhos (triggers), Linguagem de consulta estruturada (SQL, Avaliação de modelos de dados, Técnicas de engenharia reversa para criação e atualização de modelos de dados, Integração dos dados (ETL, Transferência de Arquivos e Integração via Base de Dados, Data Lakes e Soluções para Big Data, Diferenciação entre bancos relacionais, multidimensionais, documentos e grafos."
  },
  {
    "objectID": "gerenciamento_projetos.html",
    "href": "gerenciamento_projetos.html",
    "title": "Gerenciamento de Projetos",
    "section": "",
    "text": "f) Gerenciamento de Projetos: Conceitos básicos do PMBOK 7ª Edição, Metodologia SCRUM;"
  },
  {
    "objectID": "governanca_ti.html",
    "href": "governanca_ti.html",
    "title": "Governança de TI",
    "section": "",
    "text": "g) Governança de TI: ITIL versão 4 (ITIL 4): Operação de Serviços (Gerenciamento de Eventos, Gerenciamento de Incidentes, Gerenciamento de Problemas, Cumprimento de Requisições, Gerenciamento de Acessos), Desenho de Serviços (Gerenciamento de Níveis de Serviço, Gerenciamento de Capacidade, Gerenciamento de Disponibilidade, Gerenciamento de Continuidade de Serviços de TI, Gerenciamento de Continuidade de Negócio), Transição de Serviços (Gerenciamento de Configuração e Ativos de Serviços de TI, Gerenciamento de Liberação e Implantação, Gerenciamento de Mudanças), Melhoria Contínua de Serviços, Métricas (Fatores Críticos de Sucesso - CSFs, Índices Chave de Performance - KPIs)."
  },
  {
    "objectID": "grafos.html",
    "href": "grafos.html",
    "title": "Grafos",
    "section": "",
    "text": "b) Fundamentos de Banco de Dados: Diferenciação entre bancos relacionais, multidimensionais, documentos e grafos."
  },
  {
    "objectID": "grafos.html#estrutura-de-dados---aula-23---grafos---conceitos-básicos",
    "href": "grafos.html#estrutura-de-dados---aula-23---grafos---conceitos-básicos",
    "title": "Grafos",
    "section": "Estrutura de Dados - Aula 23 - Grafos - Conceitos básicos",
    "text": "Estrutura de Dados - Aula 23 - Grafos - Conceitos básicos"
  },
  {
    "objectID": "grafos.html#banco-de-dados-de-grafos-definido",
    "href": "grafos.html#banco-de-dados-de-grafos-definido",
    "title": "Grafos",
    "section": "Banco de dados de grafos definido",
    "text": "Banco de dados de grafos definido\nUm banco de dados de grafos é definido como uma plataforma especializada e de uso único para criar e manipular grafos. Os grafos contêm nós, arestas e propriedades, todos usados para representar e armazenar dados de uma forma que os bancos de dados relacionais não estão equipados a fazer.\nAnálise de grafos é outro termo comumente usado e se refere especificamente ao processo de análise de dados em um formato de gráfico usando pontos de dados como nós e relacionamentos como arestas. A análise de grafos requer um banco de dados com suporte a formatos de grafos; pode ser um banco de dados de grafos dedicado ou um banco de dados convergente com suporte a vários modelos de dados, incluindo grafos.\nAprenda a usar grafos com um workshop passo a passo\n\nTipos de banco de dados de grafos\nHá dois modelos populares de bancos de dados de grafos: grafos de propriedades e grafos RDF. O grafo de propriedades foca análises e consultas, enquanto o grafo RDF enfatiza a integração de dados. Ambos os tipos de grafos consistem em uma coleção de pontos (vértices) e nas conexões entre esses pontos (bordas). Mas também existem diferenças.\n\nGráficos de propriedades\nOs grafos de propriedades são usados para modelar relacionamentos entre dados e permitem a consulta e a análise de dados com base nesses relacionamentos. Um grafo de propriedades tem vértices que podem conter informações detalhadas sobre um assunto e bordas que denotam a relação entre os vértices. Os vértices e bordas podem ter atributos, chamados propriedades, com os quais estão associados.\nNeste exemplo, um conjunto de colegas e seus relacionamentos são representados como um grafo de propriedades.\n\nGraças à sua versatilidade, os grafos de propriedades são usados em várias indústrias e setores, como finanças, manufatura, segurança pública, varejo e muitos outros.\n\n\nGrafos RDF\nOs grafos RDF (RDF significa Resource Description Framework) cumprem um conjunto de padrões W3C (Worldwide Web Consortium) criados para representar instruções e são ideias para representar metadados complexos e dados mestre. Eles costumam ser usados para dados vinculados, integração de dados e grafos de conhecimento. Eles podem representar conceitos complexos em um domínio ou fornecer semântica avançada e inferência de dados.\nNo modelo RDF uma declaração é representada por três elementos: dois vértices conectados por uma borda refletindo o assunto, o predicado e o objeto de uma frase - isso é conhecido como um triplo RDF. Cada vértice e borda são identificados por um URI único, ou Identificador de Recurso Único. O modelo RDF fornece uma maneira de publicar dados em um formato padrão com semântica bem definida, permitindo a troca de informações. Agências de estatísticas governamentais, empresas farmacêuticas e organizações de saúde adotaram amplamente os grafos RDF.\n\n\n\nComo grafos e bancos de dados de grafos funcionam\nGrafos e bancos de dados de grafos fornecem modelos de grafos para representar relacionamentos nos dados. Eles permitem que os usuários executem consultas de travessia com base em conexões e apliquem algoritmos de grafos para encontrar padrões, caminhos, comunidades, influenciadores, pontos únicos de falha e outros relacionamentos, resultando em uma análise mais eficiente em escala em grandes volumes de dados. O poder dos grafos está na análise, nos insights que fornecem e na capacidade de vincular fontes de dados diferentes.\nQuando se trata de analisar grafos, os algoritmos exploram os caminhos e distâncias entre os vértices, a importância dos vértices e o clustering dos vértices. Por exemplo, para determinar a importância, os algoritmos costumam examinar bordas de entrada, a importância de vértices vizinhos e outros indicadores.\nAlgoritmos de gráfico, operações projetadas especificamente para analisar relacionamentos e comportamentos entre dados em gráficos, tornam possível entender coisas que são difíceis de ver com outros métodos. Quando se trata de analisar grafos, os algoritmos exploram os caminhos e distâncias entre os vértices, a importância dos vértices e o clustering dos vértices. Os algoritmos costumam examinar bordas de entrada, a importância de vértices vizinhos e outros indicadores para ajudar a determinar a importância. Por exemplo, algoritmos de grafos podem identificar qual indivíduo ou item está mais conectado a outros nas redes sociais ou processos comerciais. Os algoritmos podem identificar comunidades, anomalias, padrões comuns e caminhos que conectam indivíduos ou transações relacionadas.\nComo os bancos de dados de grafos armazenam relacionamentos de forma explícita, as consultas e os algoritmos que utilizam a conectividade entre vértices podem ser executados em subsegundos em vez de horas ou dias. Os usuários não precisam executar inúmeras junções e os dados podem ser usados com mais facilidade para análise e aprendizado de máquina para descobrir mais sobre o mundo ao nosso redor.\n\n\nVantagens de bancos de dados de grafos\nO formato de grafo fornece uma plataforma mais flexível para encontrar conexões distantes ou analisar dados com base em aspectos como força ou qualidade do relacionamento. Os grafos permitem que você explore e descubra conexões e padrões em redes sociais, IoT, big data, data warehouses e também dados de transações complexas para vários casos de uso de negócios, incluindo detecção de fraude em bancos, descoberta de conexões em redes sociais e cliente 360. Hoje, grafos são cada vez mais usados como parte da ciência de dados para tornar as conexões em relacionamentos mais claras.\nComo os bancos de dados de grafos armazenam os relacionamentos de forma explícita, as consultas e os algoritmos que utilizam a conectividade entre vértices podem ser executados em subsegundos em vez de horas ou dias. Os usuários não precisam executar inúmeras junções e os dados podem ser usados com mais facilidade para análise e aprendizado de máquina para descobrir mais sobre o mundo ao nosso redor.\nOs bancos de dados de grafos são uma ferramenta extremamente flexível e poderosa. Devido ao formato do grafo, relacionamentos complexos podem ser determinados para insights mais profundos com muito menos esforço. Os bancos de dados de grafos geralmente executam consultas em linguagens como Property Graph Query Language (PGQL). O exemplo abaixo mostra a mesma consulta em PGQL e SQL.\n\nComo visto no exemplo acima, o código PGQL é mais simples e muito mais eficiente. Como os grafos enfatizam as relações entre os dados, eles são ideais para vários tipos diferentes de análises. Em particular, os bancos de dados de grafos se destacam em:\n\nEncontrar o caminho mais curto entre dois nós\nDeterminar os nós que criam a maior atividade/influência\nAnalisar a conectividade para identificar os pontos mais fracos de uma rede\nAnalisar o estado da rede ou comunidade com base na distância/densidade de conexão em um grupo\n\n\n\nComo funcionam bancos de dados de grafos e análise de grafos\nUm exemplo simples de análise de grafos em ação é a imagem abaixo, que mostra uma representação visual do popular jogo de festa “Six Degrees of Kevin Bacon”. Para os novatos, este jogo envolve a criação de conexões entre Kevin Bacon e outro ator com base em uma cadeia de filmes mútuos. Essa ênfase nos relacionamentos se torna a maneira ideal de demonstrar a análise de grafos.\nImagine um conjunto de dados com duas categorias de nós: cada filme já feito e cada ator que esteve nesses filmes. Em seguida, usando o grafo, executamos uma consulta pedindo para conectar Kevin Bacon ao ícone Muppet, Miss Piggy. O resultado seria o seguinte:\n\nNeste exemplo, os nós disponíveis (vértices) são atores e filmes e os relacionamentos (arestas) são o status de “atuado”. A partir daqui, a consulta retorna os seguintes resultados:\n\nKevin Bacon atuou em The River Wild com Meryl Streep.\nMeryl Streep atuou em Lemony Snicket’s A Series of Unfortunate Events, com Billy Connolly.\nBilly Connolly atuou em Muppet Treasure Island com Miss Piggy.\n\nOs bancos de dados de grafos podem consultar muitos relacionamentos diferentes para este exemplo de Kevin Bacon, como:\n\n“Qual é a corrente mais curta para conectar Kevin Bacon a Miss Piggy?” (análise do caminho mais curto, conforme usado no jogo Six Degrees acima)\n“Quem trabalhou com o maior número de atores?” (grau de centralidade)\n“Qual é a distância média entre Kevin Bacon e todos os outros atores?” (centralidade de proximidade)\n\nEste é, obviamente, um exemplo mais divertido do que a maioria dos usos de análise de grafos. Mas essa abordagem funciona em quase todos os big data - qualquer situação em que um grande número de registros mostre uma conectividade natural entre si. Algumas das formas mais populares de usar a análise de grafos são para analisar redes sociais, redes de comunicação, tráfego e uso do site, dados de estradas do mundo real, e transações e contas financeiras.\n\n\nCaso de uso do banco de dados de grafos: lavagem de dinheiro\n\nConceitualmente, a lavagem de dinheiro é simples. O dinheiro sujo é repassado para misturá-lo com fundos legítimos e, depois, transformá-lo em ativos tangíveis. Esse é o tipo de processo usado na análise dos Panama Papers.\nEspecificamente, uma transferência de dinheiro circular envolve um criminoso que envia grandes quantias de dinheiro obtidas de forma fraudulenta para si mesmo, mas oculta isso por meio de uma longa e complexa série de transferências válidas entre contas “normais”. Essas contas “normais” são, na verdade, contas criadas com identidades sintéticas. Eles normalmente compartilham certas informações semelhantes porque são geradas a partir de identidades roubadas (endereços de email, endereços etc.) e são essas informações relacionadas que tornam a análise de grafos tão boa para fazê-las revelar suas origens fraudulentas.\nPara simplificar a detecção de fraude, os usuários podem criar um grafo de transações entre entidades e entidades que compartilham algumas informações, incluindo os endereços de email, senhas, endereços e muito mais. Após a criação de um grafo, uma consulta simples encontrará todos os clientes com contas com informações semelhantes e revelará quais contas enviam dinheiro entre si.\n\n\nCaso de uso do banco de dados de grafos: análise de mídia social\nOs bancos de dados de grafos podem ser usados em muitos cenários diferentes, mas são comumente usados para analisar redes sociais. Na verdade, as redes sociais são o caso de uso ideal, pois envolvem um grande volume de nós (contas de usuário) e conexões multidimensionais (engajamentos em muitas direções diferentes). Uma análise de grafos de uma rede social pode determinar:\n\nQuão ativos são os usuários? (número de nós)\nQuais usuários têm maior influência? (densidade de conexões)\nQuem tem o engajamento mais bidirecional? (direção e densidade de conexões)\n\nNo entanto, essa informação é inútil se tiver sido distorcida de maneira não natural por bots. Felizmente, a análise de grafos pode fornecer um meio excelente para identificar e filtrar bots.\nEm um caso de uso do mundo real, a equipe Oracle usou o Oracle Marketing Cloud para avaliar a publicidade e a tração nas mídias sociais, especificamente, para identificar contas de bot falsas que distorcem os dados. O comportamento mais comum desses bots envolvia retuitar contas-alvo, aumentando artificialmente sua popularidade. Uma análise de padrão simples permitiu uma olhada usando a contagem de retuítes e densidade de conexões com os vizinhos. Naturalmente, contas populares mostraram relacionamentos diferentes com os vizinhos em comparação com contas controladas por bot.\nEsta imagem mostra contas naturalmente populares.\n\nE esta imagem mostra o comportamento de uma conta controlada por bot.\n\nA chave aqui é usar o poder da análise de grafos para identificar um padrão natural versus um padrão de bot. A partir daí, é tão simples quanto filtrar essas contas, embora também seja possível cavar mais fundo para examinar, por exemplo, a relação entre bots e contas retuitadas.\nAs plataformas de redes sociais fazem o melhor para eliminar contas de bot, pois afetam a experiência geral da base de usuários. Para verificar se esse processo de detecção de bot estava correto, contas sinalizadas foram verificadas após um mês. Os resultados foram os seguintes:\n\nSuspenso: 89%\nExcluído: 2,2%\nAinda ativo: 8,8%\n\nEssa porcentagem extremamente alta de contas punidas (91,2%) mostrou a precisão tanto da identificação do padrão quanto do processo de limpeza. Isso levaria muito mais tempo em um banco de dados tabular padrão, mas com a análise de grafos, é possível identificar padrões complexos rapidamente.\n\n\nCaso de uso do banco de dados de grafos: fraude com cartão de crédito\nOs bancos de dados de grafos se tornaram uma ferramenta poderosa no setor financeiro como meio de detecção de fraudes. Apesar dos avanços na tecnologia antifraude, como o uso de chips embutidos em cartões, a fraude ainda pode ocorrer de várias maneiras. Dispositivos de leitura podem roubar detalhes de fitas magnéticas, uma técnica comumente usada em locais que ainda não instalaram leitores de chip. Depois que esses detalhes são armazenados, eles podem ser carregados em um cartão falsificado para fazer compras ou sacar dinheiro.\nComo meio de detecção de fraude, a identificação de padrões costuma ser a primeira linha de defesa. Os padrões de compra esperados são baseados na localização, frequência, tipos de lojas e outras coisas que se enquadram no perfil do usuário. Quando algo parece totalmente anormal, por exemplo, uma pessoa que fica a maior parte do tempo na área da baía de San Francisco, na Califórnia, repentinamente faz compras noturnas na Flórida, isso sinaliza como potencialmente fraudulento.\nO poder de computação necessário para isso é simplificado significativamente com a análise de grafos. A análise de grafos se destaca no estabelecimento de padrões entre nós, neste caso, as categorias de nós são definidas como contas (titulares de cartão), locais de compra, categoria de compra, transações e terminais. É fácil identificar padrões de comportamento naturais; por exemplo, em um determinado mês, uma pessoa poderia:\n\nComprar ração (categoria de compra) em diferentes lojas de animais (terminais)\nPagar por restaurantes nos finais de semana (metadados de transações) na região (locais de compra)\nComprar equipamento de reparo (categoria de compra) em uma loja de local de reparo (local da conta, local da compra)\n\nA detecção de fraude é normalmente tratada com machine learning (aprendizado de máquina), mas a análise de grafos pode complementar esse esforço para criar um processo mais preciso e eficiente. Graças ao foco nos relacionamentos, os resultados se tornaram indicadores eficazes na determinação e sinalização de registros fraudulentos, realizando curadoria e preparando os dados antes que eles possam ser realmente usados.\nPara saber mais sobre outros casos de uso de banco de dados de grafos, baixe o ebook gratuito (PDF)\n\n\nO futuro de bancos de dados de grafos\nBancos de dados de grafos e técnicas de grafos acompanharam o aumento do poder de computação e de big data na última década. Na verdade, está ficando cada vez mais claro que eles se tornarão a ferramenta padrão para analisar um admirável mundo novo de relacionamentos de dados complexos. À medida que as empresas e organizações continuam promovendo os recursos de big data e análise, a capacidade de derivar insights de maneiras cada vez mais complexas, torna os bancos de dados de grafos essenciais para as necessidades de hoje e os sucessos de amanhã.\nA Oracle facilita a adoção de tecnologias de grafos. O Oracle Database e o Oracle Autonomous Database incluem um banco de dados de grafos e um mecanismo de análise de grafos para que os usuários possam descobrir mais insights sobre dados usando o poder dos algoritmos de grafos, consultas de correspondência de padrões e visualização. Grafos fazem parte do banco de dados convergente da Oracle, que oferece suporte a requisitos de multimodelo, multicarga de trabalho e multilocatário – tudo em um único mecanismo de banco de dados.\nEmbora todos os bancos de dados de grafos aleguem seu alto desempenho, as ofertas de grafos da Oracle são eficientes tanto em desempenho quanto em algoritmos de consulta, além de serem estreitamente integradas ao banco de dados da Oracle. Isso facilita para os desenvolvedores a adição de análises de grafos aos aplicativos existentes e o uso da escalabilidade, consistência, recuperação, controle de acesso e segurança oferecidos pelo banco de dados por padrão."
  },
  {
    "objectID": "grafos.html#o-que-é-um-banco-de-dados-de-grafos",
    "href": "grafos.html#o-que-é-um-banco-de-dados-de-grafos",
    "title": "Grafos",
    "section": "O que é um banco de dados de grafos?",
    "text": "O que é um banco de dados de grafos?\n\nDefinição do banco de dados de grafos\nOs bancos de dados de grafos foram criados especificamente para possibilitar o armazenamento de relacionamentos e a navegação por eles. Os relacionamentos são elementos distintos que agregam a maior parte do valor para os bancos de dados de grafos. Os bancos de dados de grafos usam nós para armazenar entidades de dados e arestas para armazenar os relacionamentos entre as entidades. Uma aresta tem sempre um nó inicial, um nó final, um tipo e um direcionamento, o que possibilita a descrição dos relacionamentos entre pais e filhos, das ações, das propriedades e assim por diante. A quantidade e os tipos de relacionamentos que um nó pode ter são ilimitados.\n\nUm grafo em um banco de dados de grafos pode ser cruzado com tipos de arestas específicos ou por todo o grafo. Nos bancos de dados de grafos, o cruzamento das associações ou dos relacionamentos ocorre muito rapidamente, uma vez que os relacionamentos entre os nós não são calculados no momento das consultas, mas persistem no banco de dados. Os bancos de dados de grafos são vantajosos em casos de uso como redes sociais, mecanismos de recomendação e detecção de fraudes, em que é necessário criar relacionamentos entre os dados e consultar rapidamente esses relacionamentos.\nO grafo a seguir é um exemplo de grafo de rede social. Considerando as pessoas (nós) e seus relacionamentos (arestas), é possível descobrir quem são os “amigos dos amigos” de uma pessoa específica, por exemplo, os amigos dos amigos de Howard. \n\n\nCasos de uso\n\nDetecção de fraudes\nOs bancos de dados de grafos podem fazer uma prevenção sofisticada contra fraudes. Com os bancos de dados de grafos, você pode usar relacionamentos para processar transações financeiras e de compras praticamente em tempo real. Com consultas de grafos rápidas, você pode detectar, por exemplo, se um possível comprador está usando o mesmo endereço de e-mail e cartão de crédito que o usado em um caso de fraude conhecido. Os bancos de dados de grafos também podem ajudá-lo a detectar facilmente padrões de relacionamento, como várias pessoas associadas a um endereço de e-mail pessoal ou várias pessoas compartilhando o mesmo endereço IP, mas residentes em endereços físicos diferentes. \n\n\nMecanismos de recomendação\nOs bancos de dados de grafos são uma boa opção para aplicativos de recomendação. Com os bancos de dados de grafos, você pode armazenar em um grafo os relacionamentos entre as categorias de informação, como os interesses, os amigos e o histórico de compras dos clientes. Você pode usar um banco de dados de grafos altamente disponível para fazer recomendações de produtos a um usuário com base em quais produtos foram comprados por outras pessoas que seguem o mesmo esporte e têm histórico de compras similar. Além disso, é possível identificar pessoas com um amigo em comum, mas que ainda não se conhecem, para fazer uma recomendação de amizade. \n\nFontes\nhttps://www.oracle.com/br/autonomous-database/what-is-graph-database/\nhttps://aws.amazon.com/pt/nosql/graph/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resumo",
    "section": "",
    "text": "Material de estudo para concurso do BRDE, cargo Analista de Sistemas – Subárea Ciência de Dados.\nREQUISITO: Diploma de graduação, devidamente registrado, em pelo menos um dos seguintes cursos de nível superior: Análise de Sistemas; Tecnologia da Informação; Sistemas de Informação; Processamento de Dados; Ciência da Computação; Engenharia da Computação; Engenharia de Sistemas; Bacharelado em Informática; Administração com Ênfase em Análise de Sistemas.\nEdital\nData da prova: 12/03/2023"
  },
  {
    "objectID": "index.html#conhecimentos-específicos",
    "href": "index.html#conhecimentos-específicos",
    "title": "Resumo",
    "section": "Conhecimentos Específicos",
    "text": "Conhecimentos Específicos\n\nCargo C07: Analista de Sistemas – Ciência de Dados\na) Banco de Dados: Fundamentos de administração de dados: Segurança; Modelagem de dados: Modelo entidade-relacionamento (entidades, atributos, chaves e relacionamentos) e Normalização;\nb) Fundamentos de Banco de Dados: Conceitos: Sistemas de gerência de banco de dados (SGBD), Arquitetura, modelos lógicos e representação física; Organização física e métodos de acesso; Conceito de transação, concorrência, recuperação, integridade; Linguagens de definição (DDL) e manipulação de dados (DML) em SGBDs relacionais; Procedimentos (stored procedures), funções (functions), visões (views), visões materializadas (materialized views) e gatilhos (triggers), Linguagem de consulta estruturada (SQL, Avaliação de modelos de dados, Técnicas de engenharia reversa para criação e atualização de modelos de dados, Integração dos dados (ETL, Transferência de Arquivos e Integração via Base de Dados, Data Lakes e Soluções para Big Data, Diferenciação entre bancos relacionais, multidimensionais, documentos e grafos.\nc) Análise de dados e informações: Dado, informação, conhecimento e inteligência; Conceitos, fundamentos, características, técnicas e métodos de Business Intelligence (BI); Mapeamento de fontes de dados, Dados estruturados e dados não estruturados, Conceitos de OLAP e suas operações, Conceitos de Data Warehouse. Técnicas de modelagem e otimização de bases de dados multidimensionais, Construção de relatórios e dashboards interativos em ferramentas de BI, Manipulação de dados em planilhas, Geração de insights a partir de relatórios e dashboards, BI como suporte a processos de tomada decisão, Conceitos Básicos em Séries Temporais, Conceitos Básicos de estatística descritiva, probabilística e testes de hipótese, Manipulação, tratamento e visualização de dados, Tratamento de dados faltantes, Tratamento de dados categóricos, Normalização numérica, Detecção e tratamento de outliers;\nd) Aprendizado de máquina (machine learning): Regressão Linear e Regressão Logística, Classificação, Métricas de avaliação, Overfitting e underfitting de modelos, Regularização; Seleção de modelos: Erro de Generalização, Validação Cruzada, Conjuntos de Treino, Validação e Teste; Conceitos de aprendizado não supervisionado, Clustering, Árvores de decisão e random forests, Máquina de suporte de vetores (SVM), Naive Bayes, K-NN;\ne) Plataforma de BI Microsoft: SQL Server, SQL Server Management Studio, SQL Server Integration Services, SQL Server Analysis Services, Power BI Report Server, Power BI Desktop, Serviço Power BI em Nuvem;\nf) Gerenciamento de Projetos: Conceitos básicos do PMBOK 7ª Edição, Metodologia SCRUM;\ng) Governança de TI: ITIL versão 4 (ITIL 4): Operação de Serviços (Gerenciamento de Eventos, Gerenciamento de Incidentes, Gerenciamento de Problemas, Cumprimento de Requisições, Gerenciamento de Acessos), Desenho de Serviços (Gerenciamento de Níveis de Serviço, Gerenciamento de Capacidade, Gerenciamento de Disponibilidade, Gerenciamento de Continuidade de Serviços de TI, Gerenciamento de Continuidade de Negócio), Transição de Serviços (Gerenciamento de Configuração e Ativos de Serviços de TI, Gerenciamento de Liberação e Implantação, Gerenciamento de Mudanças), Melhoria Contínua de Serviços, Métricas (Fatores Críticos de Sucesso - CSFs, Índices Chave de Performance - KPIs)."
  },
  {
    "objectID": "index.html#conhecimentos-básicos",
    "href": "index.html#conhecimentos-básicos",
    "title": "Resumo",
    "section": "Conhecimentos Básicos",
    "text": "Conhecimentos Básicos\n\nLÍNGUA PORTUGUESA\n\n1. Leitura e compreensão de textos:\n1.1 Assunto. 1.2 Estruturação do texto. 1.3 Ideias principais e secundárias. 1.4 Relação entre as ideias. 1.5 Efeitos de sentido. 1.6 Figuras de linguagem. 1.7 Recursos de argumentação. 1.8 Informações implícitas: pressupostos e subentendidos. 1.9 Coesão e coerência textuais.\n\n\n2. Léxico:\n2.1 Significação de palavras e expressões no texto. 2.2 Substituição de palavras e de expressões no texto. 2.3 Estrutura e formação de palavras.\n\n\n3. Aspectos linguísticos:\n3.1 Relações morfossintáticas. 3.2 Ortografia: emprego de letras e acentuação gráfica sistema oficial vigente (inclusive o Acordo Ortográfico vigente, conforme Decreto 7.875/12). 3.3 Relações entre fonemas e grafias. 3.4 Flexões e emprego de classes gramaticais. 3.5 Vozes verbais e sua conversão. 3.6 Concordância nominal e verbal. 3.7 Regência nominal e verbal (inclusive emprego do acento indicativo de crase). 3.8 Coordenação e subordinação: emprego das conjunções, das locuções conjuntivas e dos pronomes relativos. 3.9 Pontuação.\n\n\n\nLÍNGUA INGLESA\n\nReading Comprehension.\nSimple and compound sentences: a. Noun clauses; b. Relative clauses;\n\n\n\nClause combinations – coordinators and subordinators; d. Conditional sentences;\n\n\n\nNouns: a. Compound nouns; b.Countable/ uncountable nouns;\nArticles.\nPronouns.\nAdjectives.\nAdverbs.\nPrepositions and phrasal verbs.\nVerbs.\nWord order.\nVocabulary and false friends.\nCollocations.\nPronunciation.\n\n\n\nRACIOCÍNIO LÓGICO/ANALÍTICO/QUANTITATIVO\n\nEstrutura lógica de relações arbitrárias entre pessoas, lugares, objetos ou eventos fictícios; deduzir novas informações das relações fornecidas e avaliar as condições usadas para estabelecer a estrutura daquelas relações. Diagramas lógicos.\nProposições e conectivos: Conceito de proposição, valores lógicos das proposições, proposições simples, proposições compostas. Operações lógicas sobre proposições: Negação, conjunção, disjunção, disjunção exclusiva, condicional, bicondicional.\nConstrução de tabelas-verdade. Tautologias, contradições e contingências. Implicação lógica, equivalência lógica, Leis De Morgan. Argumentação e dedução lógica.\nSentenças abertas, operações lógicas sobre sentenças abertas. Quantificador universal, quantificador existencial, negação de proposições quantificadas.\nArgumentos Lógicos Dedutivos; Argumentos Categóricos."
  },
  {
    "objectID": "integracao_dados.html",
    "href": "integracao_dados.html",
    "title": "Integração de Dados",
    "section": "",
    "text": "b) Fundamentos de Banco de Dados: Integração dos dados (ETL, Transferência de Arquivos e Integração via Base de Dados, Data Lakes e Soluções para Big Data, Diferenciação entre bancos relacionais, multidimensionais, documentos e grafos"
  },
  {
    "objectID": "integracao_dados.html#data-lake",
    "href": "integracao_dados.html#data-lake",
    "title": "Integração de Dados",
    "section": "Data Lake",
    "text": "Data Lake\n\nO que é um data lake?\nUm data lake é um repositório centralizado que permite armazenar todos os seus dados estruturados e não estruturados em qualquer escala. Você pode armazenar seus dados como estão, sem precisar primeiro estruturá-los e executar diferentes tipos de análise, desde painéis e visualizações até processamento de big data, análise em tempo real e machine learning para orientar melhores decisões.\n\n\nPor que você precisa de um data lake?\nAs organizações que geram valor empresarial com êxito a partir de seus dados superarão seus pares. Uma pesquisa da Aberdeen revelou que as organizações que implementaram um data lake superaram em 9% a performance de empresas semelhantes no crescimento orgânico da receita. Esses líderes foram capazes de fazer novos tipos de análise, como machine learning em novas fontes, como arquivos de log, dados de fluxos de cliques, mídia social e dispositivos conectados à Internet armazenados no data lake. Isso os ajudou a identificar e agir de acordo com as oportunidades de crescimento dos negócios mais rapidamente, atraindo e retendo clientes, aumentando a produtividade, mantendo dispositivos proativamente e tomando decisões informadas.\n\n\n\nData lakes comparados a data warehouses: duas abordagens diferentes\nDependendo dos requisitos, uma organização típica exigirá um data warehouse e um data lake, pois atendem a diferentes necessidades e casos de uso.\nUm data warehouse é um banco de dados otimizado para analisar dados relacionais provenientes de sistemas transacionais e aplicações de linha de negócios. A estrutura de dados e o esquema são definidos antecipadamente para otimizar consultas SQL rápidas, em que os resultados são normalmente usados para relatórios e análises operacionais. Os dados são limpos, enriquecidos e transformados para que possam atuar como a “fonte única da verdade” em que os usuários podem confiar.\nUm data lake é diferente porque armazena dados relacionais de aplicações de linha de negócios e dados não relacionais de aplicativos móveis, dispositivos IoT e mídias sociais. A estrutura dos dados ou esquema não é definida quando os dados são capturados. Isso significa que você pode armazenar todos os seus dados sem um design cuidadoso ou a necessidade de saber para quais perguntas você pode precisar de respostas no futuro. Diferentes tipos de análise em seus dados, como consultas SQL, análise de big data, pesquisa de texto completo, análise em tempo real e machine learning, podem ser usados para descobrir insights.\nÀ medida que as organizações com data warehouses veem os benefícios dos data lakes, elas evoluem seu warehouse para incluir data lakes e habilitar diversos recursos de consulta, casos de uso de ciência de dados e recursos avançados para descobrir novos modelos de informações. A Gartner chama essa evolução de “Solução de gerenciamento de dados para análise” ou “DMSA”.\n\n\n\n\n\n\n\n\nCaracterísticas\nData warehouse\nData lake\n\n\n\n\nDados\nRelacionais de sistemas transacionais, bancos de dados operacionais e aplicações de linha de negócios\nNão relacionais e relacionais de dispositivos de IoT, sites, aplicações móveis, mídia social e aplicações corporativas\n\n\nEsquema\nDefinido antes da implementação do DW (esquema na gravação)\nGravado no momento da análise (esquema na leitura)\n\n\nPreço/performance\nResultados de consulta mais rápidos, usando armazenamento de maior custo\nResultados de consulta ficando mais rápidos, usando armazenamento de menor custo\n\n\nQualidade dos dados\n\nDados altamente selecionados, que representam a versão central da verdade\nQuaisquer dados, selecionados ou não (ou seja, dados brutos)\n\n\n\nUsuários\nAnalistas de negócios\nCientistas de dados, desenvolvedores de dados e analistas de negócios (usando dados selecionados)\n\n\nAnálises\nGeração de relatórios em lote, BI e visualizações\nMachine learning, análises preditivas, descoberta de dados e criação de perfis\n\n\n\nOs elementos essenciais de uma solução de data lake e análise\nÀ medida que as organizações estão criando data lakes e uma plataforma de análise, elas precisam considerar vários recursos importantes, incluindo:\nMovimentação de dados\nOs data lakes permitem que você importe qualquer quantidade de dados que possa vir em tempo real. Os dados são coletados de várias fontes e movidos para o data lake em seu formato original. Esse processo permite escalar para dados de qualquer tamanho, economizando tempo na definição de estruturas de dados, esquemas e transformações.\n\nArmazene e catalogue dados com segurança\nOs data lakes permitem que você armazene dados relacionais, como bancos de dados operacionais e dados de aplicações de linha de negócios, e dados não relacionais, como aplicativos móveis, dispositivos IoT e mídias sociais. Eles também oferecem a capacidade de entender quais dados estão no lago por meio de crawling, catalogação e indexação de dados. Por fim, os dados devem ser protegidos para garantir que seus ativos de dados estejam protegidos.\n\n\nAnálises\nOs data lakes permitem que várias funções da organização, como cientistas de dados, desenvolvedores de dados e analistas de negócios, acessem dados com sua escolha de ferramentas e frameworks analíticos. Isso inclui frameworks de código aberto, como Apache Hadoop, Presto e Apache Spark, e ofertas comerciais de fornecedores de data warehouse e inteligência empresarial. Os data lakes permitem que você execute análises sem a necessidade de mover seus dados para um sistema de análise separado.\n\n\nMachine learning\nO data lakes permitirão que as organizações gerem diferentes tipos de insights, incluindo relatórios sobre dados históricos e machine learning, onde os modelos são criados para prever resultados prováveis e sugerir uma série de ações prescritas para alcançar o resultado ideal.\nO valor de um data lake\nA capacidade de aproveitar mais dados, de mais fontes, em menos tempo, e de capacitar os usuários a colaborar e analisar dados de diferentes maneiras leva a uma tomada de decisão melhor e mais rápida. Exemplos em que os data lakes agregaram valor incluem:\n\n\nMelhores interações com o cliente\nUm data lake pode combinar dados de clientes de uma plataforma de CRM com análise de mídia social, uma plataforma de marketing que inclui histórico de compras e tíquetes de incidentes para capacitar a empresa a entender o grupo de clientes mais lucrativo, a causa da perda de clientes e as promoções ou recompensas que aumentará a fidelidade.\n\n\nMelhorar as opções de inovação em P&D\nUm data lake pode ajudar suas equipes de P&D a testar hipóteses, refinar suposições e avaliar resultados, como escolher os materiais certos no design do produto, resultando em uma performance mais rápida, em pesquisas genômicas que levam a medicamentos mais eficazes ou no entendimento da disposição dos clientes de pagar por atributos diferentes.\n\n\nAumente as eficiências operacionais\nA Internet das Coisas (IoT) apresenta mais maneiras de coletar dados sobre processos como fabricação, com dados em tempo real provenientes de dispositivos conectados à Internet. Um data lake facilita o armazenamento e a execução de análises em dados de IoT gerados por máquina para descobrir maneiras de reduzir custos operacionais e aumentar a qualidade.  \nOs desafios dos data lakes\nO principal desafio com uma arquitetura de data lake é que os dados brutos são armazenados sem supervisão do conteúdo. Para que um data lake torne os dados utilizáveis, ele precisa ter mecanismos definidos para catalogar e proteger os dados. Sem esses elementos, os dados não podem ser encontrados ou confiáveis, resultando em um “pântano de dados”. Atender às necessidades de públicos mais amplos exige que os data lakes tenham governança, consistência semântica e controles de acesso.\n\n\nImplantar data lakes na nuvem\nData lakes são uma workload ideal para ser implantada na nuvem, porque a nuvem oferece performance, escalabilidade, confiabilidade, disponibilidade, um conjunto diversificado de mecanismos analíticos e enormes economias de escala. Uma pesquisa da ESG revelou que 39% dos entrevistados consideram a nuvem sua principal implantação para análise, 41% para data warehouses e 43% para Spark. Os principais motivos pelos quais os clientes perceberam a nuvem como uma vantagem para data lakes são: melhor segurança, tempo de implantação mais rápido, melhor disponibilidade, atualizações de recursos/funcionalidades mais frequentes, mais elasticidade, mais cobertura geográfica e custos vinculados à utilização real."
  },
  {
    "objectID": "integracao_dados.html#etl-sas",
    "href": "integracao_dados.html#etl-sas",
    "title": "Integração de Dados",
    "section": "ETL (SAS)",
    "text": "ETL (SAS)\nETL é um tipo de data integration em três etapas (extração, transformação, carregamento) usado para combinar dados de diversas fontes. Ele é comumente utilizado para construir um data warehouse. Nesse processo, os dados são retirados (extraídos) de um sistema-fonte, convertidos (transformados) em um formato que possa ser analisado, e armazenados (carregados) em um armazém ou outro sistema. Extração, carregamento, transformação (ELT) é uma abordagem alternativa, embora relacionada, projetada para jogar o processamento para o banco de dados, de modo a aprimorar a performance.\n\nHistória do ETL\nETL ganhou popularidade nos anos 1970, quando as organizações começaram a usar múltiplos repositórios ou bancos de dados para armazenar diferentes tipos de informações de negócios. A necessidade de integrar os dados que se espalhavam pelos databases cresceu rapidamente. O ETL tornou-se o método padrão para coletar dados de fontes diferentes e transformá-los antes de carregá-los no sistema-alvo ou destino.\nNo final dos anos 1980 e início dos 1990, os data warehouses entraram em cena. Sendo um tipo diferente de banco de dados, eles forneceram um acesso integrado a dados de múltiplos sistemas – computadores mainframes, minicomputadores, computadores pessoais e planilhas. Mas diferentes departamentos costumam usar diferentes ferramentas ETL com diferentes armazéns. Adicione isso a junções e aquisições, e muitas empresas acabam com distintas soluções ETL, que não foram integradas.\nCom o tempo, o número de formatos, fontes e sistemas de dados aumentou muito. Extrair, transformar e carregar é, hoje, apenas um dos vários métodos que as organizações utilizam para coletar, importar e processar dados. ETL e ELT são, ambos, partes importantes de uma estratégia ampla de data integration das empresas.\n\n\nQual a importância do ETL?\nHá anos, inúmeras empresas têm confiado no processo de ETL para obter uma visão consolidada dos dados que geram as melhores decisões de negócios. Hoje, esse método de integrar dados de múltiplos sistemas e fontes ainda é um componente central do kit de ferramentas de data integration de uma organização.\n\n\n\n\n\nExtract Transfrom Load - infographic\n\n\nO ETL é usado para mover e transformar dados de múltiplas fontes, e carregá-los em vários destinos, como o Hadoop.\n\nQuando utilizado com um data warehouse corporativo (dados em repouso), o ETL fornece o contexto histórico completo para a empresa;\nAo fornecer uma visão consolidada, o ETL facilita para os usuários corporativos a análise e a criação de relatórios sobre dados relevantes às suas iniciativas;\nO ETL pode melhorar a produtividade de profissionais analíticos, porque ele codifica e reutiliza processos que movem os dados sem que esses profissionais possuam a capacidade técnica de escrever códigos ou scripts;\nO ETL evoluiu ao longo do tempo para suportar os requisitos emergentes de integração para coisas como streaming data;\nAs organizações precisam tanto de ETL quanto ELT para unir dados, manter a precisão e fornecer a auditoria necessária para armazenar dados, criar relatórios e realizar análises. \n\n\n\nComo o ETL é usado?\nFerramentas centrais de ETL e ELT trabalham em conjunto com outras ferramentas de data integration e com outros vários aspectos do gerenciamento de dados – como data quality, data governance, virtualização e metadados. As utilizações populares de hoje incluem:\n\nETL e usos tradicionais\nETL é um método comprovado com o qual muitas empresas contam todos os dias – como varejistas, que precisam olhar os dados de vendas regularmente, ou operadoras de saúde procurando por um quadro preciso de seu uso. O ETL pode combinar e exibir dados de transações de um data warehouse ou outro banco de dados, de modo que eles estejam sempre prontos para analistas de negócios os visualizarem em um formato compreensível. O ETL também é utilizado para migrar dados de sistemas arcaicos para sistemas modernos, com diferentes formatos possíveis. É frequentemente usado para consolidar dados de fusões de empresas e para coletar e unir dados de fornecedores ou parceiros externos.\n\n\n\n\nETL com big data – transformações e adaptadores\nVence quem conseguir o maior número de dados. Embora isso não seja, necessariamente, uma verdade, ter acesso fácil a um amplo escopo de dados pode dar às empresas uma vantagem competitiva. Hoje, elas precisam de acesso a todo tipo de big data – vídeos, mídias sociais, a Internet das Coisas (IoT), logs do servidor, dados espaciais, dados abertos ou de crowdsource e muito mais. Fornecedores de ETL frequentemente adicionam novas transformações às suas ferramentas para suportar essas requisições emergentes e novas fontes de dados. Adaptadores oferecem acesso a uma ampla variedade de fontes de dados, e as ferramentas de data integration interagem com esses adaptadores para extrair e carregar dados de modo eficaz.\n\n\n\nETL para Hadoop – e mais\nO ETL evoluiu para oferecer suporte à integração entre muito mais que data warehouses tradicionais. Ferramentas avançadas de ETL podem carregar e converter dados estruturados e não-estruturados no Hadoop. Essas ferramentas leem e escrevem múltiplos arquivos em paralelo de, e para, Hadoop, simplificando como informações são fundidas em um processo de transformação comum. Algumas soluções incorporam bibliotecas de transformações ETL pré-construídas para os dados de transação e interação que são executados em Hadoop. ETL também oferece suporte à integração entre sistemas transacionais, bancos de dados operacionais, plataformas de BI, centralizadores master data management (MDM) e a nuvem.\n\n\n\nETL e acesso aos dados self-service\nData preparation self-service é uma tendência de rápido crescimento que coloca o poder de acesso, mistura e transformação de dados nas mãos dos usuários organizacionais e outros profissionais não-técnicos. Sendo específico em sua natureza, essa abordagem aumenta a agilidade organizacional e libera a TI de abastecer usuários com diferentes formatos de dados. Menos tempo é desperdiçado na preparação de dados e mais tempo é gasto na geração de insights. Consequentemente, tanto profissionais de TI ou de outros ramos da organização podem melhorar sua produtividade e as empresas podem escalonar seu uso de dados para tomarem decisões melhores.\n\n\n\nETL e data quality\nO ETL e outras ferramentas de data integration – utilizadas pra limpar, perfilar e auditar dados – garantem que os dados sejam confiáveis. As ferramentas ETL integram-se às de data quality, e fornecedores de ETL incorporam ferramentas relacionadas em suas soluções, como aquelas utilizadas para mapeamento e linhagem de dados.\n\n\n\nETL e metadados\nMetadados nos auxiliam a entender a linhagem dos dados (de onde eles vieram) e seu impacto em outros ativos de dados na organização. Conforme arquiteturas de dados se tornam mais complexas, é importante rastrear como os diferentes elementos de dados na sua organização são utilizados e relacionados. Por exemplo, se você adiciona o nome de uma conta do Twitter à sua base de dados de clientes, você vai precisar saber o que será afetado, como, por exemplo, tarefas, aplicações ou relatórios ETL.\n\n\nComo funciona?\nO ETL está intimamente relacionado a várias outras funções, processos e técnicas de data integration. Compreendê-las fornece uma visão mais clara de como o ETL funciona. \n\n\n\n\n\n\n\n\nSQL\nLinguagem de consulta estruturada (SQL, na sigla em inglês) é o método mais comum de acessar e transformar os dados de um database.\n\n\n\n\nTranformações, regras de negócios e adaptadores \n\nApós extrair os dados, o ETL utiliza regras de negócios para transformá-los em novos formatos. Os dados transformados são, então, carregados no destino.\n\n\nData mapping\nO mapeamento de dados (data mapping) é parte do processo de transformação. O mapeamento fornece instruções detalhadas para uma aplicação sobre como obter os dados necessários para processar. Ele também descreve qual campo de origem é mapeado para qual campo de destino. Por exemplo, o terceiro atributo de um feed de dados de atividades de um website pode ser o nome de usuário; o quarto pode ser o horário de quando a atividade aconteceu e o quinto pode ser o produto no qual o usuário clicou. Uma aplicação ou processo ETL, usando esses dados, teria que mapear esses mesmos campos ou atributos do sistema-fonte (por exemplo, a página de informações de atividade do site) no formato necessário pelo sistema de destino. Se o sistema de destino for um sistema de gestão de relacionamento com o cliente, ele pode primeiro armazenar o nome de usuário e o horário em quinto; ele pode nem sequer armazenar o produto. Nesse caso, uma transformação para formatar a data no formato esperado (e na ordem correta) pode acontecer enquanto os dados são lidos pela fonte e redigidos no destino.\n\n\nScripts\nETL é um método que automatiza os scripts (conjunto de instruções) que são executados no plano de fundo para mover e transformar os dados. Antes do ETL, scripts eram escritos individualmente em C ou COBOL para transferir dados entre sistemas específicos. Isso resultou em múltiplos bancos de dados executando diversos scripts. As primeiras ferramentas de ETL eram executadas em mainframes como um processo em lote. Ferramentas posteriores migraram para plataformas UNIX e PC. As organizações de hoje ainda utilizam tanto scripts quanto métodos de movimento programático de dados.\n\n\nETL versus ELT\nNo princípio, havia o ETL. Então, as empresas adicionaram o ELT, um método complementar. ELT extrai dados de um sistema-fonte, os carrega em um sistema de destino e, então, usa o poder de processamento do sistema-fonte para conduzir as transformações. Isso acelera o processamento de dados porque acontece onde os dados estão.\n\n\nData quality\nAntes que os dados sejam integrados, um ambiente de teste é normalmente criado onde eles possam ser limpos e padronizados (SP e São Paulo, Senhor e Sr. ou Bia e Beatriz), endereços possam ser verificados e duplicatas, removidas. Muitas soluções ainda são independentes, mas procedimentos de data quality podem agora ser executados como uma das transformações no processo de data integration.\n\n\nAgendando e processando\nFerramentas e tecnologias ETL podem fornecer tanto agendamento em lote quanto capacidades em tempo real. Elas podem também processar dados em altos volumes no servidor ou podem reduzir o processamento para o nível do banco de dados. Essa abordagem de processamento em um banco de dados, em vez de em um mecanismo especializado, evita duplicação dos dados e a necessidade de usar capacidades extras na plataforma do banco de dados.\n\n\nProcessamento em lote\nETL normalmente se refere a um processamento em lote que envolve a movimentação de grandes volumes de dados entre dois sistemas durante o que é chamado de “janela”. Nesse período de tempo determinado – por exemplo, entre o meio-dia e a uma da tarde – nenhuma ação pode ocorrer com o sistema-fonte, ou alvo, enquanto os dados são sincronizados. A maioria dos bancos realiza os processamentos em lote durante a noite para resolver transações que ocorrem durante o dia.\n\n\nWeb services\nWeb services são um método baseado na internet para fornecer dados ou funcionalidades a várias aplicações em tempo quase real. Esse método simplifica os processos de data integration e pode entregar, rapidamente, mais valor a partir dos dados. Por exemplo, imagine que um cliente entre em contato com a sua central de atendimento. Você poderia criar um web service que devolve o perfil completo do cliente em uma fração de segundo ao informar, simplesmente, um número de telefone para o web service que extrai os dados de múltiplas fontes ou de um hub MDM. Com um conhecimento mais rico do cliente, o atendente pode tomar decisões melhores sobre como interagir com esse cliente.\n\n\nMaster data management \n\nMDM é o processo de unir os dados para criar uma visão única deles, através de múltiplas fontes. Ele inclui tanto ETL quanto capacidades de data integration para misturar as informações e criar um “registro de ouro” ou um “melhor registro”.\n\n\n\nData virtualization\nVirtualização é um método ágil de misturar os dados para criar um panorama virtual sem movê-los. Data virtualization difere de ETL porque, embora mapeamento e combinação de dados ainda ocorram, não há a necessidade de uma tabela de teste física para armazenar os resultados. Isso porque o panorama é geralmente armazenado na memória e em cachê para melhorar a performance. Algumas soluções de data virtualization, como a SAS Federation Server, fornecem funções dinâmicas de mascaramento de dados, aleatorização e hashing para proteger dados confidenciais de grupos específicos. O SAS também fornece data quality sob demanda enquanto a exibição é gerada.\n\n\n\nEvent stream processing and ETL\nQuando a velocidade dos dados aumenta para milhões de eventos por segundo, event stream processing pode ser usado para monitorar e processar fluxos de dados, e ajudar a tomar decisões mais rapidamente. Um exemplo no setor de produção de energia é utilizar análises preditivas em fluxos de dados para detectar quando uma bomba submersa precisa de reparo, reduzindo o tempo de inatividade, além do escopo e do tamanho do dano à bomba."
  },
  {
    "objectID": "integracao_dados.html#etl-oracle",
    "href": "integracao_dados.html#etl-oracle",
    "title": "Integração de Dados",
    "section": "ETL (Oracle)",
    "text": "ETL (Oracle)\n\nO que é ETL?\nExtrair, transformar e carregar (ETL) é o processo que as organizações orientadas a dados usam para coletar dados de várias fontes e reuni-los para dar suporte à descoberta, à geração de relatórios, à análise e à tomada de decisões.\nAs origens de dados podem ser muito diversas em tipo, formato, volume e confiabilidade, de modo que os dados precisam ser processados para serem úteis quando reunidos. Os armazenamentos de dados de destino podem ser bancos de dados, data warehouses ou data lakes, dependendo das metas e da implementação técnica.\n\n\nAs três etapas distintas do ETL\nExtrair\nDurante a extração, o ETL identifica os dados e os copia de suas origens, de forma que possa transportar os dados para o armazenamento de dados de destino. Os dados podem vir de fontes estruturadas e não estruturadas, incluindo documentos, emails, aplicações de negócios, bancos de dados, equipamentos, sensores, terceiros e muito mais.\nTransformar\nComo os dados extraídos são brutos em sua forma original, eles precisam ser mapeados e transformados para prepará-los para o armazenamento de dados eventual. No processo de transformação, o ETL valida, autentica, desduplica e/ou agrega os dados de formas que tornam os dados resultantes confiáveis e consultáveis.\nCarregar\nO ETL move os dados transformados para o armazenamento de dados de destino. Esta etapa pode implicar o carregamento inicial de todos os dados de origem ou pode ser o carregamento de alterações incrementais nos dados de origem. Você pode carregar os dados em tempo real ou em lotes programados.\n\n\nELT ou ETL: Qual é a diferença?\nA etapa de transformação é de longe a mais complexa do processo ETL. Por conseguinte, a ETL e a ELT diferem em dois pontos principais:\n\nQuando a transformação ocorre\nO local da transformação\n\nEm um data warehouse tradicional, os dados são extraídos primeiro de “sistemas de origem” (sistemas de ERP, sistemas de CRM etc.). As ferramentas OLAP e as consultas SQL dependem da padronização das dimensões dos conjuntos de dados para obter resultados agregados. Isso significa que os dados devem passar por uma série de transformações.\nTradicionalmente, essas transformações foram feitas antes que os dados fossem carregados no sistema de destino, normalmente um data warehouse relacional.\nNo entanto, à medida que as tecnologias subjacentes de armazenamento e processamento de dados que sustentam o armazenamento de dados evoluem, tornou-se possível efetuar transformações no sistema alvo. Os processos ETL e ELT envolvem áreas de preparação. No ETL, essas áreas são encontradas na ferramenta, sejam elas proprietárias ou personalizadas. Elas ficam entre o sistema de origem (por exemplo, um sistema CRM) e o sistema de destino (o data warehouse).\nPor outro lado, com ELTs, a área de preparação está no data warehouse, e o mecanismo de banco de dados que alimenta o DBMS faz as transformações, em vez de uma ferramenta ETL. Portanto, uma das consequências imediatas dos ELTs é que você perde as funções de preparação e limpeza de dados que as ferramentas ETL fornecem para ajudar no processo de transformação de dados.\n\n\nData warehouses corporativos e de ETL\nTradicionalmente, as ferramentas para ETL eram usadas principalmente para entregar dados a data warehouses corporativos que suportam aplicações de BI (Business Intelligence). Esses data warehouses são projetados para representar uma fonte confiável de verdade sobre tudo o que está acontecendo em uma empresa em todas as atividades. Os dados nesses warehouses são cuidadosamente estruturados com esquemas, metadados e regras rígidos que controlam a validação de dados.\nAs ferramentas de ETL para data warehouses corporativos devem atender aos requisitos de integração de dados, como carregamentos em lote de alto volume e alto desempenho; processos de integração de fluxo lento e orientados a eventos; transformações programáveis; e orquestrações para que possam lidar com as transformações e fluxos de trabalho mais exigentes e tenham conectores para as mais diversas fontes de dados.\nApós carregar os dados, você tem várias estratégias para mantê-los sincronizados entre os armazenamentos de dados de origem e de destino. É possível recarregar o conjunto de dados completo periodicamente, programar atualizações periódicas dos dados mais recentes ou confirmar para manter a sincronização total entre o data warehouse de origem e o de destino. Essa integração em tempo real é denominada captura de dados de alteração (CDC). Para esse processo avançado, as ferramentas ETL precisam entender a semântica de transação dos bancos de dados de origem e transmitir corretamente essas transações para o data warehouse de destino.\n\n\nETL e data marts\nOs data marts são armazenamentos de dados de destino menores e mais focados do que os data warehouses corporativos. Eles podem, por exemplo, se concentrar em informações sobre um único departamento ou uma única linha de produtos. Por isso, os usuários de ferramentas ETL para data marts costumam ser especialistas em linha de negócios (LOB), analistas de dados e/ou cientistas de dados.\nAs ferramentas ETL para data marts devem ser usadas pelo pessoal de negócios e pelos gerentes de dados, em vez de por programadores e pela equipe de TI. Portanto, essas ferramentas devem ter um workflow visual para facilitar a configuração de pipelines ETL.\n\n\nETL ou ELT e data lakes\nOs Data lakes seguem um padrão diferente dos data warehouses e data marts. Os data lakes geralmente armazenam seus dados no armazenamento de objetos ou nos HDFS (Hadoop Distributed File Systems), e portanto podem armazenar dados menos estruturados sem esquema; e oferecem suporte a várias ferramentas para consultar esses dados não estruturados.\nUm padrão adicional que isso permite é extrair, carregar e transformar (ELT), no qual os dados são armazenados primeiro “como estão” e serão transformados, analisados e processados depois que os dados forem capturados no data lake. Esse padrão oferece vários benefícios.\n\nTodos os dados são registrados; nenhum sinal é perdido devido à agregação ou filtragem.\nOs dados podem ser ingeridos muito rapidamente, o que é útil para o streaming da Internet das Coisas (IoT), análise de log, métricas de site etc.\nEle permite a descoberta de tendências que não eram esperadas no momento da captura.\nEle permite a implementação de novas técnicas de inteligência artificial (IA) que se destacam na detecção de padrões em conjuntos de dados grandes e não estruturados.\n\nAs ferramentas ETL para data lakes incluem ferramentas visuais de integração de dados, porque são eficazes para cientistas de dados e engenheiros de dados. As ferramentas adicionais que geralmente são usadas na arquitetura do data lake incluem o seguinte:\n\nServiços de Cloud Streaming que podem ingerir grandes fluxos de dados em tempo real em data lakes para mensagens, logs de aplicações, telemetria operacional, rastreamento de dados de fluxo de cliques na Web, processamento de eventos e análise de segurança. A compatibilidade com o Kafka garante que esses serviços possam recuperar dados de origens de dados quase infinitas.\nServiços de nuvem baseados em Spark que podem executar rapidamente tarefas de processamento e transformação de dados em conjuntos de dados muito grandes. Os serviços Spark podem carregar os conjuntos de dados a partir do armazenamento de objetos ou do HDFS, processá-los e transformá-los na memória em clusters escaláveis de instâncias de computação e gravar a saída de volta no data lake ou em data marts e/ou data warehouses.\n\n\n\nCasos de uso ETL\nO processo ETL é fundamental para muitos setores por causa de sua capacidade de ingerir dados de forma rápida e confiável em data lakes para ciência de dados e análise, criando modelos de alta qualidade. As soluções ETL também podem carregar e transformar dados transacionais em escala para criar uma visão organizada de grandes volumes de dados. Isso permite que as empresas visualizem e prevejam tendências do setor. Vários setores contam com o ETL para permitir insights acionáveis, tomada de decisões rápida e maior eficiência.\nServiços financeiros\nAs instituições de serviços financeiros coletam grandes quantidades de dados estruturados e não estruturados para obter insights sobre o comportamento do consumidor. Esses insights podem analisar o risco, otimizar os serviços financeiros dos bancos, melhorar as plataformas online e até mesmo fornecer caixas eletrônicos com dinheiro.\nSetores de petróleo e gás\nOs setores de petróleo e gás usam soluções ETL para gerar previsões sobre uso, armazenamento e tendências em áreas geográficas específicas. O ETL funciona para reunir o máximo de informações possível de todos os sensores de um site de extração e processar essas informações para facilitar a leitura.\nAutomotivo\nAs soluções de ETL permitem que as concessionárias e os fabricantes entendam os padrões de vendas, calibrem suas campanhas de marketing, reabram o estoque e acompanhem os leads dos clientes.\nTelecomunicações\nCom o volume e a variedade sem precedentes de dados produzidos hoje, os provedores de telecomunicações contam com soluções ETL para melhor gerenciar e entender esses dados. Uma vez processados e analisados esses dados, as empresas podem usá-los para melhorar a publicidade, a mídia social, o SEO, a satisfação do cliente, a lucratividade e muito mais.\nAssistência Médica\nCom a necessidade de reduzir custos e também melhorar a assistência médica, o setor de assistência médica emprega soluções ETL para gerenciar registros de pacientes, coletar informações de seguros e atender a requisitos regulatórios em evolução.\nCiências biológicas\nOs laboratórios clínicos contam com soluções ETL e inteligência artificial (IA) para processar vários tipos de dados que estão sendo produzidos por instituições de pesquisa. Por exemplo, colaborar no desenvolvimento de vacinas requer que grandes quantidades de dados sejam coletados, processados e analisados.\nSetor público\nCom o surgimento dos recursos de Internet das Coisas (IoT), as cidades inteligentes estão usando o ETL e o poder da IA para otimizar o tráfego, monitorar a qualidade da água, melhorar o estacionamento e muito mais."
  },
  {
    "objectID": "itil.html",
    "href": "itil.html",
    "title": "ITIL 4",
    "section": "",
    "text": "g) Governança de TI: ITIL versão 4 (ITIL 4): Operação de Serviços (Gerenciamento de Eventos, Gerenciamento de Incidentes, Gerenciamento de Problemas, Cumprimento de Requisições, Gerenciamento de Acessos), Desenho de Serviços (Gerenciamento de Níveis de Serviço, Gerenciamento de Capacidade, Gerenciamento de Disponibilidade, Gerenciamento de Continuidade de Serviços de TI, Gerenciamento de Continuidade de Negócio), Transição de Serviços (Gerenciamento de Configuração e Ativos de Serviços de TI, Gerenciamento de Liberação e Implantação, Gerenciamento de Mudanças), Melhoria Contínua de Serviços, Métricas (Fatores Críticos de Sucesso - CSFs, Índices Chave de Performance - KPIs)."
  },
  {
    "objectID": "itil.html#o-que-é-o-sistema-itil",
    "href": "itil.html#o-que-é-o-sistema-itil",
    "title": "ITIL 4",
    "section": "O que é o Sistema ITIL",
    "text": "O que é o Sistema ITIL\nO Sistema ITIL é um conjunto de boas práticas de gerenciamento de serviços de tecnologia de informação. Ele está de acordo com a norma ISO/IEC 20000, primeira padronização da International Organization for Standardization (ISO) voltada exclusivamente para a gestão de TI.\nITIL é a sigla para Information Technology Infrastructure Library – que pode ser traduzido para “biblioteca de infraestrutura de tecnologia da informação”.\nO sistema foi desenvolvido pela Agência Central de Computação e Telecomunicações (CCTA, na sigla em inglês) do Reino Unido na década de 1980, com o objetivo de estabelecer um padrão de segurança e confiabilidade na gestão de processos de TI, garantindo assim uma boa experiência para os usuários.\nPara isso, o modelo descreve boas práticas de infraestrutura, manutenção e operações. As orientações estão alinhadas aos métodos ágeis, ao DevOps e ao Lean, bastante utilizados por times de tecnologia.\nDesde que o ITIL foi proposto pela CCTA, o sistema passou por quatro grandes atualizações:\n\nITIL v1\nA primeira versão do ITIL era voltada para as agências governamentais, que começavam a se informatizar na década de 1980. Era, literalmente, uma biblioteca: uma coleção de livros físicos que chegou a mais de 30 volumes em 1996.\n\n\nITIL v2\nNos anos 2000 foi lançada a segunda versão do ITIL. Os 30 volumes foram condensados em 9, mas ainda eram voltados para entidades do governo britânico.\n\n\nITIL v3\nCom a popularização do modelo em empresas e demais entidades privadas, foi lançada uma nova atualização em maio de 2007.\nO ITIL v3 era descrito em 5 livros, que reuniam 26 processos e 4 funções. A maior inovação foi o conceito do Ciclo de Vida de Serviço (CVS), que era composto por dois componentes básicos:\n\nNúcleo do ITIL: conjunto de melhores práticas que podem ser adotadas por todas as organizações que prestam serviços ao negócio;\nGuias complementares do ITIL: boas práticas complementares reunidas em publicações específicas para diferentes setores da indústria, modelos operacionais e arquiteturas de TI.\n\nEm 2011, oITIL v3 ganhou uma grande atualização para dar clareza a conceitos e adicionar novas práticas ao CVS.\nÉ nesse período também que a CCTA foi incorporada ao Escritório de Comércio Governamental (OGC, na sigla em inglês), entidade do Reino Unido responsável por promover a eficiência nos processos de negócios do Estado.\nO sistema ITIL passou a ser atualizado pelo OGC até 2013, quando uma joint venture entre o governo britânico e a empresa Capita, a Axelos, assumiu o framework. Em 2021, a Axelos passou a fazer parte do grupo PeopleCert.\n\n\nITIL 4\nA quarta e última atualização do ITIL veio em fevereiro de 2019, com a publicação do livro “ITIL Foundations”.\nO modelo de gestão de TI foi alterado para atender as necessidades da Era Digital, com foco na criação de valor para os usuários, na condução de estratégias de negócios e na adaptação à transformação digital.\n\n\nAs principais mudanças do ITIL 4\nO ITIL 4 foi desenvolvido em conjunto entre a Axelos e a comunidade de profissionais de TI para adaptar o ITIL v3 às mudanças cada vez mais aceleradas da Era Digital.\nA principal mudança é a maior flexibilidade na execução dos processos. Na versão anterior, havia um certo engessamento no CVS, que dependia de uma série de estágios para ser executado.\nO ITIL 4 propõe o Sistema de Valor de Serviço (SVS) para alterar esse cenário. O SVS é um conjunto de componentes e atividades de uma empresa que possibilita a criação de valor.\nA flexibilidade vem da criação de um ecossistema entre organização, fornecedores, stakeholders e clientes. Todos devem atuar juntos para manter o sistema funcionando.\nOs componentes do SVS são:\n\nCadeia de valor de serviço: modelo operacional flexível para a entrega e aprimoramento contínuo de serviços. Tem como atividades principais planejar, melhorar, engajar, desenhar, construir e entregar;\n34 práticas que atualizam os processos do ITIL v3;\nGovernança: conjunto de normas e práticas que são a base para a definição de processos internos, de acordo com as exigências do setor e os valores da organização. Facilita a integração com outros frameworks como o COBIT.\nMelhoria contínua.\n\n\n\n\nRepresentação gráfica do Sistema de Valor de Serviços do ITIL 4.\n\n\nOutra mudança importante é a inclusão de tecnologias emergentes, como Cloud Computing, Infraestrutura como Serviço (IaaS), Machine Learning e blockchain.\nO ITIL 4 também introduziu novos conceitos, que você vai conhecer em detalhes a seguir.\n\n\nOs princípios do ITIL 4\nSão 7 princípios que devem orientar os profissionais de TI na adoção do SVS e, assim, adaptar o ITIL à realidade de suas empresas:\n\nConcentrar-se no valor;\nComeçar por onde você está;\nAvançar iterativamente com feedback;\nColaborar e promover a visibilidade;\nPensar e trabalhar pensando no todo;\nManter os processos simples e práticos;\nOtimizar e automatizar constantemente.\n\n\n\nAs 4 dimensões do ITIL 4\n\n\n\nRepresentação gráfica das dimensões do ITIL 4.\n\n\nAs dimensões do ITIL são necessárias para a entrega de valor ao cliente, além de facilitar a visão holística da gestão de serviços. Todas são afetadas por fatores internos e externos à organização.\nAs 4 dimensões são:\n\nOrganizações e pessoas;\nInformação e tecnologia;\nParceiros e fornecedores;\nFluxos de valor e processos.\n\n\n\nAs 34 práticas do ITIL 4\nAs práticas do ITIL 4 são “um conjunto de recursos necessários para realizar o trabalho ou cumprir um objetivo”. Elas têm como objetivo dar uma visão holística do sistema de serviços, ao considerar elementos como cultura, tecnologia, informações e gerenciamento de dados.\nA palavra “prática” também evita as ambiguidades do termo “processos”, que é usado no dia a dia das empresas em todos os departamentos. Hoje, o ITIL considera um processo como “um conjunto de atividades que transformam entradas em saídas”.\nAs práticas são divididas em 3 grandes grupos:\n\n\n1. Práticas gerais de gestão\n\nGerenciamento da estratégia\nGerenciamento da segurança da informação\nGerenciamento de fornecedor\nGerenciamento de mudança organizacional\nGerenciamento de projetos\nGerenciamento de relacionamento\nGerenciamento de riscos\nGerenciamento de talento e força de trabalho\nGerenciamento do conhecimento\nGerenciamento do portfólio\nGerenciamento financeiro dos serviços\nGestão da arquitetura\nMedição e reporte\nMelhoria contínua\n\n\n\n2. Práticas de gestão de serviço\n\nAnálise de negócio\nCentral de serviço\nDesenho de serviço\nGerenciamento de ativos de TI\nGerenciamento de capacidade e desempenho\nGerenciamento do catálogo de serviços\nGerenciamento de configuração de serviço\nGerenciamento de continuidade de serviço\nGerenciamento de disponibilidade\nGerenciamento de incidente\nGerenciamento de liberação\nGerenciamento de nível de serviço\nGerenciamento de problema\nGerenciamento de requisição de serviço\nHabilitação de mudança\nMonitoramento e gerenciamento de evento\nValidação e teste de serviço\n\n\n\n3. Práticas de gestão técnica\n\nDesenvolvimento e gerenciamento de software\nGerenciamento de implantação\nGerenciamento de infraestrutura e plataforma\n\nNenhuma prática está vinculada a um elemento do SVS nem é pré-requisito para a adoção de outras práticas. Não é obrigatório usar todas as 34 simultaneamente.\n\n\n\n\n\n\nNote\n\n\n\nEm negrito itens citados no edital."
  },
  {
    "objectID": "itil.html#itil-4---prof.-gabriel-pacheco-youtube",
    "href": "itil.html#itil-4---prof.-gabriel-pacheco-youtube",
    "title": "ITIL 4",
    "section": "ITIL 4 - Prof. Gabriel Pacheco (Youtube)",
    "text": "ITIL 4 - Prof. Gabriel Pacheco (Youtube)"
  },
  {
    "objectID": "itil.html#itil-4---prof.-adriano-martins-antonio-youtube",
    "href": "itil.html#itil-4---prof.-adriano-martins-antonio-youtube",
    "title": "ITIL 4",
    "section": "ITIL 4 - Prof. Adriano Martins Antonio (Youtube)",
    "text": "ITIL 4 - Prof. Adriano Martins Antonio (Youtube)\nPrincípios Orientadores da ITIL 4 (Preparatório para o exame ITIL 4 Foundation)\nSistema de Valor de Serviços da ITIL 4 - ITSM\nSimulado 1 Comentado da ITIL 4 Foundation (40 Perguntas - Parte 1)\nSimulado 1 Comentado da ITIL 4 Foundation (40 Perguntas - Parte 2)\nSimulado 2 Comentado da ITIL 4 Foundation (40 Perguntas - Parte 1)\nSimulado 2 Comentado da ITIL 4 Foundation (40 Perguntas - Parte 2)"
  },
  {
    "objectID": "itil.html#section",
    "href": "itil.html#section",
    "title": "ITIL 4",
    "section": "",
    "text": "ITIL®: o que é, para que serve e como tirar a certificação\nO ITIL tem sido um dos mais populares frameworks dos últimos 20 anos, servindo como uma referência para as empresas gerenciarem melhor seus serviços habilitados por tecnologia.\nAtualmente é difícil encontrar alguém que nunca tenha ouvido falar sobre ITIL. Seja em um artigo, um comentário na empresa ou uma vaga de TI. \nMas se você está procurando um conteúdo mais completo sobre este tema, especialmente sobre a versão mais recente do ITIL, então está no lugar certo. \n\nO que é ITIL?\nO acrônimo ITIL se refere à Information Technology Infrastructure Library ou Biblioteca de Infraestrutura de Tecnologia da Informação. \nEste acrônimo há algum tempo não representa mais o que a ITIL significa nos dias de hoje. Portanto, podemos considerar que o nome foi mantido mais por uma questão de marca e reconhecimento do que pelo significado do acrônimo. \nDe maneira simples, o ITIL pode ser considerado como um conjunto de práticas detalhadas para se fazer um bom gerenciamento de serviços habilitados pela tecnologia da informação. \nMas dada a sua evolução através dos anos e a sua popularidade gigantesca, podemos considerar que o ITIL é hoje a principal referência para gerenciamento de serviços de TI. \n\n\nQual é a finalidade e a importância do ITIL?\nDesde 1989, em mais de 180 países, ITIL® tem sido reconhecida mundialmente como líder em Gerenciamento de Serviço de TI  (IT Service Management – ITSM).\nO propósito da ITIL 4 é oferecer às organizações uma estrutura prática e flexível como suporte na jornada rumo ao novo mundo da transformação digital, ajudando a alinhar os recursos humanos, digitais e físicos para competir em um cenário moderno e complexo.\nO ITIL4 foi construído com base neste guia por meio da reformulação das práticas de ITSM estabelecidas no contexto mais amplo da experiência do cliente, dos fluxos de valor, da transformação digital e do pensamento sistêmico. Também adota novas formas de trabalho, como Lean, Agile e DevOps.\nAlém disso, a nova versão do ITIL também trás orientações para as organizações navegarem na nova era tecnológica, mais conhecida como a Quarta Revolução Industrial, marcada por tecnologias emergentes, como: inteligência artificial, Internet das Coisas, nanotecnologia e muito mais.\nPor fim, o ITIL também é a qualificação de TI imprescindível para profissionais que buscam atingir todo o seu potencial, dar um salto na carreira e entregar uma mudança poderosa a organizações ambiciosas.\nPara marcas mundiais de grande importância, como IBM, NASA, Shell, HP, British Airways e Disney, os profissionais com certificação ITIL são a força que mantém a continuidade das operações de negócio.\nE para quem deseja construir (e reconstruir) as bases do mundo digital, tudo começa com treinamento e certificação.\n\n\nQuem usa o ITIL?\nO ITIL pode beneficiar qualquer organização que forneça um produto ou serviço habilitado (ou não) por TI.  Ele é usado por organizações em todo o mundo em todas as indústrias e setores:\n\nGrandes, médias e pequenas empresas\nGovernos nacionais, estaduais e locais.\nUniversidades e instituições de educação\nOrganizações não governamentais\n\nEmbora o ITIL seja usado em todo o mundo, é difícil encontrar uma lista definitiva e oficial de organizações que o utilizam. No entanto, A Axelos (quem atualmente cuida do desenvolvimento do ITIL ) publica regularmente informações sobre empresas que usam ITIL através de artigos, estudos de caso, white papers e webinarios (todos em inglês, infelizmente).\n\n\nOrigens do ITIL\nITIL4 (e não ITIL v4)\nEm 18 de fevereiro de 2019, é lançada a quarta versão, que deixa de usar o acrônimo “v”, passando a se chamar simplesmente ITIL4. \nHá dois motivos para a ausência do “v”. \n\nUma referência à quarta revolução industrial, ou indústria 4.0.\n\n\n\nA estrutura sugere que as publicações não serão tão centralizadas quanto eram na versão anterior.\n\nNo ITIL v3 haviam 5 publicações core, cada uma com algumas centenas de páginas. Qualquer atualização nesta estrutura geraria um grande esforço.\nNo ITIL4 os arquitetos parecem ter incorporado os princípios dos quais descrevem na própria literatura, como o de progredir iterativamente com feedback.\nIsso significa que, ao que tudo indica, teremos publicações mais específicas e um volume maior de atualizações independentes nas publicações, que não afetam a publicação core – ITIL4 Foundation. \n\n\nO que há de novo no ITIL4?\nUm novo modelo conceitual\nO ITIL4 traz agora uma nova forma de contextualizar e fornecer serviços, de forma muito mais adaptável, rápida e totalmente transparente.\nMuito diferente da versão anterior, a primeira quebra de conceitos e paradigmas é a forma de conceber o serviço como um todo. \nO conceito de ciclo de vida do serviço descrito na versão anterior, mesmo que não intencionalmente, sugeria uma sequência de estágios e processos que descreviam a vida do serviço desde a sua concepção até a sua descontinuação. \nPor exemplo, para que um serviço estivesse num momento de codificação (Transição), teoricamente ele deveria ter passado pelos ciclos de Estratégia e Desenho. \nA nova versão baseia-se agora em um fluxo de valor flexível, conhecido como Service Value System (Sistema de Valor de Serviço). \nEste novo modelo conceitual é mais robusto e não se concentra apenas em processos, mas também em outro componentes de vital importância para o gerenciamento de serviços e a entrega de valor, como a governança e princípios orientadores, que veremos com mais detalhes ao longo deste artigo.\n\n\nQuais os elementos chave do ITIL4?\nO modelo conceitual do ITIL4 apresenta 5 elementos principais. São eles:\n\nO Sistema de Valor de Serviço (SVS)\nA Cadeia de Valor de Serviço (CVS)\nAs quatro dimensões do gerenciamento de serviços\nOs princípios orientadores\nAs práticas ITIL\n\n\n\nSistema de valor de serviço\nO sistema de valor de serviço (SVS) é um componente chave do ITIL4 que facilita a cocriação de valor. Ele descreve como todos os componentes e atividades de uma organização trabalham juntos para permitir a criação de valor.\nComo o SVS tem interfaces com outras organizações, ele forma um ecossistema e também pode criar valor para essas organizações, seus clientes e partes interessadas.\nOs componentes do sistema de valor de serviço podem ser combinados de uma forma flexível, o que requer integração e coordenação para manter a organização consistente.\n\n\nOs componentes chave do SVS são:\n\nA cadeia de valor de serviço;\nAs práticas da ITIL (uma modernização dos antigos “processos” do ITILv3);\nOs princípios orientadores;\nGovernança (abrindo espaço para integração com frameworks como o COBIT 2019;\nMelhoria contínua.\n\nNo vídeo abaixo você pode aprender mais sobre o conceito do Sistema de Valor de Serviço (SVS)\n\n\n\n\n\nSistema de Valor de Serviço do ITIL4\n\n\n\n\nCadeia de Valor de Serviço\nNo centro da SVS está a cadeia de valor de serviços – um modelo operacional flexível para a criação, entrega e melhoria contínua dos serviços. A cadeia de valor do serviço define seis atividades principais:\n\nPlanejar;\nMelhorar;\nEngajar;\nDesenho e transição;\nObter / construir;\nEntregar e suportar. \n\n\n\n\n\n\nCadeia de Valor de Serviço do ITIL4\n\n\nEles podem ser combinados em muitas sequências diferentes, o que significa que a cadeia de valor de serviço permite que uma organização defina um número de variantes de fluxos de valor, por ex. o ciclo de vida do serviço v3.\nA flexibilidade da cadeia de valor do serviço permite que uma organização reaja de forma efetiva e eficiente às demandas mutáveis ​​das partes interessadas.\nNo vídeo a seguir explico um pouco mais sobre a Cadeia de Valor de Serviço (CVS):\n\n\nAs quatro dimensões do gerenciamento de serviços\n\nUma abordagem holística para o gerenciamento de serviços é fundamental no ITIL 4. Ele define quatro dimensões que são críticas para a facilitação bem-sucedida de valor para os clientes e outras partes interessadas.\nAs quatro dimensões são:\n\nOrganizações e pessoas: Uma organização precisa de uma cultura que apoie seus objetivos e o nível certo de capacidade e competência entre sua força de trabalho.\nInformação e tecnologia: No contexto da SVS, isso inclui as informações e o conhecimento, bem como as tecnologias necessárias para o gerenciamento de serviços.\nParceiros e fornecedores: refere-se aos relacionamentos de uma organização com as outras empresas envolvidas no design, na implantação, na entrega, no suporte e na melhoria contínua dos serviços.\nFluxos e processos de valor: Como as várias partes da organização trabalham de forma integrada e coordenada é importante para permitir a criação de valor através de produtos e serviços.\n\nÉ essencial que seja dada uma quantidade adequada de foco a cada uma dessas dimensões, para que o SVS permaneça equilibrado e eficaz.\n\n\nPrincípios orientadores\nO ITIL4 apresenta sete princípios orientadores. Estes princípios não são novos, e visam ajudar os profissionais de TI a adotar e adaptar as orientações da ITIL às suas próprias necessidades e circunstâncias específicas.\nOs princípios orientadores são:\n\nConcentre-se no valor\nComece por onde você está\nProgrida iterativamente com feedback\nColabore e promova a visibilidade\nPense e trabalhe de forma holística\nMantenha simples e prático\nOtimize e automatize\n\nOs princípio orientadores devem ser avaliados em todas as etapas da prestação de serviços e, quando relevantes, podem ser empregados de maneira individual ou agrupada, independentemente do nível ou das circunstâncias. \nO foco do ITIL4 em colaboração, automação e simplicidade refletem os princípios encontrados nas metodologias Ágil, DevOps e Lean.\nNeste outro vídeo falo com um pouco mais de detalhes sobre os princípios orientadores:\n\n\nDe processos à práticas\nO ITIL até agora utilizou “processos” para gerenciar serviços de TI. A atualização expande os processos para que elementos como cultura, tecnologia, informações e gerenciamento de dados também sejam considerados para promover assim uma visão holística das formas de trabalho.\nIsso é conhecido como “práticas”, uma parte fundamental do novo framework. O SVS inclui 34 práticas de gerenciamento, que são conjuntos de recursos organizacionais para executar o trabalho ou realizar um objetivo.\nAs práticas compartilham o mesmo valor e importância dos processos, mas seguem uma abordagem mais holística.\nO ITIL4 inclui 34 práticas de gerenciamento como “conjuntos de recursos organizacionais projetados para realizar trabalho ou cumprir um objetivo”. Para cada prática, há vários tipos de orientação, como termos e conceitos-chave, fatores de sucesso, atividades-chave, objetos de informação, etc.\n\n\nVeja a seguir a lista das práticas do ITIL4:\nAs 34 práticas são agrupadas em três categorias:\n\nPráticas gerais de gerenciamento\nPráticas de gerenciamento de serviço\nPráticas de gerenciamento técnico\n\n\n\nPráticas gerais de gerenciamento:\n\nGerenciamento da estratégia\nGerenciamento do portfólio\nGestão da arquitetura\nGerenciamento financeiro dos serviços\nGerenciamento de talento e força de trabalho\nMelhoria contínua\nMedição e reporte\nGerenciamento de riscos\nGerenciamento da segurança da informação\nGerenciamento do conhecimento\nGerenciamento de mudança organizacional\nGerenciamento de projetos\nGerenciamento de relacionamento\nGerenciamento de fornecedor\n\n\n\nPráticas de gerenciamento de serviço:\n\nAnálise de negócio\nGerenciamento do catálogo de serviços\nDesenho de serviço\nGerenciamento de nível de serviço\nGerenciamento de disponibilidade\nGerenciamento de capacidade e desempenho\nGerenciamento de continuidade de serviço\nMonitoramento e gerenciamento de evento\nCentral de serviço\nGerenciamento de incidente\nGerenciamento de requisição de serviço\nGerenciamento de problema\nGerenciamento de liberação\nHabilitação de mudança\nValidação e teste de serviço\nGerenciamento de configuração de serviço\nGerenciamento de ativos de TI\n\n\n\nPráticas de gerenciamento técnico:\n\nGerenciamento de implantação\nGerenciamento de infraestrutura e plataforma\nDesenvolvimento e gerenciamento de software\n\nSe você quer entender um pouco mais sobre as práticas do ITIL4, recomendo este vídeo:\n\n\nQuais os benefícios do ITIL para o negócio?\nDo ponto de vista de negócios, a adoção de práticas ITIL por\nOs provedores de serviços de TI – sejam fornecedores internos ou externos – garantem muitos benefícios, incluindo:\n\n Serviços de TI que se alinham melhor com as prioridades e objetivos do negócio, o que significa que o negócio alcança mais em termos de seus objetivos estratégicos.\nCustos de TI conhecidos e gerenciáveis, garantindo que a empresa planeje melhor suas finanças.\nAumento da produtividade, eficiência e eficácia dos negócios, porque os serviços de TI são mais confiáveis ​​e funcionam melhor para os usuários de negócios.\nEconomias financeiras de gerenciamento aprimorado de recursos e redução do retrabalho.\nGerenciamento de mudanças mais eficaz, permitindo que a empresa acompanhe as mudanças e impulsione as mudanças nos negócios a seu favor.\nMaior satisfação do usuário e do cliente com a TI.\nMelhor percepção do cliente final e imagem da marca.\n\n\n\nConclusão\nO ITIL padronizou o “idioma” do gerenciamento de serviços. Os termos que a maior parte das empresas utilizam, como SLA, Incidente, Mudança, Problema, Requisição de Serviço e Service Desk surgiram ou foram popularizados pelo ITIL.\nEmpresas de todos os portes e formatos podem se beneficiar de ITIL. Sendo assim, as ferramentas práticas, visões e percepções que você adquire por meio do guia ITIL, mas elas sempre devem ser adaptadas para atender às necessidades específicas da sua organização."
  },
  {
    "objectID": "itil_praticas.html",
    "href": "itil_praticas.html",
    "title": "Práticas ITIL",
    "section": "",
    "text": "g) Governança de TI: ITIL versão 4 (ITIL 4): Operação de Serviços (Gerenciamento de Eventos, Gerenciamento de Incidentes, Gerenciamento de Problemas, Cumprimento de Requisições, Gerenciamento de Acessos), Desenho de Serviços (Gerenciamento de Níveis de Serviço, Gerenciamento de Capacidade, Gerenciamento de Disponibilidade, Gerenciamento de Continuidade de Serviços de TI, Gerenciamento de Continuidade de Negócio), Transição de Serviços (Gerenciamento de Configuração e Ativos de Serviços de TI, Gerenciamento de Liberação e Implantação, Gerenciamento de Mudanças), Melhoria Contínua de Serviços, Métricas (Fatores Críticos de Sucesso - CSFs, Índices Chave de Performance - KPIs)."
  },
  {
    "objectID": "itil_praticas.html#section",
    "href": "itil_praticas.html#section",
    "title": "Práticas ITIL",
    "section": "",
    "text": "Introdução as práticas da ITIL4"
  },
  {
    "objectID": "itil_praticas.html#quais-são-as-práticas-de-gerenciamento-do-itil-4",
    "href": "itil_praticas.html#quais-são-as-práticas-de-gerenciamento-do-itil-4",
    "title": "Práticas ITIL",
    "section": "Quais são as práticas de gerenciamento do ITIL 4?",
    "text": "Quais são as práticas de gerenciamento do ITIL 4?\nO ITIL 4 contém 34 práticas de gerenciamento para ajudar as organizações a fornecer uma entrega de serviço eficaz em toda a cadeia de valor.\nEnquanto as versões anteriores do ITIL focavam em serviços de TI, o ITIL 4 expande suas práticas de gerenciamento para incluir também cultura, tecnologia e gerenciamento de dados. Isso reflete uma mudança geral no ITIL 4, afastando-se do pensamento dominante do processo e em direção a uma forma diversa e dinâmica de operação.\nExistem três categorias de práticas de gerenciamento ITIL 4, que veremos em profundidade neste blog:\n\nPráticas gerais de gestão , que se aplicam a toda a organização para o sucesso do negócio e dos serviços que presta.\nPráticas de gerenciamento de serviços , que se aplicam a serviços específicos que estão sendo desenvolvidos, implantados, entregues e suportados.\nPráticas de gerenciamento técnico , que são adaptadas de domínios de gerenciamento de tecnologia para fins de gerenciamento de serviços.\n\n\nPráticas gerais de gerenciamento\nEsta seção contém 14 práticas de gerenciamento:\n\n1. Gerenciamento de arquitetura\nEssa prática ajuda as organizações a gerenciar a maneira muitas vezes complexa em que sua arquitetura organizacional se relaciona com várias partes do negócio.\nEle fornece os princípios, padrões e ferramentas para ajudar a gerenciar mudanças de forma estruturada e ágil.\n\n\n2. Melhoria contínua\nAs organizações devem ser capazes de alinhar seus processos e serviços com as necessidades de negócios em constante mudança.\nA prática de melhoria contínua os ajuda a conseguir isso. Ele garante que as organizações identifiquem oportunidades de melhoria nos serviços, componentes de serviço, práticas ou outras partes do gerenciamento de serviço.\n\n\n3. Gestão da segurança da informação\nEssa prática está relacionada à maneira como uma organização protege suas informações confidenciais contra uso indevido. Especificamente, o gerenciamento de segurança da informação procura maneiras de evitar violações da confidencialidade, integridade e disponibilidade dos dados.\nNesse contexto, a confidencialidade refere-se à informação sendo visualizada apenas por pessoas autorizadas, a integridade da informação sendo precisa e a disponibilidade da informação sendo acessível quando necessário.\n\n\n4. Gestão do conhecimento\nEssa prática ajuda as organizações a melhorar a maneira como usam os dados. Centra-se na conveniência, eficácia e eficiência do conhecimento e uso de dados.\n\n\n5. Medição e relatórios\nPara tomar boas decisões e melhorar continuamente os sistemas, as organizações devem realizar pesquisas baseadas em evidências.\nEsta prática fornece uma estrutura para fazer isso, recomendando avaliações de risco e coleta de dados relevantes.\n\n\n6. Gestão de mudanças organizacionais\nEssa prática ajuda as organizações a implementar as mudanças recomendadas durante o processo de melhoria contínua.\nEle enfatiza o aspecto humano da gestão de mudanças e os benefícios duradouros que podem ser obtidos se os desafios e as oportunidades dos indivíduos forem levados em consideração.\n\n\n7. Gerenciamento de portfólio\nEssa prática garante que a organização tenha a combinação certa de programas, produtos e serviços para atingir seus objetivos.\nEle também considera o financiamento da organização e as restrições de recursos.\n\n\n8. Gerenciamento de projetos\nEssa prática ajuda as organizações a supervisionar seus projetos em andamento e garantir que sejam entregues com sucesso.\nAborda a forma como os projetos são planejados, delegados, monitorados e mantidos. Aborda também as relações entre as partes interessadas e visa manter os envolvidos no projeto motivados.\n\n\n9. Gestão de relacionamento\nPara que os projetos sejam bem-sucedidos, as organizações devem estabelecer e nutrir os relacionamentos entre as partes interessadas. Essa prática ajuda as organizações a identificar, analisar, monitorar e melhorar continuamente os relacionamentos.\n\n\n10. Gestão de riscos\nEssa prática ajuda as organizações a entender e lidar com os riscos. Existem inúmeras maneiras pelas quais os problemas podem se materializar e é essencial que sejam identificados o mais rápido possível para evitar interrupções, consequências financeiras e problemas de sustentabilidade.\n\n\n11. Gestão financeira do serviço\nEssa prática apóia as estratégias e os planos da organização, garantindo que os recursos financeiros e os investimentos sejam usados ​​de forma eficiente.\n\n\n12. Gestão da estratégia\nEssa prática ajuda as organizações a definir objetivos específicos e formas de alcançá-los. Também garante que os recursos necessários sejam alocados para atingir essas metas e esclarece as prioridades da organização.\n\n\n13. Gestão de fornecedores\nAs organizações devem gerenciar seus fornecedores de forma eficaz se quiserem garantir a produção e a entrega tranquilas de produtos e serviços.\nEssa prática ajuda a fomentar esses relacionamentos, focando na criação de oportunidades de colaboração e na identificação de formas de fazer melhorias.\n\n\n14. Gestão de talentos da força de trabalho\nEssa prática ajuda as organizações a colocar pessoas talentosas e qualificadas nas funções certas. Ele se concentra no planejamento, recrutamento, integração e treinamento de funcionários.\nTambém analisa a forma como as organizações avaliam o desempenho dos funcionários e como desenvolver o planejamento de sucessão.\n\n\n\nPráticas de gerenciamento de serviços\nEsta seção contém mais 17 práticas de gerenciamento:\n\n15. Gerenciamento de disponibilidade\nCom essa prática, as organizações podem garantir que a disponibilidade de produtos e serviços atenda às necessidades do cliente. Essas necessidades devem ter sido acordadas no início do projeto.\n\n\n16. Análise de negócios\nEssa prática ajuda as organizações a analisar seus processos de negócios ou elementos dentro deles. Destina-se a ajudar a resolver problemas específicos e melhorar a criação de valor para as partes interessadas.\n\n\n17. Gestão de capacidade e desempenho\nEssa prática ajuda as organizações a garantir que seus produtos e serviços atendam aos níveis de desempenho esperados. Ele também atende às demandas atuais e futuras, ajudando as organizações a identificar quaisquer mudanças que possam afetar sua capacidade.\n\n\n18. Habilitação de mudança\nEssa prática garante que as organizações maximizem as mudanças de TI bem-sucedidas. Ele faz isso garantindo que as avaliações de risco sejam realizadas, que as autorizações apropriadas estejam em vigor para implementar mudanças e que as mudanças sejam gerenciadas com eficiência.\n\n\n19. Gestão de incidentes\nO objetivo desta prática é mitigar o impacto negativo de incidentes disruptivos. Ele ajuda as organizações a identificar maneiras de restaurar a operação normal do serviço o mais rápido possível.\n\n\n20. Gerenciamento de ativos de TI\nEssa prática ajuda as organizações a gerenciar o ciclo de vida completo de seus ativos de TI. Baseia-se na maximização de valor, controle de custos, gestão de riscos, tomada de decisão, gestão de reutilização de ativos e aposentadoria.\nTambém aborda os requisitos regulatórios e contratuais relacionados aos ativos de TI.\n\n\n21. Monitoramento e gerenciamento de eventos\nCom essa prática, as organizações podem observar sistematicamente serviços e componentes de serviço, além de registrar e relatar alterações selecionadas.\nEles podem fazer isso identificando e priorizando infraestrutura, serviços, processos de negócios e eventos de segurança da informação. A prática também estabelece as respostas a esses eventos.\n\n\n22. Gerenciamento de problemas\nEssa prática ajuda as organizações a mitigar o impacto e a probabilidade de eventos disruptivos. Ele faz isso concentrando-se na identificação de possíveis causas de incidentes e nas formas de navegar por eles.\n\n\n23. Gerenciamento de liberação\nEssa prática se concentra na maneira como os serviços são implantados. Ele aborda serviços e recursos novos e alterados.\n\n\n24. Gerenciamento do catálogo de serviços\nEssa prática garante que as organizações tenham uma única fonte de informações consistentes para todos os seus serviços. Garante que a informação esteja disponível para públicos relevantes sempre que necessário.\n\n\n25. Gerenciamento de configuração de serviço\nEssa prática garante que as informações sobre a configuração dos serviços de uma organização permaneçam disponíveis e precisas. Ele também aborda os itens de configuração que suportam esses serviços.\n\n\n26. Gestão da continuidade do serviço\nEsta prática fornece uma estrutura para a construção de resiliência organizacional. Ele ajuda as organizações a proteger os serviços no caso de um incidente perturbador e garantir que sua disponibilidade e desempenho permaneçam em um nível suficiente.\n\n\n27. Projeto de serviço\nEssa prática ajuda as organizações a projetar produtos e serviços adequados ao uso e alinhados com o propósito definido. Também garante que os serviços possam ser entregues com sucesso pela organização em seu ecossistema atual.\nA prática tem como foco o planejamento de produtos e serviços, bem como a gestão de pessoas, parceiros, fornecedores, informação, redes de comunicação e tecnologia.\n\n\n28. Balcão de atendimento\nEssa prática ajuda as organizações a capturar a demanda por resolução de incidentes e solicitações de serviço. Deve também ser o ponto de contacto do prestador de serviços e dos seus utilizadores.\n\n\n29. Gerenciamento do nível de serviço\nEssa prática define metas de negócios para a execução dos serviços. Ele garante que a prestação de serviços possa ser avaliada adequadamente, permitindo que a organização identifique problemas e melhore suas práticas.\n\n\n30. Gerenciamento de solicitação de serviço\nCom essa prática, as organizações podem oferecer suporte à qualidade de serviço acordada ao lidar com todas as solicitações de serviço predefinidas e iniciadas pelo usuário de maneira eficaz e amigável.\n\n\n31. Validação e teste do serviço\nEssa prática garante que produtos e serviços novos ou alterados atendam aos requisitos definidos.\nAs organizações devem fazer isso medindo o valor do serviço com base nas informações dos clientes, objetivos de negócios e requisitos regulamentares.\n\n\n\nPráticas de gestão técnica\nEsta seção contém as três práticas de gerenciamento finais:\n\n32. Gerenciamento de implantação\nAs práticas de gerenciamento de implantação ajudam as organizações a mover hardware, software, documentação e processos novos ou alterados de um ambiente de produção para um ambiente ativo.\nTambém os ajuda a mover esses componentes para outros ambientes para teste ou preparação.\n\n\n33. Gerenciamento de infraestrutura e plataforma\nEssa prática ajuda as organizações a supervisionar sua infraestrutura e plataformas, permitindo que monitorem tecnologias implantadas internamente e por provedores de serviços.\n\n\n34. Desenvolvimento e gerenciamento de software\nEssa prática garante que os aplicativos atendam às necessidades das partes interessadas. Ele aborda a funcionalidade do software, confiabilidade, manutenção, conformidade e sua capacidade de ser auditado.\n\nFontes: https://www.itgovernance.co.uk/blog/what-are-the-itil-4-management-practices"
  },
  {
    "objectID": "normalizacao.html",
    "href": "normalizacao.html",
    "title": "Normalização",
    "section": "",
    "text": "a) Banco de Dados: Fundamentos de administração de dados: Segurança; Modelagem de dados: Modelo entidade-relacionamento (entidades, atributos, chaves e relacionamentos) e Normalização;"
  },
  {
    "objectID": "normalizacao.html#normalização",
    "href": "normalizacao.html#normalização",
    "title": "Normalização",
    "section": "Normalização",
    "text": "Normalização"
  },
  {
    "objectID": "olap.html",
    "href": "olap.html",
    "title": "Conceitos de OLAP",
    "section": "",
    "text": "c) Análise de dados e informações: Conceitos de OLAP"
  },
  {
    "objectID": "olap.html#canaltech",
    "href": "olap.html#canaltech",
    "title": "Conceitos de OLAP",
    "section": "Canaltech",
    "text": "Canaltech\nO OLAP, do inglês “On-line Analytical Processing”, trata da capacidade de analisar grandes volumes de informações nas mais diversas perspectivas dentro de um Data Warehouse (DW). O OLAP também faz referência às ferramentas analíticas utilizadas no BI para a visualização das informações gerenciais e dá suporte para as funções de análises do negócio organizacional.\nOs sistemas OLTP e OLAP se diferenciam em diversos outros aspectos. Vejamos:\n\nEm resumo podemos dizer que a grande diferença está no fato de que um está direcionado ao funcionamento dentro do ambiente operacional (OLTP) e o outro com foco essencialmente gerencial (OLAP)."
  },
  {
    "objectID": "olap.html#brasil-escola",
    "href": "olap.html#brasil-escola",
    "title": "Conceitos de OLAP",
    "section": "Brasil Escola",
    "text": "Brasil Escola\n5.2. OLAP\nSegundo Michel (2003), OLAP (On-Line Analytical Processing ou Processamento Analítico On-Line) é um sistema de informação multidimensional cuja tecnologia de construção permite aos analistas de negócios, gerentes e executivos analisar e visualizar dados corporativos de forma rápida, consistente e principalmente interativa, ou seja, é onde são extraídos e gerados os relatórios para os usuários. Na figura 8, mostra que o OLAP fornece informações aos usuários.\nFigura 8: OLAP\n\nDa mesma forma como o BI, O OLAP não pode ser definido, como uma ferramenta ou um processo, mas sim um conjunto dos mesmos, pois os elementos essenciais para a criação de um OLAP é sua aplicabilidade em diversas camadas da tecnologia, como armazenamento e linguagem de programação, THOMSEN, 2002, complementa dizendo que “[...]De modo geral, pode-se falar de conceitos OLAP, linguagens OLAP, camadas de produtos OLAP e produtos de OLAP completos[...]”.\nO OLAP se difere do ETL, basicamente, pelo fato de ETL fazer a extração de dados diretamente de vários bancos, visando a sua organização e as soluções OLAP, extraem informações que foram geradas pelo ETL, se referindo a um conjunto de ferramentas voltadas para o acesso e análise ad-hoc de dados.\nBILL INMON, 2002 conceitua ad-hoc como:\nConsultas com acesso casual único e tratamento dos dados segundo parâmetros nunca antes utilizados, geralmente executados de forma iterativa e heurística. Isso tudo nada mais é do que o próprio usuário gerar consultas de acordo com suas necessidades de cruzar as informações de uma forma não vista e com métodos que o levem a descoberta daquilo que procura.”\nEm BI, o OLAP pode se apresentar principalmente de duas formas, como MOLAP, que é mais indicado para Data Marts e ROLAP que é mais indicado para Data Warehouse. Nos Data Marts o método de armazenamento de dados OLAP é chamado de MOLAP, que usa a tecnologia MDDB (MultiDimensional Database), isto se deve pelo fato de que os DM são mais específicos e a análise será mais limitada e com pouco detalhamento. Nos DW, o método é o ROLAP, que utiliza a tecnologia (Relational DataBase Management System), que possibilita um uso maior de funções e uma análise com mais confiabilidade na grande gama de informações que o DW possui.\nPara “navegar” nas dimensões do “cubo” OLAP, emprega-se o uso de operadores dimensionais, que tem papeis distintos, podendo ser para aumentar e diminuir a granulidade, que é o nível de detalhamento a ser visualizado, ou então para ordenar e classificar as informações, na seção 4.2.1 algumas operações são demonstradas.\n\n5.3. Operações\nUma das características mais importantes das ferramentas OLAP é a possibilidade de realizar algumas operações no decorrer da implementação, que nos fornece total controle das informações a serem exibidas e ordenadas. Existem variados tipos de comandos, mas comumente no BI utiliza-se algumas principais e são elas: Drill Across, Drill Up, Drill Down e Drill-Through.\nFigura 9: Operações\n\n5.3.1. Drill Across\nÉ um comando para pular de um nível intermediário dentro de uma dimensão para outra dimensão. É necessário a utilização de duas tabelas fatos e essas tabelas tem que compartilhar a mesma dimensão intermediária. Segundo Kimball (2002), “trata-se de uma operação sobre dois cubos. Os dados nos dois cubos são combinados nas dimensões comuns aos mesmos”. Além de “pular” entre as dimensões, também é possível compara-las, por exemplo, é possível traçar um comparativo entre duas dimensões, como o valor total de vendas, pelo numero de um determinado produto vendido, sendo necessário elas apenas compartilharem alguma dimensão. Barbieri, 2001 completa dizendo que “[...] embora correlacionadas, estão em estruturas separadas, porém unidas por algumas dimensões coerentes”.\n\n5.3.2. Drill Up e Drill Down\nO Drill-up é o aumento na hierarquia de uma dimensão, por exemplo, imagina uma dimensão “Tempo” onde estão organizadas as informações em dia, mês, semestre e ano, vamos supor que queiramos ir, do dia 10 para o mês de março, essas operações não vão de um cubo a outro, mas sim na mesma dimensão, aumentando assim a granularidade do DW e diminui o nível de detalhamento.\nJá o Drill Down é o contrário de Drill UP, é a descida na hierarquia de uma dimensão, indo de um mês para um dia diminuindo a granularidade e aumentando o nível de detalhe.\n\n\n5.3.3. Drill Through\nPossui o funcionamento parecido com o Drill Down, porém, tem como característica a possibilidade buscar os dados, ou informações, fora da estrutura principal. Por exemplo, após alcançar o nível máximo de detalhe em uma tabela fato, tem necessidade de se obter mais detalhe sobre determinada célula, como a nota fiscal, por exemplo, com o Drill Through é possível acessar o arquivo de origem dessa informação, diminuindo a granularidade, aumentando o nível de detalhe e saindo da estrutura principal do DW ou de um Data Mart."
  },
  {
    "objectID": "olap.html#estratégia-concursos",
    "href": "olap.html#estratégia-concursos",
    "title": "Conceitos de OLAP",
    "section": "Estratégia Concursos",
    "text": "Estratégia Concursos\nO que é OLAP e como ele é utilizado na Tecnologia da Informação\n\nConceito\nO OLAP (On-Line Analytical Processing / Processamento Analítico On-line) é uma importante ferramenta utilizada para realizar análises em grandes quantidades de dados, geralmente armazenados em Data Warehouses, sendo comumente utilizado em modelos dimensionais.\nEsta ferramenta é muito aplicada no ambiente empresarial, sendo que, através da sua utilização, é possível realizar o fornecimento de informações a respeito do comportamento dos negócios do usuário, podendo identificar tendências do mercado, condutas dos clientes, entre outras vantagens.\nA análise multidimensional é a grande característica do OLAP. Mas o que isso quer dizer? Bom, este tipo de análise é baseado em cubos de informações, na qual é possível que sejam realizados estudos por meio de diferentes variáveis (dimensões), simultaneamente. Vamos ilustrar? Observe a figura abaixo:\n\n\n\nCubo de dados\n\n\nAs informações do banco de dados são distribuídas em dimensões (Tempo, Região e Produto), dentro de um cubo. Desse modo, por exemplo, através do OLAP, é possível analisar os dados das vendas de determinado produto P1, na região R3, durante o período de tempo T2.\nVamos dar nomes a essas variáveis, de modo a clarear o raciocínio. Suponha-se que determinada empresa, que iniciou suas operações em 2018 e está em operação atualmente, comercialize os seguintes produtos: caminhões, carros, motos e bicicletas, sendo que ela realiza vendas em diversas regiões do Brasil, como a região Sul, Norte, Nordeste e Sudeste. Assim, através da análise por meio do OLAP, é possível identificar quantos carros foram vendidos na região Nordeste no ano de 2019, de maneira fácil e rápida.\nMais adiante explicaremos os diversos tipos de operações que podem ser realizadas utilizando o cubo de dados.\n\n\nOLAP x OLTP\n\nOLTP\nOs bancos de dados tradicionais também possuem suporte para o processamento de transações de um negócio, sendo que, para isto, é utilizada a ferramenta OLTP (On-Line Transaction Processing / Processamento de Transações On-Line).\nO OLTP é focado em realizar transações rotineiras e cotidianas em tempo real, com muita agilidade, através de inserções, exclusões e atualizações de informações, sendo também possível realizar consultas dos dados armazenados.\n\n\n\nDiferenças\nUm ponto que precisa estar fixado na mente do estudante é que o OLTP é focado em transações cotidianas e rotineiras, sendo processado de maneira mais rápida do que o OLAP, enquanto que o OLAP é focado em análises, de maneira a subsidiar decisões gerenciais e estratégicas na empresa, não realizando alterações na base de dados, como o OLTP, apenas o carregamento e a consulta das informações.\nAlém disso, os dados a serem analisados pelo OLTP geralmente são estruturados em bancos de dados relacionais com características transacionais, sendo que o OLAP é focado em dados estruturados em modelos multidimensionais.\nPARA FIXAR:\nOLAP -> foco em análise; modelo multidimensional; carga e consulta de dados; mais lento.\nOLTP -> foco em transações rotineiras; modelo relacional de uso transacional; inclusão e exclusão de dados, mais rápido.\n\n\nVariações de OLAP\nO OLAP pode ser dividido de acordo com a estrutura de armazenamento dos dados e de acordo com a origem da consulta de informações.\n\nEstrutura de armazenamento\n\nMOLAP (Processamento Analítico Multidimensional On-Line)\n\nNesta estrutura, são realizadas consultas multidimensionais diretamente em dados armazenados em bancos também multidimensionais, possuindo um alto desempenho. Além disso, o MOLAP possui como característica a limitação da quantidade de informações que pode ser analisada simultaneamente (baixa escalabilidade).\n\nROLAP (Processamento Analítico Relacional On-line)\n\nJá no ROLAP, os dados utilizados para as operações multidimensionais estão armazenados em bases de dados relacionais, através de tabelas formadas por linhas e colunas, possuindo um baixo desempenho de consulta. De maneira contrária ao MOLAP, não há restrições quanto à quantidade de dados a ser analisada, possuindo, assim, uma alta escalabilidade.\n\nHOLAP (Processamento Analítico On-Line Híbrido)\n\nEsse é o modelo híbrido de OLAP, ou seja, ele é uma combinação dos modelos MOLAP e ROLAP, possuindo, simultaneamente, um bom desempenho em consultas, bem como uma boa escalabilidade.\n\n\nOrigem da consulta\n\nDOLAP (Desktop On-Line Analytical Processing)\n\nNeste modelo, a consulta é realizada por meio de uma estação cliente diretamente a um servidor, retornando o cubo de informações solicitadas. Desse modo, o tráfego na rede é reduzido, melhorando o desempenho do servidor.\n\nWOLAP (Web On-Line Analytical Processing)\n\nNo WOLAP, a consulta é realizada por meio de um navegador web a um servidor, retornando ao usuário o cubo de dados solicitados.\n\n\n\nOperações OLAP\nComo já introduzimos no início do artigo, o OLAP é capaz de realizar análises através de cubos de informações com diversas dimensões.\nAs operações OLAP permitem que sejam visualizados subconjuntos específicos dentro do cubo de dados, ou seja, dentro do universo da base de informações, sendo possível solicitar e receber apenas aqueles dados de interesse do usuário. Vamos analisar os principais tipos de operações.\n\nSlice\nEsta operação é caracterizada pela fixação de um valor para uma das dimensões, obtendo, assim, uma fatia (slice em inglês) do cubo de dados.\nDe modo a exemplificar esta situação, vamos supor, na figura abaixo, que as variáveis correspondem aos seguintes nomes:\nT1, T2, T3 e T4 -> 2018, 2019, 2020 e 2021.\nR1, R2, R3 e R4 -> Sul, Norte, Sudeste e Nordeste.\nP1, P2, P3 e P4 -> Caminhão, Carro, Moto e Bicicleta.\nDesse modo, realizando a operação Slice, vamos fixar o produto Carro da dimensão Produto:\n\n\n\nOperações OLAP – SLICE\n\n\nAssim, temos uma fatia do cubo, obtendo as informações de vendas de carros durante todo o período de tempo (2018 a 2021) em todas as regiões (Sul, Sudeste, Norte e Nordeste).\n\n\nDice\nA operação Dice é realizada através da seleção de dois ou mais valores das dimensões, de modo a formar um subcubo de informações, como podemos ver abaixo:\n\n\n\nOperações OLAP – DICE\n\n\nDe acordo com o desenho acima, foi retornada a quantidade de vendas de motos e bicicletas (P3 e P4), nas regiões Sul e Norte (R1 e R2), durante o período de 2018 e 2019 (T1 e T2).\n\n\nPivot (Rotate)\nO Pivot realiza a rotação do cubo, de modo a alterar a posição das dimensões, sendo utilizado para realizar uma apresentação alternativa das informações da base de dados multidimensional, como podemos ver abaixo:\n\n\n\nOperações OLAP – PIVOT\n\n\n\n\nDrill Down (Roll Down)\nOutra importante operação OLAP é o Drill Down. Ele é utilizado quando o usuário necessita de informações mais detalhadas, sendo isto realizado através da redução de granularidade da análise. Não entendeu? Vamos exemplificar.\nVamos supor que o usuário não queira as informações baseadas em anos, mas em meses (ou seja, ele quer um maior detalhamento das informações). Desse modo, haverá uma redução da hierarquia da dimensão Tempo (ou seja, redução da granularidade). Assim, em vez de analisar o ano, irá ser analisado o mês, que é um conjunto menor da respectiva dimensão. Dizemos, assim, que houve a redução do grão Tempo, de ano para mês, sendo possível diminuir ainda mais, como a análise por dia.\nOutro exemplo é em relação às regiões, em que pode haver a necessidade do cliente em aumentar o detalhamento da análise dos dados desta dimensão, sendo que, em vez de obter os dados das vendas da região Sul, ele queira o desmembramento dela, de modo a obter os dados individuais dos estados que compõem esta região, como o Paraná, Santa Catarina e Rio Grande do Sul.\n\n\nDrill Up (Roll Up)\nEste é o inverso do Drill Down, sendo utilizado quando o usuário necessita de menos detalhes, havendo um aumento da granularidade. Por exemplo, caso os dados sejam exibidos por mês, é possível utilizar o Drill Up e aumentar a granularidade da dimensão tempo, de maneira a visualizar os dados por ano.\nPARA FIXAR:\nDRILL DOWN: Aumenta o detalhamento, diminuindo a granularidade.\nDRILL UP: Diminui o detalhamento, aumentando a granularidade.\n\n\nDrill Across\nEste processo navega entre os dados, realizando saltos entre os níveis dentro de uma mesma dimensão, sem a necessidade de passar pelos níveis intermediários.\nPor exemplo, suponha que dentro da dimensão Região haja os níveis Regiões, Estados e Municípios. Sendo assim, é possível navegar dentro da consulta do nível Região para o nível Município, sem a necessidade de entrar no nível Estado.\n\n\nDrill Through\nJá no Drill Through, há a mudança de dimensões durante a consulta. Por exemplo, o usuário pode estar navegando na dimensão região e, após, analisar os dados da dimensão tempo."
  },
  {
    "objectID": "pmbok.html",
    "href": "pmbok.html",
    "title": "PMBOK 7ª Edição",
    "section": "",
    "text": "f) Gerenciamento de Projetos: Conceitos básicos do PMBOK 7ª Edição, Metodologia SCRUM;"
  },
  {
    "objectID": "pmbok.html#princípios-pmbok-7---pmi",
    "href": "pmbok.html#princípios-pmbok-7---pmi",
    "title": "PMBOK 7ª Edição",
    "section": "Princípios PMBOK 7 - PMI",
    "text": "Princípios PMBOK 7 - PMI"
  },
  {
    "objectID": "pmbok.html#pmbok-7-princípios-mudanças-e-como-ficam-as-certificações",
    "href": "pmbok.html#pmbok-7-princípios-mudanças-e-como-ficam-as-certificações",
    "title": "PMBOK 7ª Edição",
    "section": "PMBOK 7: princípios, mudanças e como ficam as certificações",
    "text": "PMBOK 7: princípios, mudanças e como ficam as certificações"
  },
  {
    "objectID": "pmbok.html#pmbok-7---youtube",
    "href": "pmbok.html#pmbok-7---youtube",
    "title": "PMBOK 7ª Edição",
    "section": "PMBOK 7 - Youtube",
    "text": "PMBOK 7 - Youtube"
  },
  {
    "objectID": "proposicoes.html",
    "href": "proposicoes.html",
    "title": "Proposições e conectivos e Operações",
    "section": "",
    "text": "Proposições e conectivos: Conceito de proposição, valores lógicos das proposições, proposições simples, proposições compostas. Operações lógicas sobre proposições: Negação, conjunção, disjunção, disjunção exclusiva, condicional, bicondicional."
  },
  {
    "objectID": "rac_logico.html",
    "href": "rac_logico.html",
    "title": "Raciocínio Lógico/Analítico/Quantitativo",
    "section": "",
    "text": "Estrutura lógica de relações arbitrárias entre pessoas, lugares, objetos ou eventos fictícios; deduzir novas informações das relações fornecidas e avaliar as condições usadas para estabelecer a estrutura daquelas relações. Diagramas lógicos.\n\nProposições e conectivos: Conceito de proposição, valores lógicos das proposições, proposições simples, proposições compostas. Operações lógicas sobre proposições: Negação, conjunção, disjunção, disjunção exclusiva, condicional, bicondicional.\nConstrução de tabelas-verdade. Tautologias, contradições e contingências. Implicação lógica, equivalência lógica, Leis De Morgan. Argumentação e dedução lógica.\nSentenças abertas, operações lógicas sobre sentenças abertas. Quantificador universal, quantificador existencial, negação de proposições quantificadas.\nArgumentos Lógicos Dedutivos; Argumentos Categóricos."
  },
  {
    "objectID": "scrum.html",
    "href": "scrum.html",
    "title": "Metodologia SCRUM",
    "section": "",
    "text": "f) Gerenciamento de Projetos: Metodologia SCRUM;"
  },
  {
    "objectID": "scrum.html#scrum-guide---pt-br",
    "href": "scrum.html#scrum-guide---pt-br",
    "title": "Metodologia SCRUM",
    "section": "Scrum Guide - PT-BR",
    "text": "Scrum Guide - PT-BR"
  },
  {
    "objectID": "scrum.html#glossário-scrum",
    "href": "scrum.html#glossário-scrum",
    "title": "Metodologia SCRUM",
    "section": "Glossário Scrum",
    "text": "Glossário Scrum"
  },
  {
    "objectID": "scrum.html#resumo-scrum",
    "href": "scrum.html#resumo-scrum",
    "title": "Metodologia SCRUM",
    "section": "Resumo Scrum",
    "text": "Resumo Scrum\n\n\n\n\n\n\n3 Funções:\nProduct Owner\nScrum Master\nIntegrante de Equipe\n\n\n5 Eventos:\nSprint\nPlanejamento do Sprint\nDaily Scrum: reunião diária de 15 minutos, no mesmo horário e local.\nRevisão do Sprint\nRetrospectiva do Sprint\n\n\n3 Artefatos:\nBacklog do produto\nBacklog do sprint\nIncrementos\n\n\n5 Valores:\nComprometimento\nCoragem\nFoco\nRespeito\nTransparência\n\nFontes:\nhttps://scrumguides.org/\nhttps://www.scrum.org/resources/what-is-scrum\nhttps://pt.wikipedia.org/wiki/Scrum\nScrum Guia Prático - J.J. Sutherland, editora Sextante, 2020."
  },
  {
    "objectID": "seguranca_bd.html",
    "href": "seguranca_bd.html",
    "title": "Segurança",
    "section": "",
    "text": "a) Banco de Dados: Fundamentos de administração de dados: Segurança; Modelagem de dados: Modelo entidade-relacionamento (entidades, atributos, chaves e relacionamentos) e Normalização;"
  },
  {
    "objectID": "seguranca_bd.html#section",
    "href": "seguranca_bd.html#section",
    "title": "Segurança",
    "section": "",
    "text": "Segurança"
  },
  {
    "objectID": "sentencas_abertas.html",
    "href": "sentencas_abertas.html",
    "title": "Sentenças Abertas",
    "section": "",
    "text": "Sentenças abertas, operações lógicas sobre sentenças abertas. Quantificador universal, quantificador existencial, negação de proposições quantificadas."
  },
  {
    "objectID": "tabelas.html",
    "href": "tabelas.html",
    "title": "Construção de tabelas-verdade",
    "section": "",
    "text": "Construção de tabelas-verdade. Tautologias, contradições e contingências. Implicação lógica, equivalência lógica, Leis De Morgan. Argumentação e dedução lógica."
  },
  {
    "objectID": "visoes.html",
    "href": "visoes.html",
    "title": "Procedimentos, Funções, Visões e gatilhos",
    "section": "",
    "text": "b) Fundamentos de Banco de Dados: Conceitos: Sistemas de gerência de banco de dados (SGBD), Arquitetura, modelos lógicos e representação física; Organização física e métodos de acesso; Conceito de transação, concorrência, recuperação, integridade; Linguagens de definição (DDL) e manipulação de dados (DML) em SGBDs relacionais; Procedimentos (stored procedures), funções (functions), visões (views), visões materializadas (materialized views) e gatilhos (triggers), Linguagem de consulta estruturada (SQL, Avaliação de modelos de dados, Técnicas de engenharia reversa para criação e atualização de modelos de dados, Integração dos dados (ETL, Transferência de Arquivos e Integração via Base de Dados, Data Lakes e Soluções para Big Data, Diferenciação entre bancos relacionais, multidimensionais, documentos e grafos."
  },
  {
    "objectID": "visoes.html#stored-procedures",
    "href": "visoes.html#stored-procedures",
    "title": "Procedimentos, Funções, Visões e gatilhos",
    "section": "Stored Procedures",
    "text": "Stored Procedures"
  },
  {
    "objectID": "visoes.html#views",
    "href": "visoes.html#views",
    "title": "Procedimentos, Funções, Visões e gatilhos",
    "section": "Views",
    "text": "Views"
  },
  {
    "objectID": "visoes.html#funções",
    "href": "visoes.html#funções",
    "title": "Procedimentos, Funções, Visões e gatilhos",
    "section": "Funções",
    "text": "Funções"
  },
  {
    "objectID": "visoes.html#section",
    "href": "visoes.html#section",
    "title": "Procedimentos, Funções, Visões e gatilhos",
    "section": "",
    "text": "Triggers"
  }
]